"use strict";(self.webpackChunkStylistWidget=self.webpackChunkStylistWidget||[]).push([[3596],{45978:function(t,e,s){s.d(e,{RW:function(){return l},g7:function(){return p}});var n=s(9495),i=s(15841),o=s(44813),a=s(78825),r=s(79730);class l{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof l)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,e,s){if(null!=this.id2Value[t.id])throw new i.Qp(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=function(t,e){if(null==t.dtype||t.dtype===e.dtype)return e;try{return(0,n.cast)(e,t.dtype)}catch(s){throw new i.Qp(`The dtype of the feed (${e.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,e),this.name2Id[t.name]=t.id,null!=s&&(this.id2Mask[t.id]=s),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof r.Ar){if(null==this.id2Value[t.id])throw new i.Qp(`Nonexistent key: ${t.name}`);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new i.Qp(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Value[e]}}getMask(t){if(t instanceof r.Ar){if(null==this.id2Value[t.id])throw new i.Qp(`Nonexistent key: ${t.name}`);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new i.Qp(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&(0,n.dispose)(this.id2Mask)}}const h={},u={};function p(t,e,s,i){const r=null!=s&&s.training,p=Array.isArray(t),g=p?t:[t],m=g.map((t=>t.name)),y=[],w=e.names();for(const n of m)-1!==w.indexOf(n)?y.push(e.getValue(n)):y.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const b=m.join(",")+"|"+e.names().join(",");let $,v;if(null==h[b]){const t=function(t,e){n.util.assert(null!=t&&t.length>0,(()=>"Expected at least one fetch, got none"));let s=[],i={};if(1===t.length){const n=d(t[0],e);s=n.sorted,i=n.recipientMap}else{const n=new Set;for(const o of t){const{sorted:t,recipientMap:a}=d(o,e);for(const e of t)n.has(e.name)||(s.push(e),n.add(e.name));for(const e in a)null==i[e]&&(i[e]=new Set),a[e].forEach((t=>i[e].add(t)))}}return{sorted:s,recipientCounts:c(i)}}(g,e);$=t.sorted,v=t.recipientCounts,h[b]=$,u[b]=v}$=h[b],v={},r||Object.assign(v,u[b]);const T=new l(e);for(let l=0;l<$.length;++l){if(null!=i){const t=(0,n.memory)().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const t=$[l],h=t.sourceLayer;if(h instanceof a.m)continue;const u=[],p=[],c=[];let d=!1;for(const s of t.inputs){const t=T.getValue(s),n=T.getMask(s);u.push(t),p.push(n),null!=n&&(d=!0),r||(v[s.name]--,0!==v[s.name]||e.hasKey(s)||-1!==m.indexOf(s.name)||t.isDisposed||!0===s.sourceLayer.stateful||c.push(t))}d&&((s=s||{}).mask=p[0]);const g=(0,o.st)(h.apply(u,s));let w=null;h.supportsMasking&&(w=h.computeMask(u,p));const b=f(t),S=Array.isArray(b)?b:[b];for(let e=0;e<S.length;++e){T.hasKey(S[e])||T.add(S[e],g[e],Array.isArray(w)?w[0]:w);const t=m.indexOf(S[e].name);-1!==t&&(y[t]=g[e])}r||(0,n.dispose)(c)}return T.disposeMasks(),p?y:y[0]}function c(t){const e={};for(const s in t)e[s]=t[s].size;return e}function d(t,e){const s=new Set,n=[],i={};for(const r of e.names())s.add(r);const o=[],a=[];for(o.push(t);o.length>0;){const t=o[o.length-1];if(s.has(t.name)){o.pop();continue}const e=a[a.length-1]===o.length-1;if(0===t.inputs.length||e)o.pop(),n.push(t),s.add(t.name),e&&a.pop();else{a.push(o.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),s.has(e.name)||o.push(e)}}return{sorted:n,recipientMap:i}}function f(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let s=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const n of t.sourceLayer.inboundNodes[e].outputTensors)if(n.id===t.id){s=e;break}e=t.sourceLayer.getOutputAt(s)}return e}},71837:function(t,e,s){s.d(e,{Gw:function(){return B}});var n=s(9495),i=s(47661),o=s(39459),a=s(15841),r=s(55795),l=s(48981),h=s(37937),u=s(27430),p=s(94551),c=s(44813),d=s(9917),f=s(73072),g=s(91928),m=s(52700),y=s(17328),w=s(45978),b=s(85244),$=s(96681);function v(t,e,s){const n=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>null));if(1===n)return Array.isArray(t)&&1===t.length?t:"object"===typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==n)throw new Error(`Provided ${s} is an array of ${t.length} element(s), but the model has ${n} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"===typeof t&&Object.keys(t).length>0&&"object"===typeof t[Object.keys(t)[0]]){const s=[];return e.forEach((e=>{e in t?s.push(t[e]):s.push(null)})),s}throw new Error(`The model has multiple (${n}) outputs, so ${s} must be either an array with ${n} elements or an object with ${e} keys. Provided ${s} not understood: ${JSON.stringify(t)}`)}function T(t,e){return v(t,e,"classWeight")}async function S(t,e,s,i){if(null!=e||null!=i)throw new Error("Support sampleWeight is not implemented yet");if(null!=s){const e=(0,n.tidy)((()=>{if(1===t.shape.length)return t.clone();if(2===t.shape.length){if(t.shape[1]>1){const e=1;return t.argMax(e)}if(1===t.shape[1])return t.reshape([t.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)})),i=Array.from(await e.data());(0,n.dispose)(e);const o=[];return i.forEach((t=>{if(null==s[t])throw new Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);o.push(s[t])})),(0,n.tensor1d)(o,"float32")}return null}function N(t,e){return(0,n.mul)(t,e)}function k(t,e){let s,i;const o=e;s=o.xs,i=o.ys,n.util.assert(null!=s&&null!=i,(()=>`A Dataset iterator for fitDataset() is expected to generate objects of the form \`{xs: xVal, ys: yVal}\`, where the two values may be \`tf.Tensor\`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates ${e}`));const a=x("input",t.inputNames,s),r=x("output",t.outputNames,i),l=a[0].shape[0];n.util.assert(a.length===t.inputs.length,(()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: ${JSON.stringify(t.inputNames)})`)),n.util.assert(r.length===t.outputs.length,(()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${r.length} outputs.  (Expected output keys: ${JSON.stringify(t.outputNames)})`));for(let h=0;h<a.length;h++)n.util.assert(a[h].shape[0]===l,(()=>`Batch size mismatch: input ${t.inputNames[h]} has ${a[h].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));for(let h=0;h<r.length;h++)n.util.assert(r[h].shape[0]===l,(()=>`Batch size mismatch: output ${t.outputNames[h]} has ${r[h].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));return{xs:a,ys:r}}function x(t,e,s){if(s instanceof n.Tensor)return[s];if(Array.isArray(s))return n.util.assert(s.length===e.length,(()=>`Received an array of ${s.length} Tensors, but expected ${e.length} to match the ${t} keys ${e}.`)),s;{const n=[];for(const i of e){if(null==s[i])throw new a.Qp(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);n.push(s[i])}return n}}async function A(t,e,s){const i=null!=s.batchesPerEpoch;if(n.util.assert(null!=t.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),n.util.assert(null!=s,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),n.util.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),(()=>`For fitDataset(), config.epochs is expected to be a positive integer, but got ${s.epochs}`)),n.util.assert(!i||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),(()=>`For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got ${s.batchesPerEpoch}`)),n.util.assert(null==s.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const o=null!=s.validationData;let r,l;if(o)if(I(s.validationData))n.util.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),(()=>`For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got ${s.validationBatches}`));else{const t=function(t){if(3===t.length)throw new a.EH("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);r=t.xs,l=t.ys}const h=t.makeTrainFunction(),u=t.getDedupedMetricsNames();let p;p=o?u.slice().concat(u.map((t=>"val_"+t))):u.slice();const d=(0,b.Eq)(s.callbacks,s.yieldEvery),f=null==s.verbose?1:s.verbose,{callbackList:g,history:m}=(0,b.dY)(d,f,s.epochs,null,null,function(t,e){let s=null;null!=e.batchesPerEpoch?s=e.batchesPerEpoch:Number.isFinite(t.size)&&(s=t.size);return s}(e,s),null,o,p);g.setModel(t),t.history=m,await g.onTrainBegin(),t.stopTraining_=!1;let y=null==s.initialEpoch?0:s.initialEpoch,w=await e.iterator();for(;y<s.epochs;){const a={};await g.onEpochBegin(y);let p=0,d=0;for(i||(w=await e.iterator());!i||p<s.batchesPerEpoch;){const e=await w.next();if(i&&e.done)break;if(null!=e.value){const{xs:i,ys:o}=k(t,e.value),a={};a.batch=d,a.size=i[0].shape[0],await g.onBatchBegin(d,a);const r=[];if(null!=s.classWeight){const e=T(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)r.push(await S(o[t],null,e[t]))}const l=i.concat(o).concat(r),c=h(l);n.dispose(l);for(let t=0;t<u.length;++t){const e=u[t],s=c[t];a[e]=s,n.keep(s)}await g.onBatchEnd(d,a),(0,$.i)(a),d++,p++}if(i?p>=s.batchesPerEpoch:e.done){if(o){let e;e=I(s.validationData)?(0,c.st)(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):(0,c.st)(t.evaluate(r,l,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let s=0;s<t.metricsNames.length;++s)a[`val_${t.metricsNames[s]}`]=e[s]}break}if(t.stopTraining_)break}if(await g.onEpochEnd(y,a),y++,t.stopTraining_)break}return await g.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}function I(t){return"function"===typeof t.iterator}function E(t){n.util.assert(t>0&&Number.isInteger(t),(()=>`batchSize is required to be a positive integer, but got ${t}`))}function L(t,e,s){return null==t?[null]:Array.isArray(t)?t.map((t=>(0,i.Dh)(t,e,s-e))):(0,i.Dh)(t,e,s-e)}function _(t,e){return n.tidy((()=>null==t?null:Array.isArray(t)?t.map((t=>_(t,e))):(0,i.kg)(t,"int32"===e.dtype?e:e.toInt())))}function O(t,e){const s=[];let n=0,i=null;for(;n<t;)i=n+e,i>=t&&(i=t),s.push([n,i]),n=i;return s}async function z(t,e,s,o={}){if(t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let r,l,h,u,p,c,d;t.isTraining=!0;try{const g=null==o.batchSize?32:o.batchSize;E(g);const m=!1,y=await t.standardizeUserData(e,s,o.sampleWeight,o.classWeight,m,g);r=y[0],l=y[1],d=y[2];let w,v=!1;if(null!=o.validationData&&o.validationData.length>0){if(v=!0,2!==o.validationData.length)throw 3===o.validationData.length?new a.EH("validationData including sample weights is not supported yet."):new a.Qp(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${o.validationData} is invalid.`);h=o.validationData[0],u=o.validationData[1];const e=!0,s=await t.standardizeUserData(h,u,null,null,e,g);p=s[0],c=s[1],w=p.concat(c)}else if(null!=o.validationSplit&&o.validationSplit>0&&o.validationSplit<1){v=!0;const t=Math.floor(r[0].shape[0]*(1-o.validationSplit)),e=r[0].shape[0];p=L(r,t,e),r=L(r,0,t),c=L(l,t,e),l=L(l,0,t),w=p.concat(c)}else null!=o.validationSteps&&(v=!0);const T=r.concat(l).concat(d);t.checkTrainableWeightsConsistency();const S=t.makeTrainFunction(),N=t.getDedupedMetricsNames();let k,x;v?(t.makeTestFunction(),k=t.testFunction,x=N.slice().concat(N.map((t=>"val_"+t)))):(k=null,w=[],x=N.slice());const A=(0,b.Eq)(o.callbacks,o.yieldEvery),I=await async function(t,e,s,o,r,l,h,u,p,c,d,g,m,y,w){null==r&&(r=32),null==l&&(l=1),null==d&&(d=!0),null==m&&(m=0);let v=!1;if(null!=p&&null!=c&&(v=!0),null!=w&&(v=!0,null==y))throw new a.Qp("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const T=t.checkNumSamples(s,r,y,"steps_per_epoch");let S;null!=T&&(S=(0,f.y1)(0,T)),null==h&&(h=1);const{callbackList:N,history:k}=(0,b.dY)(u,h,l,m,T,y,r,v,g);N.setModel(t),t.history=k,await N.onTrainBegin(),t.stopTraining_=!1;for(let f=m;f<l;++f){await N.onEpochBegin(f);const l={};if(null!=y)throw new a.EH("stepsPerEpoch mode is not implemented yet.");{if("batch"===d)throw new a.EH("batch shuffling is not implemneted yet");d&&n.util.shuffle(S);const h=(0,n.tensor1d)(S),u=O(T,r);for(let a=0;a<u.length;++a){const d={};if(await N.onBatchBegin(a,d),n.tidy((()=>{const f=u[a][0],g=u[a][1],m=(0,i.Dh)(h,f,g-f);d.batch=a,d.size=g-f;const y=_(s,m),w=e(y);for(let t=0;t<o.length;++t){const e=o[t],s=w[t];d[e]=s,n.keep(s)}if(a===u.length-1&&v){const e=t.testLoop(p,c,r);for(let t=0;t<o.length;++t){const s=o[t],i=e[t];n.keep(i),l["val_"+s]=i}}})),await N.onBatchEnd(a,d),(0,$.i)(d),t.stopTraining_)break}h.dispose()}if(await N.onEpochEnd(f,l),t.stopTraining_)break}return await N.onTrainEnd(),await t.history.syncData(),t.history}(t,S,T,N,g,o.epochs,o.verbose,A,k,w,o.shuffle,x,o.initialEpoch,null,null);return I}finally{t.isTraining=!1,W(r,e),W(l,s),W(p,h),W(c,u),null!=d&&n.dispose(d)}}function D(t){const e=[];t instanceof n.Tensor&&(t=[t]);for(let s=0;s<t.length;++s){const n=t[s];if(1===n.rank)e.push((0,i.UG)(n,1));else{if(0===n.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");e.push(n)}}return e}function W(t,e){if(null==t)return;const s=[];if(e instanceof n.Tensor)s.push(e.id);else if(Array.isArray(e))e.forEach((t=>s.push(t.id)));else if(null!=e)for(const n in e){const t=e[n];s.push(t.id)}const i=[];if(t instanceof n.Tensor)-1===s.indexOf(t.id)&&i.push(t);else if(Array.isArray(t))t.forEach((t=>{-1===s.indexOf(t.id)&&i.push(t)}));else if(null!=t)for(const n in t){const e=t[n];-1===s.indexOf(e.id)&&i.push(e)}i.forEach((t=>{t.isDisposed||t.dispose()}))}function M(t){return Array.isArray(t)}function C(t){return!function(t){return t instanceof n.Tensor}(t)&&!M(t)}function Q(t,e,s,n=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(M(t)&&t.length>0)e=!0;else if(C(t)){for(const s in t)if(t.hasOwnProperty(s)){e=!0;break}}else e=!0;if(e)throw new a.Qp(`Error when checking model ${i} expected no data, but got ${t}`)}return[]}if(null==t)return e.map((t=>null));let o;if(C(t)){o=[];for(const s of e){if(null==t[s])throw new a.Qp(`No data provided for "${s}". Need data for each key in: ${e}`);o.push(t[s])}}else if(M(t)){if(t.length!==e.length)throw new a.Qp(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): ${t}`);o=t}else{if(e.length>1)throw new a.Qp(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${t.shape}`);o=[t]}if(o=D(o),null!=s)for(let r=0;r<e.length;++r){if(null==s[r])continue;const t=o[r];if(t.shape.length!==s[r].length)throw new a.Qp(`Error when checking ${i}: expected ${e[r]} to have ${s[r].length} dimension(s). but got array with shape ${t.shape}`);for(let o=0;o<s[r].length;++o){if(0===o&&!n)continue;const l=t.shape[o],h=s[r][o];if(null!=h&&h>=0&&l!==h)throw new a.Qp(`Error when checking ${i}: expected ${e[r]} to have shape [${s[r]}], but got array with shape [${t.shape}].`)}}return o}function F(t,e,s,n=!0,i=""){let o;if(Array.isArray(t)){if(t.length!==e.length)throw new a.Qp(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);o=t}else{if(e.length>1)throw new a.Qp(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(t.shape)}.`);o=[t]}if(null!=s)for(let r=0;r<e.length;++r){if(null==s[r])continue;const t=o[r];if(t.shape.length!==s[r].length)throw new a.Qp(`Error when checking ${i}: expected ${e[r]} to have ${s[r].length} dimension(s), but got array with shape ${JSON.stringify(t.shape)}`);for(let o=0;o<s[r].length;++o){if(0===o&&!n)continue;const l=t.shape[o],h=s[r][o];if(null!=h&&h!==l)throw new a.Qp(`Error when checking ${i}: expected ${e[r]} to have shape ${JSON.stringify(s[r])} but got array with shape ${JSON.stringify(t.shape)}.`)}}}class B extends y.m{constructor(t){super(t),this.isTraining=!1}summary(t,e,s=console.log){if(!this.built)throw new a.Qp("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");(0,d.o)(this,t,e,s)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"===typeof t.optimizer)this.optimizer_=u.K(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof n.Optimizer))throw new a.Qp("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let e=[];if(Array.isArray(t.loss)||"string"===typeof t.loss||"function"===typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new a.Qp(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const s=t.loss;e=s.map((t=>l.Jt(t)))}else{const s=l.Jt(t.loss);this.outputs.forEach((t=>{e.push(s)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new a.Qp(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: ${this.outputNames}`);for(const s of this.outputNames)t.loss[s],e.push(l.Jt(t.loss[s]))}this.lossFunctions=e,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let n=0;n<this.outputs.length;++n){const t=this.internalOutputShapes[n],e=this.outputNames[n];this.feedOutputNames.push(e),this.feedOutputShapes.push(t),this.feedLossFns.push(this.lossFunctions[n])}const s=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],(0,o.IU)("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const i=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let s;if("string"===typeof t||"function"===typeof t)s=[t];else{if(!Array.isArray(t)&&"object"!==typeof t)throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${t}`);s=t}if(Array.isArray(s))return e.map((t=>s));{const t=[];for(const n of e){let e=s.hasOwnProperty(n)?s[n]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),r=(t,e,s)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([s,t])};(0,o.IU)("metric",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;(e=>{let s,n,i;for(const a of e){if("string"===typeof a&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(a)){const e=this.internalOutputShapes[t];let o;1===e[e.length-1]||this.lossFunctions[t]===l.Jc?-1!==["accuracy","acc"].indexOf(a)?n=h.xM:-1!==["crossentropy","ce"].indexOf(a)&&(n=h.Jc):this.lossFunctions[t]===l.qp?-1!==["accuracy","acc"].indexOf(a)?n=h.qv:-1!==["crossentropy","ce"].indexOf(a)&&(n=h.qp):-1!==["accuracy","acc"].indexOf(a)?n=h.yP:-1!==["crossentropy","ce"].indexOf(a)&&(n=h.Ol),-1!==["accuracy","acc"].indexOf(a)?o="acc":-1!==["crossentropy","ce"].indexOf(a)&&(o="ce"),i=n,s=""+o}else{const t=h.Jt(a);i=t,s=""+h.GT(a)}let e;(0,o.IU)(s,(()=>{e=i})),r(t,s,e)}})(i[t])}})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&(this.trainableWeights.length,this.collectedTrainableWeights.length)}evaluate(t,e,s={}){const n=null==s.batchSize?32:s.batchSize;E(n);const i=this.standardizeUserDataXY(t,e,!0,n);try{const t=i[0].concat(i[1]);this.makeTestFunction();const e=this.testFunction,o=this.testLoop(e,t,n,s.verbose,s.steps);return(0,c.wL)(o)}finally{W(i[0],t),W(i[1],e)}}async evaluateDataset(t,e){return this.makeTestFunction(),async function(t,e,s){const i=null!=(s=s||{}).batches,o=t.testFunction;let r=[];if(s.verbose>0)throw new a.EH("Verbose mode is not implemented yet.");n.util.assert(!i||s.batches>0&&Number.isInteger(s.batches),(()=>`Test loop expects \`batches\` to be a positive integer, but received ${JSON.stringify(s.batches)}`));const l="function"===typeof e.next?e:await e.iterator();let h=0,u=0;for(;!i||u<s.batches;){const e=await l.next();if(r=n.tidy((()=>{if(e.value){const{xs:s,ys:i}=k(t,e.value),a=s.concat(i),l=n.tidy((()=>o(a)));if(n.dispose(a),0===u)for(let t=0;t<l.length;++t)r.push((0,n.scalar)(0));const p=a[0].shape[0];for(let t=0;t<l.length;++t){const e=l[t],s=r[t];r[t]=n.tidy((()=>n.add(r[t],n.mul(p,e)))),u>0&&n.dispose(s)}n.dispose(l),h+=p,++u}return r})),e.done)break}for(let a=0;a<r.length;++a){const t=r[a];r[a]=n.div(r[a],h),n.dispose(t)}return(0,c.wL)(r)}(this,t,e)}checkNumSamples(t,e,s,n="steps"){let i;if(null!=s){if(i=null,null!=e)throw new a.Qp(`If ${n} is set, batchSize must be null or undefined.Got batchSize = ${e}`)}else{if(null==t)throw new a.Qp(`Either the input data should have a defined shape, or ${n} shoud be specified.`);i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,e){if(Array.isArray(e)&&0===e.length)throw new a.Qp("`outputs` is an empty Array, which is not allowed.");const s=Array.isArray(e),i=s?e:[e],o=this.retrieveSymbolicTensors(i),r=new w.RW;if(t instanceof n.Tensor&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new a.Qp(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)r.add(this.inputs[e],t[e])}else for(const n of this.inputs){const e=t[n.name];if(null==e)throw new a.Qp(`No value is provided for the model's input ${n.name}`);r.add(n,e)}const l=(0,w.g7)(o,r);return s?l:l[0]}retrieveSymbolicTensors(t){const e=(0,c.fD)(null,t.length);let s=t.length;for(const n of this.layers){const i=Array.isArray(n.output)?n.output:[n.output],o=i.map((t=>t.name));for(let n=0;n<t.length;++n){const a=o.indexOf(t[n]);if(-1!==a&&(e[n]=i[a],s--),0===s)break}if(0===s)break}if(s>0){const s=[];throw e.forEach(((e,n)=>{null==e&&s.push(t[n])})),new a.Qp(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(s)}`)}return e}predictLoop(t,e=32,s=!1){return n.tidy((()=>{const i=this.checkNumSamples(t);if(s)throw new a.EH("Verbose predictLoop() is not implemented yet.");const o=O(i,e),r=this.outputs.map((t=>[]));for(let e=0;e<o.length;++e){n.tidy((()=>{const s=o[e][0],n=o[e][1],i=L(t,s,n),a=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)a.push({key:this.inputs[t],value:i[t]});else a.push({key:this.inputs[0],value:i});const r=new w.RW(a);return(0,w.g7)(this.outputs,r)})).forEach(((t,e)=>r[e].push(t)))}return(0,c.wL)(r.map((t=>n.concat(t,0))))}))}predict(t,e={}){const s=D(t);F(s,this.inputNames,this.feedInputShapes,!1);try{const t=null==e.batchSize?32:e.batchSize;return E(t),this.predictLoop(s,t)}finally{W(s,t)}}predictOnBatch(t){F(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,e,s=!0,i){if(null==this.optimizer_)throw new a.bu("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const o=[];for(let n=0;n<this.feedOutputShapes.length;++n){const t=this.feedOutputShapes[n];this.feedLossFns[n]===l.qp?o.push(t.slice(0,t.length-1).concat([1])):o.push(t)}if(function(t,e){const s=(0,c.Am)(t.map((t=>t.shape[0])));s.sort();const i=(0,c.Am)(e.map((t=>t.shape[0])));if(i.sort(),s.length>1)throw new a.Qp(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(t.map((t=>t.shape)))}`);if(i.length>1)throw new a.Qp(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(e.map((t=>t.shape)))}`);if(s.length>0&&i.length>0&&!n.util.arraysEqual(s,i))throw new a.Qp(`Input Tensors should have the same number of samples as target Tensors. Found ${s[0]} input sample(s) and ${i[0]} target sample(s).`)}(t=Q(t,this.feedInputNames,this.feedInputShapes,!1,"input"),e=Q(e,this.feedOutputNames,o,!1,"target")),function(t,e,s){const n=[l.bt,l.Jc,l.Ol];for(let i=0;i<t.length;++i){const o=t[i],r=e[i],h=s[i];if(null!=r){if(r===l.Ol&&1===o.shape[o.shape.length-1])throw new a.Qp(`You are passing a target array of shape ${o.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==n.indexOf(r)){const t=o.shape.slice(1),e=h.slice(1);for(let s=0;s<t.length;++s){const n=t[s],i=e[s];if(null!=i&&n!==i)throw new a.Qp(`A target Tensor with shape ${o.shape} was passed for an output of shape ${h}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(e,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!==0)throw new a.Qp(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,e]}async standardizeUserData(t,e,s,n,i=!0,o){const[a,r]=this.standardizeUserDataXY(t,e,i,o);if(null!=s)throw new Error("sample weight is not supported yet.");let l=null;if(null!=n){const t=T(n,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await S(r[e],null,t[e]))}return[a,r,l]}testLoop(t,e,s,o=0,r){return n.tidy((()=>{const l=this.checkNumSamples(e,s,r,"steps"),h=[];if(o>0)throw new a.EH("Verbose mode is not implemented yet.");if(null!=r)throw new a.EH("steps mode in testLoop() is not implemented yet");{const o=O(l,s),a=(0,n.tensor1d)((0,f.y1)(0,l));for(let s=0;s<o.length;++s){const r=o[s][0],l=o[s][1],u=i.Dh(a,r,l-r),p=_(e,u),c=t(p);if(0===s)for(let t=0;t<c.length;++t)h.push((0,n.scalar)(0));for(let t=0;t<c.length;++t){const e=c[t];h[t]=n.add(h[t],n.mul(l-r,e))}}for(let t=0;t<h.length;++t)h[t]=n.div(h[t],l)}return h}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let s=0;s<t.length;++s){const n=t[s];let i=n;if((0,c.U9)(t,n)>1){i+=`_${(0,c.U9)(t.slice(0,s),n)}`}e.push(i)}return e}makeTrainFunction(){return t=>{const e=[],s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),o=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),a=[],r=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:s[e]});const r=new w.RW(t),l=(0,w.g7)(this.outputs,r,{training:!0});let h;for(let s=0;s<this.lossFunctions.length;++s){let t=(0,this.lossFunctions[s])(i[s],l[s]);null!=o[s]&&(t=N(t,o[s]));const a=n.mean(t);e.push(a),h=0===s?t:n.add(h,t)}for(let s=0;s<this.metricsTensors.length;++s){let t;if(this.outputs.length>1&&s<this.outputs.length)t=e[s];else{const e=this.metricsTensors[s][0],o=this.metricsTensors[s][1];t=n.mean(e(i[o],l[o]))}n.keep(t),a.push(t)}return h=n.mean(h),this.calculateLosses().forEach((t=>{h=n.add(h,t)})),h}),!0,r)].concat(a)}}makeTestFunction(){this.testFunction=t=>n.tidy((()=>{const e=[];let s;const i=t.slice(0,this.inputs.length),o=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),a=[];for(let t=0;t<this.inputs.length;++t)a.push({key:this.inputs[t],value:i[t]});const r=new w.RW(a),l=(0,w.g7)(this.outputs,r);for(let t=0;t<this.lossFunctions.length;++t){const i=this.lossFunctions[t],a=n.mean(i(o[t],l[t]));s=0===t?a:n.add(s,a),e.push(s)}for(let t=0;t<this.metricsTensors.length;++t){const s=this.metricsTensors[t][0],i=this.metricsTensors[t][1],a=n.mean(s(o[i],l[i]));e.push(a)}return e}))}async fit(t,e,s={}){return z(this,t,e,s)}async fitDataset(t,e){return A(this,t,e)}async trainOnBatch(t,e){const s=await this.standardizeUserData(t,e),i=s[0],o=s[1],a=this.makeTrainFunction()(i.concat(o)),r=[];for(const n of a){const t=await n.data();r.push(t[0])}return n.dispose(a),(0,c.wL)(r)}getNamedWeights(t){const e=[],s=null!=t&&t.trainableOnly,n=s?this.trainableWeights:this.weights,i=this.getWeights(s);for(let o=0;o<n.length;++o)s&&!n[o].trainable||e.push({name:n[o].originalName,tensor:i[o]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const e=n.memory().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=e-n.memory().numTensors}return t}getLossIdentifiers(){let t;if("string"===typeof this.loss)t=(0,c.uc)(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!==typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>(0,c.uc)(t)))}else{const e=Object.keys(this.loss);t={};const s=this.loss;for(const n of e){if("string"!==typeof s[n])throw new Error("Serialization of non-string loss is not supported.");t[n]=(0,c.uc)(s[n])}}return t}getMetricIdentifiers(){if("string"===typeof this.metrics||"function"===typeof this.metrics)return[(0,c.uc)(h.GT(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>(0,c.uc)(h.GT(t))));{const t={};for(const e in this.metrics)t[e]=(0,c.uc)(h.GT(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=(0,g.w)(t.optimizer_config),s=(0,r.i)(e);let n,i;if("string"===typeof t.loss)n=(0,c.Cb)(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>(0,c.Cb)(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=(0,c.Cb)(t.loss[e])}if(Array.isArray(t.metrics))i=t.metrics.map((t=>(0,c.Cb)(t)));else if(null!=t.metrics){i={};for(const e in t.metrics)i[e]=(0,c.Cb)(t.metrics[e])}this.compile({loss:n,metrics:i,optimizer:s})}async save(t,e){if("string"===typeof t){const e=n.io.getSaveHandlers(t);if(0===e.length)throw new a.Qp(`Cannot find any save handlers for URL '${t}'`);if(e.length>1)throw new a.Qp(`Found more than one (${e.length}) save handlers for URL '${t}'`);t=e[0]}if(null==t.save)throw new a.Qp("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const s=await n.io.encodeWeights(this.getNamedWeights(e)),i={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:`TensorFlow.js tfjs-layers v${m.r}`,convertedBy:null};if(null!=e&&e.includeOptimizer&&null!=this.optimizer){i.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:e,specs:o}=await n.io.encodeWeights(await this.optimizer.getWeights(),t);s.specs.push(...o),s.data=n.io.concatenateArrayBuffers([s.data,e])}if(null!=this.userDefinedMetadata){const t=!0;(0,p.Yl)(this.userDefinedMetadata,this.name,t),i.userDefinedMetadata=this.userDefinedMetadata}return i.weightData=s.data,i.weightSpecs=s.specs,t.save(i)}setUserDefinedMetadata(t){(0,p.Yl)(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}B.className="Model",n.serialization.registerClass(B);class U extends B{}U.className="Functional",n.serialization.registerClass(U)},78825:function(t,e,s){s.d(e,{m:function(){return r},p:function(){return l}});var n=s(9495),i=s(91686),o=s(15841),a=s(79730);class r extends a.Wd{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:(0,i.v)("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new o.Qp("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new o.Qp("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new o.Qp("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const s=t.dtype||"float32";this.batchInputShape=e,this.dtype=s,this.inputSpec=[{shape:e}];const n=new a.Ar(this.dtype,this.batchInputShape,this,[],{},this.name);n.nodeIndex=0,n.tensorIndex=0,new a.bP({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[n],outputTensors:[n],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new o.Qp(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function l(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new o.Qp("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let s=t.dtype;null==s&&(s="float32");return new r({batchInputShape:e,name:t.name,dtype:s,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}r.className="InputLayer",n.serialization.registerClass(r)},79730:function(t,e,s){s.d(e,{Ar:function(){return d},Wd:function(){return y},X6:function(){return w},bP:function(){return g},eO:function(){return c}});var n=s(9495),i=s(91686),o=s(39459),a=s(15841),r=s(59351),l=s(44813),h=s(63057),u=s(54390),p=s(71765);class c{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class d{constructor(t,e,s,n,a,r,l){this.dtype=t,this.shape=e,this.sourceLayer=s,this.inputs=n,this.callArgs=a,this.outputTensorIndex=l,this.id=(0,i.j)(),null!=r&&(this.originalName=(0,o.BC)(r),this.name=(0,o.Uc)(this.originalName)),this.rank=e.length}}let f=0;class g{constructor(t,e){this.callArgs=e,this.id=f++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const s of t.inboundLayers)null!=s&&s.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let m=0;class y extends n.serialization.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=m++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=l.uc(t)+"_"+(0,i.v)(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let s=null;null!=t.batchSize&&(s=t.batchSize),e=[s].concat(t.inputShape)}this.batchInputShape=e;let s=t.dtype;null==s&&(s=t.inputDType),null==s&&(s="float32"),this.dtype=s}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new a.bu(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new a.Qp(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return l.wL(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return l.wL(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new a.l7(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);if(0===this.inboundNodes.length)throw new a.l7(`Layer ${this.name} is not connected, no input to return.`);return l.wL(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new a.l7(`Layer ${this.name} has no inbound nodes.`);if(this.inboundNodes.length>1)throw new a.l7(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);return l.wL(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=l.st(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=l.st(this.inputSpec);if(t.length!==e.length)throw new a.Qp(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: ${t}`);for(let s=0;s<t.length;s++){const n=t[s],i=e[s];if(null==i)continue;const o=n.rank;if(null!=i.ndim&&o!==i.ndim)throw new a.Qp(`Input ${s} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${o}`);if(null!=i.maxNDim&&o>i.maxNDim)throw new a.Qp(`Input ${s} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${o}`);if(null!=i.minNDim&&o<i.minNDim)throw new a.Qp(`Input ${s} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${o}.`);if(null!=i.dtype&&n.dtype!==i.dtype)throw new a.Qp(`Input ${s} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${n.dtype}.`);if(i.axes){const t=n.shape;for(const e in i.axes){const n=Number(e),o=i.axes[e],r=n>=0?t[n]:t[t.length+n];if(null!=o&&-1===[o,null].indexOf(r))throw new a.Qp(`Input ${s} is incompatible with layer ${this.name}: expected axis ${n} of input shape to have value ${o} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],o=n.shape[t];if(null!=e&&null!=o&&e!==o)throw new a.Qp(`Input ${s} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${n.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const s=l.st(t);let n=!0;for(const o of s)if(!(o instanceof d)){n=!1;break}let i=!0;for(const o of s)if(o instanceof d){i=!1;break}if(n===i)throw new a.Qp("Arguments to apply() must be all SymbolicTensors or all Tensors");return(0,o.IU)(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const s of l.st(t))e.push(s.shape);this.build(l.wL(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let n=this.call(t,e);const i=l.st(n),o=[];for(let t of i)-1!==s.indexOf(t)&&(t=t.clone()),o.push(t);if(n=l.wL(o),null!=this.activityRegularizer)throw new a.EH("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return n}{const s=function(t){t=l.st(t);const e=[];for(const s of t)e.push(s.shape);return l.wL(e)}(t),n=this.computeOutputShape(s);let i;const o="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?s[0]:s),i=null!=n&&n.length>0&&Array.isArray(n[0])?n.map(((s,n)=>new d(o,s,this,l.st(t),e,this.name,n))):new d(o,n,this,l.st(t),e,this.name),this.addInboundNode(t,i,null,null,s,n,e),this._refCount++,null!=this.activityRegularizer)throw new a.EH("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length);else{let e=!1;this.batchInputShape.forEach(((s,n)=>{null!=s&&null!=t[n]&&t[n]!==s&&(e=!0)}))}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new a.l7(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const s=JSON.stringify(e.outputShapes);-1===t.indexOf(s)&&t.push(s)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new a.l7(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new a.bu(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return u.Y(this.weights)}build(t){this.built=!0}getWeights(t=!1){return(0,p.ex)(t?this.trainableWeights:this.weights)}setWeights(t){(0,n.tidy)((()=>{const e=this.weights;if(e.length!==t.length)throw new a.Qp(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${e.length} weights. Provided weights: ${t}...`);if(0===e.length)return;const s=[],i=(0,p.ex)(e);for(let o=0;o<i.length;++o){const r=i[o],l=e[o],h=t[o];if(!n.util.arraysEqual(r.shape,h.shape))throw new a.Qp(`Layer weight shape ${r.shape} not compatible with provided weight shape ${h.shape}`);s.push([l,h])}(0,p.UM)(s)}))}addWeight(t,e,s,n,i,o,l){if(-1!==this._addedWeightNames.indexOf(t))throw new a.Qp(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==s&&(s="float32"),this.fastWeightInitDuringBuild&&(n=(0,r.Fe)("zeros"));const h=n.apply(e,s),u=new p.eR(h,s,t,o,l);return h.dispose(),null!=i&&this.addLoss((()=>i.apply(u.read()))),null==o&&(o=!0),o?this._trainableWeights.push(u):this._nonTrainableWeights.push(u),u}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=l.st(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach((t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return e}addInboundNode(t,e,s,n,i,o,a=null){const r=l.st(t);e=l.st(e),s=l.st(s),n=l.st(n),i=h.FS(i),o=h.FS(o);const u=[],p=[],c=[];for(const l of r)u.push(l.sourceLayer),p.push(l.nodeIndex),c.push(l.tensorIndex);new g({outboundLayer:this,inboundLayers:u,nodeIndices:p,tensorIndices:c,inputTensors:r,outputTensors:e,inputMasks:s,outputMasks:n,inputShapes:i,outputShapes:o},a);for(let l=0;l<e.length;l++)e[l].sourceLayer=this,e[l].nodeIndex=this.inboundNodes.length-1,e[l].tensorIndex=l}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0===--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function w(t,e,s){if((null==e||null!=s&&s>0)&&(e=t.sourceLayer,s=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[s];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let s=0;s<t.inboundLayers.length;s++){const n=w(t.inputTensors[s],t.inboundLayers[s],t.nodeIndices[s]);for(const t of n)-1===e.indexOf(t)&&e.push(t)}return e}}}}}]);
//# sourceMappingURL=stylist-vendors-f9a8c152.851b144f8e08293d3f8b.js.map