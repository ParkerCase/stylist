{"version":3,"file":"stylist-vendors-f6e5d052.4e8f92d5acdb2e409a2a.js","mappings":"oJAEA,MAAMA,EAAU,O,wBC4BT,SAASC,EAAQC,GACpB,IAAIC,EAAUD,EAAME,OAChBC,EAAO,EACPC,EAAQ,EAEZ,KAAOH,EAAU,GAEbG,EAASC,KAAKC,SAAWL,EAAW,EAEpCA,IAEAE,EAAOH,EAAMC,GACbD,EAAMC,GAAWD,EAAMI,GACvBJ,EAAMI,GAASD,CAEvB,CAiBO,SAASI,EAEhBP,EAEAQ,GACI,GAAIR,EAAME,SAAWM,EAAON,OACxB,MAAM,IAAIO,MACN,yEAA0BT,EAAME,iCACLM,EAAON,UAE1C,IACIC,EAAMO,EADNT,EAAUD,EAAME,OAEhBE,EAAQ,EAEZ,KAAOH,EAAU,GAEbG,EAASC,KAAKC,SAAWL,EAAW,EAEpCA,IAEAE,EAAOH,EAAMC,GACbS,EAAQF,EAAOP,GACfD,EAAMC,GAAWD,EAAMI,GACvBI,EAAOP,GAAWO,EAAOJ,GACzBJ,EAAMI,GAASD,EACfK,EAAOJ,GAASM,CAExB,CAEO,SAASC,EAAMC,EAAKC,EAAGC,GAC1B,OAAOT,KAAKS,IAAIF,EAAKP,KAAKO,IAAIC,EAAGC,GACrC,CACO,SAASC,EAAkBC,GAC9B,OAAOA,EAAM,IAAM,EAAIA,EAAMA,EAAM,CACvC,CACO,SAASC,EAAIC,GAChB,IAAID,EAAM,EACV,IAAK,IAAIE,EAAI,EAAGA,EAAID,EAAIhB,OAAQiB,IAC5BF,GAAOC,EAAIC,GAEf,OAAOF,CACX,CAQO,SAASG,EAAYC,EAAGC,GAC3B,MAAMC,EAAIlB,KAAKC,SACf,OAAQgB,EAAIC,GAAM,EAAIA,GAAKF,CAC/B,CAEO,SAASG,EAAYH,EAAGC,GAC3B,IAAIG,EAAS,EACb,IAAK,IAAIN,EAAI,EAAGA,EAAIE,EAAEnB,OAAQiB,IAAK,CAC/B,MAAMO,EAAOC,OAAON,EAAEF,IAAMQ,OAAOL,EAAEH,IACrCM,GAAUC,EAAOA,CACrB,CACA,OAAOD,CACX,CAgBO,SAASG,EAAOC,EAAMC,GACzB,IAAKD,EACD,MAAM,IAAIpB,MAAqB,kBAARqB,EAAmBA,EAAMA,IAExD,CACO,SAASC,EAAkBC,EAAQC,EAAQC,EAAqB,IACnEN,EAAOO,EAAYH,EAAQC,IAAS,IAAMC,EAAqB,WAAWF,SAAcC,gBAC5F,CACO,SAASG,EAAcf,GAC1BO,EAAY,MAALP,GAAW,IAAM,iEAC5B,CAoBO,SAASgB,EAAQnB,EAAKO,EAAS,GAAIa,GAAiB,GAIvD,GAHc,MAAVb,IACAA,EAAS,IAETc,MAAMC,QAAQtB,IAAQuB,EAAavB,KAASoB,EAC5C,IAAK,IAAInB,EAAI,EAAGA,EAAID,EAAIhB,SAAUiB,EAC9BkB,EAAQnB,EAAIC,GAAIM,EAAQa,QAI5Bb,EAAOiB,KAAKxB,GAEhB,OAAOO,CACX,CAYO,SAASkB,EAAcC,GAC1B,GAAqB,IAAjBA,EAAM1C,OAEN,OAAO,EAEX,IAAI2C,EAAOD,EAAM,GACjB,IAAK,IAAIzB,EAAI,EAAGA,EAAIyB,EAAM1C,OAAQiB,IAC9B0B,GAAQD,EAAMzB,GAElB,OAAO0B,CACX,CACO,SAASC,EAAcF,GAC1B,OAAwB,IAAjBA,EAAM1C,MACjB,CACO,SAASiC,EAAYY,EAAIC,GAC5B,GAAID,IAAOC,EACP,OAAO,EAEX,GAAU,MAAND,GAAoB,MAANC,EACd,OAAO,EAEX,GAAID,EAAG7C,SAAW8C,EAAG9C,OACjB,OAAO,EAEX,IAAK,IAAIiB,EAAI,EAAGA,EAAI4B,EAAG7C,OAAQiB,IAC3B,GAAI4B,EAAG5B,KAAO6B,EAAG7B,GACb,OAAO,EAGf,OAAO,CACX,CACO,SAAS8B,EAAM5B,GAClB,OAAOA,EAAI,IAAM,CACrB,CACO,SAAS6B,EAAKrC,GAEjB,GAAiB,MAAbR,KAAK6C,KAEL,OAAO7C,KAAK6C,KAAKrC,GAErB,GAAIA,IAAMsC,IACN,OAAO,EAEN,GAAItC,KAAOsC,IACZ,OAAQ,EAEP,CACD,MAAMC,EAAM/C,KAAKgD,IAAI,EAAIxC,GACzB,OAAQuC,EAAM,IAAMA,EAAM,EAC9B,CACJ,CACO,SAASE,EAAoBT,GAChC,MAAMU,EAAQlD,KAAKmD,KAAKnD,KAAKoD,KAAKZ,IAClC,MAAO,CAACU,EAAOlD,KAAKmD,KAAKX,EAAOU,GACpC,CAaO,SAASG,EAAsBC,GAClC,MAAMC,EAAkB,IAAIC,YAAYF,GACxC,IAAK,IAAIxC,EAAI,EAAGA,EAAIwC,IAAKxC,EACrByC,EAAgBzC,GAAKA,EAGzB,OADApB,EAAQ6D,GACDA,CACX,CACO,SAASE,EAASzC,EAAGwB,GACxB,OAAIA,GAAQxB,EAAEnB,OACHmB,EAEJA,EAAI,IAAI0C,OAAOlB,EAAOxB,EAAEnB,OACnC,CACO,SAAS8D,EAAYC,EAASC,EAAWjE,GAAY,EAAGkE,GAC3D,OAAO,IAAIC,SAAQ,CAACC,EAASC,KACzB,IAAIC,EAAW,EACf,MAAMC,EAAQ,KACV,GAAIP,IAEA,YADAI,IAGJE,IACA,MAAME,EAAcP,EAAQK,GACV,MAAdJ,GAAsBI,GAAYJ,EAClCG,IAGJI,WAAWF,EAAOC,EAAY,EAElCD,GAAO,GAEf,CAUO,SAASG,EAAuB/B,EAAOC,GAC1C,IAAI+B,EAAY,EACZC,GAAe,EACnB,IAAK,IAAI1D,EAAI,EAAGA,EAAIyB,EAAM1C,SAAUiB,EAChC,GAAIyB,EAAMzB,IAAM,EACZyD,GAAahC,EAAMzB,QAElB,IAAkB,IAAdyB,EAAMzB,GAAW,CACtB,IAAqB,IAAjB0D,EACA,MAAMpE,MACF,yDAAmBoE,aAAuB1D,KAElD0D,EAAc1D,CAClB,MACK,GAAIyB,EAAMzB,GAAK,EAChB,MAAMV,MAAM,gCAAgCmC,EAAMzB,aAAaA,KAGvE,IAAqB,IAAjB0D,EAAoB,CACpB,GAAIhC,EAAO,GAAKA,IAAS+B,EACrB,MAAMnE,MAAM,QAAQoC,sCAAyCD,KAEjE,OAAOA,CACX,CACA,GAAkB,IAAdgC,EACA,MAAMnE,MAAM,qCAAqCmC,gCAGrD,GAAIC,EAAO+B,IAAc,EACrB,MAAMnE,MACF,wDAAOoC,OAAU+B,KAEzB,MAAME,EAAWlC,EAAMmC,QAEvB,OADAD,EAASD,GAAehC,EAAO+B,EACxBE,CACX,CACO,SAASE,EAAeC,EAAMrC,GACjC,MAAMsC,EAAOtC,EAAM1C,OAUnB,OANA0B,GAFAqD,EAAe,MAARA,EAAerC,EAAMuC,KAAI,CAACC,EAAGjE,IAAMA,IAAK,GAAGkE,OAAOJ,IAE7CK,OAAMC,GAAMA,IAAOL,GAAQK,EAAKL,KAAO,IAAM,+CAA+CA,MAASA,mBACjGD,MAEhBrD,EAAOqD,EAAKK,OAAMC,GAAMtC,EAAMsC,MAAM,IAChC,0DAAYN,MAETA,EAAKE,KAAI9D,GAAKA,EAAI,EAAI6D,EAAO7D,EAAIA,GAC5C,CAEO,SAASmE,EAAa5C,EAAOqC,GAChC,MAAMH,EAAW,GACXW,EAAW,GACXC,EAAuB,MAART,GAAgB1C,MAAMC,QAAQyC,IAAyB,IAAhBA,EAAK/E,OAC3DyF,EAAgB,MAARV,GAAgBS,EAC1B,KACAV,EAAeC,EAAMrC,GAAOgD,OAChC,IAAIC,EAAI,EACR,IAAK,IAAI1E,EAAI,EAAGA,EAAIyB,EAAM1C,SAAUiB,EAAG,CACnC,GAAY,MAARwE,EAAc,CACd,GAAIA,EAAKE,KAAO1E,GAAkB,IAAbyB,EAAMzB,GACvB,MAAM,IAAIV,MAAM,sBAAsBU,oBAAoByB,EAAMzB,iBAEpD,MAAXwE,EAAKE,IAAcF,EAAKE,GAAK1E,IAAmB,IAAbyB,EAAMzB,KAC1C2D,EAASpC,KAAKE,EAAMzB,IACpBsE,EAAS/C,KAAKvB,IAEdwE,EAAKE,IAAM1E,GACX0E,GAER,CACiB,IAAbjD,EAAMzB,KACN2D,EAASpC,KAAKE,EAAMzB,IACpBsE,EAAS/C,KAAKvB,GAEtB,CACA,MAAO,CAAE2D,WAAUW,WACvB,CACO,SAASK,EAAuBC,EAAOlD,GAC1C,IAAImD,EAAS,KACb,GAAa,MAATD,GAA2B,YAAVA,EACjBC,EAAS,IAAIC,aAAapD,QAEzB,GAAc,UAAVkD,EACLC,EAAS,IAAIE,WAAWrD,OAEvB,IAAc,SAAVkD,EAIL,MAAM,IAAItF,MAAM,qBAAqBsF,KAHrCC,EAAS,IAAIG,WAAWtD,EAI5B,CACA,OAAOmD,CACX,CACO,SAASI,EAAkBL,EAAOlD,GACrC,IAAImD,EAAS,KACb,GAAa,MAATD,GAA2B,YAAVA,EACjBC,EAAS,IAAIC,aAAapD,QAEzB,GAAc,UAAVkD,EACLC,EAAS,IAAIE,WAAWrD,QAEvB,GAAc,SAAVkD,EACLC,EAAS,IAAIG,WAAWtD,OAEvB,IAAc,WAAVkD,EAIL,MAAM,IAAItF,MAAM,qBAAqBsF,KAHrCC,EAAS,IAAIzD,MAAMM,EAIvB,CACA,OAAOmD,CACX,CACO,SAASK,EAAyBC,EAAMP,GAC3C,IAAK,IAAI5E,EAAI,EAAGA,EAAImF,EAAKpG,OAAQiB,IAAK,CAClC,MAAMoF,EAAMD,EAAKnF,GACjB,GAAIqF,MAAMD,KAASE,SAASF,GACxB,MAAM9F,MAAM,oBAAoBsF,6BAAiCQ,KAEzE,CACJ,CAEO,SAASG,EAAaX,GACzB,MAAiB,SAAVA,GAA8B,cAAVA,GAAmC,YAAVA,GACtC,UAAVA,GAA+B,WAAVA,CAC7B,CAKO,SAASY,EAAgBC,EAASC,GACrC,MAAgB,cAAZA,KAGY,YAAZA,GAAqC,cAAZD,MAGb,UAAZC,GAAmC,YAAZD,GAAqC,cAAZA,KAGpC,SAAZC,GAAkC,SAAZD,IAI9B,CACO,SAASnE,EAAapB,GACzB,OAAOA,aAAa4E,cAAgB5E,aAAa6E,YAC7C7E,aAAa8E,UACrB,CACO,SAASW,EAAgBf,GAC5B,GAAc,YAAVA,GAAiC,UAAVA,EACvB,OAAO,EAEN,GAAc,cAAVA,EACL,OAAO,EAEN,GAAc,SAAVA,EACL,OAAO,EAGP,MAAM,IAAItF,MAAM,iBAAiBsF,IAEzC,CAOO,SAASgB,EAAqB7F,GACjC,GAAW,MAAPA,EACA,OAAO,EAEX,IAAI8F,EAAQ,EAEZ,OADA9F,EAAI+F,SAAQpG,GAAKmG,GAASnG,EAAEX,SACrB8G,CACX,CAEO,SAASE,EAASC,GACrB,MAAwB,kBAAVA,GAAsBA,aAAiBC,MACzD,CACO,SAASC,EAAUF,GACtB,MAAwB,mBAAVA,CAClB,CACO,SAASG,EAASH,GACrB,MAAwB,kBAAVA,CAClB,CACO,SAASI,EAAWvB,GACvB,OAAIzD,MAAMC,QAAQwD,GACPuB,EAAWvB,EAAO,IAEzBA,aAAkBC,aACX,UAEFD,aAAkBE,YAAcF,aAAkBG,WAChD,QAEFmB,EAAStB,GACP,UAEFkB,EAASlB,GACP,SAEFqB,EAAUrB,GACR,OAEJ,SACX,CACO,SAASwB,EAAWC,GACvB,SAAUA,GAAKA,EAAEC,aAAeD,EAAEE,MAAQF,EAAEG,MAChD,CACO,SAASC,EAAehF,EAAMiF,GACjC,IAAK,IAAI3G,EAAI2G,EAAO3G,EAAI0B,IAAQ1B,EAC5B,GAAI0B,EAAO1B,IAAM,EACb,OAAOA,EAGf,OAAO0B,CACX,CACO,SAASkF,EAAenF,GAC3B,MAAMsC,EAAOtC,EAAM1C,OACnB,GAAIgF,EAAO,EACP,MAAO,GAIX,MAAM8C,EAAU,IAAIzF,MAAM2C,EAAO,GACjC8C,EAAQ9C,EAAO,GAAKtC,EAAMsC,EAAO,GACjC,IAAK,IAAI/D,EAAI+D,EAAO,EAAG/D,GAAK,IAAKA,EAC7B6G,EAAQ7G,GAAK6G,EAAQ7G,EAAI,GAAKyB,EAAMzB,EAAI,GAE5C,OAAO6G,CACX,CACA,SAASC,EAAkBC,EAAQtF,EAAOvB,EAAG8G,GAAY,GACrD,MAAMC,EAAM,IAAI7F,MAChB,GAAqB,IAAjBK,EAAM1C,OAAc,CACpB,MAAMmI,EAAIzF,EAAM,IAAMuF,EAAY,EAAI,GACtC,IAAK,IAAIhH,EAAI,EAAGA,EAAIkH,EAAGlH,IACnBiH,EAAIjH,GAAKE,EAAE6G,EAAS/G,EAE5B,KACK,CACD,MAAMkH,EAAIzF,EAAM,GACV0F,EAAO1F,EAAMmC,MAAM,GACnBwD,EAAMD,EAAKE,QAAO,CAACC,EAAKC,IAAMD,EAAMC,KAAMP,EAAY,EAAI,GAChE,IAAK,IAAIhH,EAAI,EAAGA,EAAIkH,EAAGlH,IACnBiH,EAAIjH,GAAK8G,EAAkBC,EAAS/G,EAAIoH,EAAKD,EAAMjH,EAAG8G,EAE9D,CACA,OAAOC,CACX,CAEO,SAASO,EAAc/F,EAAOvB,EAAG8G,GAAY,GAChD,GAAqB,IAAjBvF,EAAM1C,OAEN,OAAOmB,EAAE,GAEb,MAAMwB,EAAOD,EAAM4F,QAAO,CAACC,EAAKC,IAAMD,EAAMC,KAAMP,EAAY,EAAI,GAClE,GAAa,IAATtF,EAEA,MAAO,GAEX,GAAIA,IAASxB,EAAEnB,OACX,MAAM,IAAIO,MAAM,IAAImC,oCAAwCvB,EAAEnB,SAASiI,EAAY,wBAA0B,OAEjH,OAAOF,EAAkB,EAAGrF,EAAOvB,EAAG8G,EAC1C,CACO,SAASS,EAAmB/F,EAAMkD,GACrC,MAAM/F,EAAQ6I,EAAoBhG,EAAMkD,GACxC,IAAK,IAAI5E,EAAI,EAAGA,EAAInB,EAAME,OAAQiB,IAC9BnB,EAAMmB,GAAK,EAEf,OAAOnB,CACX,CACO,SAAS6I,EAAoBhG,EAAMkD,GACtC,GAAa,MAATA,GAA2B,YAAVA,GAAiC,cAAVA,EACxC,OAAO,IAAIE,aAAapD,GAEvB,GAAc,UAAVkD,EACL,OAAO,IAAIG,WAAWrD,GAErB,GAAc,SAAVkD,EACL,OAAO,IAAII,WAAWtD,GAGtB,MAAM,IAAIpC,MAAM,qBAAqBsF,IAE7C,CAMO,SAAS+C,EAA0BlG,EAAOmD,GAC7C,MAAMlD,EAAOD,EAAM4F,QAAO,CAACO,EAAMC,IAASD,EAAOC,GAAM,GACvD,GAAa,MAATjD,GAA2B,YAAVA,EACjB,OAAO4C,EAAc/F,EAAO,IAAIqD,aAAapD,IAE5C,GAAc,UAAVkD,EACL,OAAO4C,EAAc/F,EAAO,IAAIsD,WAAWrD,IAE1C,GAAc,SAAVkD,EACL,OAAO4C,EAAc/F,EAAO,IAAIuD,WAAWtD,IAG3C,MAAM,IAAIpC,MAAM,qBAAqBsF,IAE7C,CACO,SAASkD,EAAmCrG,GAC/CA,EAAMqE,SAAQiC,IACVtH,EAAOD,OAAOwH,UAAUD,IAAYA,GAAW,GAAG,IAC9C,0EAAUtG,OAAU,GAEhC,CASO,SAASwG,EAAWC,EAAMnE,EAAM8C,GACnC,GAAa,IAAT9C,EACA,OAAO,EAEN,GAAa,IAATA,EACL,OAAOmE,EAAK,GAEhB,IAAIjJ,EAAQiJ,EAAKA,EAAKnJ,OAAS,GAC/B,IAAK,IAAIiB,EAAI,EAAGA,EAAIkI,EAAKnJ,OAAS,IAAKiB,EACnCf,GAAS4H,EAAQ7G,GAAKkI,EAAKlI,GAE/B,OAAOf,CACX,CASO,SAASkJ,EAAWlJ,EAAO8E,EAAM8C,GACpC,GAAa,IAAT9C,EACA,MAAO,GAEN,GAAa,IAATA,EACL,MAAO,CAAC9E,GAEZ,MAAMiJ,EAAO,IAAI9G,MAAM2C,GACvB,IAAK,IAAI/D,EAAI,EAAGA,EAAIkI,EAAKnJ,OAAS,IAAKiB,EACnCkI,EAAKlI,GAAKd,KAAKkJ,MAAMnJ,EAAQ4H,EAAQ7G,IACrCf,GAASiJ,EAAKlI,GAAK6G,EAAQ7G,GAG/B,OADAkI,EAAKA,EAAKnJ,OAAS,GAAKE,EACjBiJ,CACX,CAMO,SAASG,EAAUC,GAOtB,OAAOA,GAAUA,EAAOC,MAA+B,oBAAhBD,EAAOC,IAClD,C,gnCClpBO,IAAIC,EAYPC,EAOAC,EAOAC,EAOAC,E,iFAhCJ,SAAWJ,GACPA,EAAS,GAAI,KACbA,EAAS,GAAI,KACbA,EAAS,GAAI,KACbA,EAAS,GAAI,KACbA,EAAS,GAAI,KACbA,EAAS,GAAI,KACbA,EAAS,GAAI,IAChB,CARD,CAQGA,IAASA,EAAO,CAAC,IAIpB,SAAWC,GACPA,EAA2B,QAAI,UAC/BA,EAAyB,MAAI,QAC7BA,EAAwB,KAAI,QAC5BA,EAA6B,UAAI,WACpC,CALD,CAKGA,IAAsBA,EAAoB,CAAC,IAE9C,SAAWC,GACPA,EAA0B,QAAI,UAC9BA,EAAwB,MAAI,QAC5BA,EAAuB,KAAI,OAC3BA,EAA4B,UAAI,WACnC,CALD,CAKGA,IAAqBA,EAAmB,CAAC,IAE5C,SAAWC,GACPA,EAA6B,QAAI,UACjCA,EAA2B,MAAI,UAC/BA,EAA0B,KAAI,UAC9BA,EAA+B,UAAI,WACtC,CALD,CAKGA,IAAwBA,EAAsB,CAAC,IAElD,SAAWC,GACPA,EAA+B,QAAI,YACnCA,EAA6B,MAAI,YACjCA,EAA4B,KAAI,YAChCA,EAAiC,UAAI,WACxC,CALD,CAKGA,IAA0BA,EAAwB,CAAC,IACtD,MAAMC,EAAgB,CAClB,QAAWF,EACX,MAASF,EACT,KAAQC,EACR,UAAaE,GAEV,SAASE,EAAWC,EAAOC,GAC9B,GAAc,WAAVD,GAAgC,WAAVC,EAAoB,CAC1C,GAAc,WAAVD,GAAgC,WAAVC,EACtB,MAAO,SAEX,MAAM,IAAI1J,MAAM,kBAAkByJ,UAAcC,IACpD,CACA,OAAOH,EAAcE,GAAOC,EAChC,CAEO,SAASC,EAAWC,GACvB,OAAOJ,EAAWI,EAAM,QAC5B,C,mJChDC,IAAmB,EAAAC,EAAc,IAAmB,IACjD,IAAkB,IAAiB,IAChC,MAAMC,EAAQ,CACjBC,IAAK,IAAsBA,IAC3BC,SAAU,IAAsBA,SAChCC,SAAU,IAAsBA,SAChCC,QAAS,IAAsBA,QAC/BC,QAAS,IAAsBA,QAC/BC,OAAQ,IAAsBA,OAC9BC,KAAM,IAAsBA,K,6DCkxBrB,E,oBCzwBX,SAASC,EAAgBC,EAAOC,EAAOC,EAAO,IAAIC,IAAOC,EAAc,IAAIC,KACvE,GAAa,MAATL,EACA,OAAO,KAEX,GAAII,EAAYE,IAAIN,GAChB,MAAM,IAAIvK,MAAM,0CAEpB,GAAIyK,EAAKI,IAAIN,GACT,OAAOE,EAAKK,IAAIP,GAEpB,MAAMvJ,EAASwJ,EAAMD,GACrB,GAAIvJ,EAAO+J,SAA4B,OAAjB/J,EAAO0F,MACzB,MAAM,IAAI1G,MAAM,qEAEpB,GAAKgB,EAAO+J,QAIP,IAAI,EAAWR,GAAQ,CAExB,MAAMS,EAAiBlJ,MAAMC,QAAQwI,GAAS,GAAK,CAAC,EACpDI,EAAYM,IAAIV,GAChB,IAAK,MAAMW,KAAKX,EAAO,CACnB,MACMY,EAAcb,EADNC,EAAMW,GACuBV,EAAOC,EAAME,GACxDK,EAAeE,GAAKC,CACxB,CAEA,OADAR,EAAYS,OAAOb,GACZS,CACX,CAEI,MAAM,IAAIhL,MAAM,yCAAyCuK,IAC7D,CAhBI,OADAE,EAAKY,IAAId,EAAOvJ,EAAO0F,OAChB1F,EAAO0F,KAiBtB,CAyBO,SAAS4E,EAAQC,EAAQC,EAAQC,GACpC,OAAOC,EAAgBH,EAAQC,EACnC,CAKA,SAASE,EAAgBH,EAAQC,EAAOb,EAAc,IAAIC,KAGtD,MAAML,EAAQgB,EAAO,GACrB,GAAIZ,EAAYE,IAAIN,GAChB,MAAM,IAAIvK,MAAM,0CAEpB,MAAMgB,EAASwK,EAAMD,GACrB,GAAIvK,EAAO+J,SAA4B,OAAjB/J,EAAO0F,MACzB,MAAM,IAAI1G,MAAM,qEAEpB,GAAKgB,EAAO+J,QAGP,IAAI,EAAWR,GAAQ,CAExB,MAAMS,EAAiBlJ,MAAMC,QAAQwI,GAAS,GAAK,CAAC,EACpDI,EAAYM,IAAIV,GAChB,IAAK,MAAMW,KAAKX,EAAO,CACnB,MACMY,EAAcO,EADHH,EAAO7G,KAAItE,GAAKA,EAAE8K,KACWM,EAAOb,GACrDK,EAAeE,GAAKC,CACxB,CAEA,OADAR,EAAYS,OAAOb,GACZS,CACX,CAEI,MAAM,IAAIhL,MAAM,yCAAyCuK,IAC7D,CAhBI,OAAOvJ,EAAO0F,KAiBtB,CAEO,SAAS+E,EAAUrL,GACtB,OAAU,OAANA,EACO,KAGP,EAAWA,EAAE,IACN,CAAEsG,MAAO,KAAMqE,SAAS,GAGxB,CAAErE,MAAOtG,EAAG2K,SAAS,EAEpC,CAkDO,SAAS,EAAWY,GACvB,OAAc,MAAPA,IAAiBC,YAAYC,OAAOF,KACtC7J,MAAMC,QAAQ4J,IACK,kBAARA,KAAsBA,aAAe,UACzD,CC1LO,SAASG,EAAUC,GACtB,ODgBOzB,EChBQyB,EAAWC,EAC9B,CAEA,SAASA,EAAcC,GACnB,OAAIA,aAAgB,SACT,CAAGvF,MAAOuF,EAAKC,QAASnB,SAAS,GAEnC,EAAWkB,GACT,CAAEvF,MAAO,KAAMqE,SAAS,GAGxB,CAAErE,MAAOuF,EAAMlB,SAAS,EAEvC,CCbO,MAAMoB,EAKT,WAAAlF,CAAYmF,GAOR,GANAC,KAAKD,SAAWA,EAIhBC,KAAKC,MAAQ,EACbD,KAAKE,IAAM,EACK,MAAZH,EACA,MAAM,IAAII,WAAW,mDAEzB,GAAIJ,EAAW,EACX,MAAM,IAAII,WAAW,6CAEzBH,KAAKI,KAAO,IAAI3K,MAAMsK,GACtBC,KAAKK,gBAAkB,EAAIN,CAC/B,CAIA,IAAAO,CAAKhN,GAED,KAAOA,EAAQ,GACXA,GAAS0M,KAAKK,gBAElB,OAAO/M,EAAQ0M,KAAKK,eACxB,CACA,GAAA5B,CAAInL,GACA,GAAIA,EAAQ,EACR,MAAM,IAAI6M,WAAW,uCAEzB,OAAOH,KAAKI,KAAK9M,EAAQ0M,KAAKD,SAClC,CACA,GAAAf,CAAI1L,EAAO+G,GACP,GAAI/G,EAAQ,EACR,MAAM,IAAI6M,WAAW,uCAEzBH,KAAKI,KAAK9M,EAAQ0M,KAAKD,UAAY1F,CACvC,CAIA,MAAAjH,GACI,IAAIA,EAAS4M,KAAKE,IAAMF,KAAKC,MAI7B,OAHI7M,EAAS,IACTA,EAAS4M,KAAKK,gBAAkBjN,GAE7BA,CACX,CAMA,MAAAmN,GACI,OAAOP,KAAK5M,WAAa4M,KAAKD,QAClC,CAMA,OAAAS,GACI,OAAyB,IAAlBR,KAAK5M,QAChB,CAIA,IAAAwC,CAAKyE,GACD,GAAI2F,KAAKO,SACL,MAAM,IAAIJ,WAAW,wBAEzBH,KAAKhB,IAAIgB,KAAKE,IAAK7F,GACnB2F,KAAKE,IAAMF,KAAKM,KAAKN,KAAKE,IAAM,EACpC,CAIA,OAAAO,CAAQvH,GACJ,IAAK,MAAMmB,KAASnB,EAChB8G,KAAKpK,KAAKyE,EAElB,CAIA,GAAAqG,GACI,GAAIV,KAAKQ,UACL,MAAM,IAAIL,WAAW,yBAEzBH,KAAKE,IAAMF,KAAKM,KAAKN,KAAKE,IAAM,GAChC,MAAMvL,EAASqL,KAAKvB,IAAIuB,KAAKE,KAE7B,OADAF,KAAKhB,IAAIgB,KAAKE,SAAKS,GACZhM,CACX,CAIA,OAAAiM,CAAQvG,GACJ,GAAI2F,KAAKO,SACL,MAAM,IAAIJ,WAAW,wBAEzBH,KAAKC,MAAQD,KAAKM,KAAKN,KAAKC,MAAQ,GACpCD,KAAKhB,IAAIgB,KAAKC,MAAO5F,EACzB,CAIA,KAAAwG,GACI,GAAIb,KAAKQ,UACL,MAAM,IAAIL,WAAW,yBAEzB,MAAMxL,EAASqL,KAAKvB,IAAIuB,KAAKC,OAG7B,OAFAD,KAAKhB,IAAIgB,KAAKC,WAAOU,GACrBX,KAAKC,MAAQD,KAAKM,KAAKN,KAAKC,MAAQ,GAC7BtL,CACX,CAUA,aAAAmM,CAAcC,GACV,GAAIf,KAAKQ,UACL,MAAM,IAAIL,WAAW,yBAEzB,MAAM7M,EAAQ0M,KAAKM,KAAKN,KAAKC,MAAQc,GAC/BpM,EAASqL,KAAKvB,IAAInL,GAExB,OADA0M,KAAKhB,IAAI1L,EAAO0M,KAAKU,OACd/L,CACX,EC5IG,MAAMqM,UAA0BlB,EAInC,WAAAlF,GACIqG,MAAMD,EAAkBE,iBAC5B,CACA,MAAAX,GACI,OAAO,CACX,CACA,IAAA3K,CAAKyE,GACG4G,MAAMV,UACNP,KAAKmB,SAETF,MAAMrL,KAAKyE,EACf,CACA,OAAAuG,CAAQvG,GACA4G,MAAMV,UACNP,KAAKmB,SAETF,MAAML,QAAQvG,EAClB,CAIA,MAAA8G,GACI,MAAMC,EAA8B,EAAhBpB,KAAKD,SACnBsB,EAAU,IAAI5L,MAAM2L,GACpB3F,EAAMuE,KAAK5M,SAGjB,IAAK,IAAIiB,EAAI,EAAGA,EAAIoH,EAAKpH,IACrBgN,EAAQhN,GAAK2L,KAAKvB,IAAIuB,KAAKM,KAAKN,KAAKC,MAAQ5L,IAEjD2L,KAAKI,KAAOiB,EACZrB,KAAKD,SAAWqB,EAChBpB,KAAKK,gBAAkB,EAAIL,KAAKD,SAChCC,KAAKC,MAAQ,EACbD,KAAKE,IAAMzE,CACf,EJLG,SAAS,EAAqB6F,GACjC,OAAO,IAAIC,EAAqBD,EACpC,CAaO,SAASE,EAAyBC,EAAeC,GACpD,OAAO,IAAIC,EAAgBF,EAAeC,EAC9C,CIVAV,EAAkBE,iBAAmB,GJgE9B,MAAMU,EAST,aAAMC,GACF,MAAMlN,EAAS,GACf,IAAIZ,QAAUiM,KAAK8B,OACnB,MAAQ/N,EAAEgO,MACNpN,EAAOiB,KAAK7B,EAAEsG,OACdtG,QAAUiM,KAAK8B,OAEnB,OAAOnN,CACX,CAYA,oBAAMqN,GACF,MAAMC,EAASjC,KAAKkC,SAAS,KACvBvN,EAAS,GACf,IAAIZ,QAAUkO,EAAOH,OACrB,MAAQ/N,EAAEgO,MACNpN,EAAOiB,KAAK7B,EAAEsG,OACdtG,QAAUkO,EAAOH,OAErB,OAAOnN,CACX,CAQA,kBAAMwN,GACF,IAAIpO,QAAUiM,KAAK8B,OACnB,MAAQ/N,EAAEgO,MACNhO,QAAUiM,KAAK8B,MAEvB,CAQA,kBAAMM,CAAaC,GACf,IAAItO,QAAUiM,KAAK8B,OACfQ,EAAiBD,EAAUtO,EAAEsG,OACjC,MAAStG,EAAEgO,MAASO,GAChBvO,QAAUiM,KAAK8B,OACfQ,EAAiBD,EAAUtO,EAAEsG,MAErC,CAaA,YAAAkI,CAAaC,GACT,OAAO,IAAIC,EAA0BzC,KAAMwC,EAC/C,CAUA,MAAAE,CAAOL,GACH,OAAO,IAAIM,EAAe3C,KAAMqC,EACpC,CASA,GAAAhK,CAAIuK,GACA,OAAO,IAAIC,EAAY7C,KAAM4C,EACjC,CASA,QAAAE,CAASF,GACL,OAAO,IAAIG,EAAiB/C,KAAM4C,EACtC,CASA,cAAAI,CAAeJ,GACX,OAAO,IAAIG,EAAiB/C,KAAM4C,GAAWK,QACjD,CASA,OAAAC,CAAQN,GACJ,OAAO,IAAIO,EAAgBnD,KAAM4C,EACrC,CAMA,kBAAMQ,CAAazI,GACf,OAAOqF,KAAK3H,IAAIsC,GAAGwH,cACvB,CAQA,mBAAMkB,CAAc1I,GAChB,OAAOqF,KAAKgD,eAAerI,GAAGyH,cAAarO,IAAY,IAANA,GACrD,CAmBA,aAAAuP,CAAcC,EAAWC,GAAiB,GACtC,OAAO,IAAIC,EAAsBzD,KAAMuD,EAAWC,EACtD,CAiCA,gBAAAE,CAAiBH,EAAWC,GAAiB,EAE7CrE,EAAQC,GAKJ,OAHmBY,KAAKsD,cAAcC,EAAWC,GAG/BnL,KAAItE,GAAKkL,EAAQlL,EAAGoL,IAC1C,CAWA,WAAAwE,CAAYC,EAAUlC,GAClB,OAAO,IAAIC,EAlUR,IAAIkC,EAkUsC,CAAC7D,KAAM4D,IAAYlC,EACpE,CAQA,IAAAoC,CAAKC,GACD,OAAIA,EAAQ,GAAc,MAATA,EACN/D,KAEJ,IAAIgE,EAAahE,KAAM+D,EAClC,CAOA,IAAAE,CAAKF,GACD,OAAIA,EAAQ,GAAc,MAATA,EACN/D,KAEJ,IAAIkE,EAAalE,KAAM+D,EAClC,CAUA,QAAA7B,CAASiC,GACL,OAAO,IAAIC,EAAiBpE,KAAMmE,EACtC,CAUA,OAAAlR,CAAQoR,EAAYC,GAChB,OAAO,IAAIC,EAAgBvE,KAAMqE,EAAYC,EACjD,CAKA,MAAArB,GACI,OAAO,IAAIuB,EAAexE,KAC9B,EASJ,MAAM6D,UAAsBjC,EACxB,WAAAhH,CAAY6J,GACRxD,QACAjB,KAAKyE,MAAQA,EACbzE,KAAK0E,KAAO,CAChB,CACA,OAAAC,GACI,MAAO,YAAY3E,KAAKyE,MAAMrR,cAClC,CACA,UAAM0O,GACF,GAAI9B,KAAK0E,MAAQ1E,KAAKyE,MAAMrR,OACxB,MAAO,CAAEiH,MAAO,KAAM0H,MAAM,GAEhC,MAAMnC,EAAOI,KAAKyE,MAAMzE,KAAK0E,MAE7B,OADA1E,KAAK0E,OACE,CAAErK,MAAOoF,EAAUG,GAAOmC,MAAM,EAC3C,EAEJ,MAAMR,UAA6BK,EAC/B,WAAAhH,CAAYgK,GACR3D,QACAjB,KAAK4E,OAASA,CAClB,CACA,OAAAD,GACI,MAAO,eACX,CACA,UAAM7C,GACF,IACI,OAAO9B,KAAK4E,QAChB,CACA,MAAOC,GAIH,MAFAA,EAAEC,QACE,mDAAmDD,EAAEC,UACnDD,CACV,CACJ,EAEJ,MAAML,UAAuB5C,EACzB,WAAAhH,CAAYmK,GACR9D,QACAjB,KAAK+E,SAAWA,EAChB/E,KAAKgF,SAAW1N,QAAQC,QAAQ,CAAE8C,MAAO,KAAM0H,MAAM,GACzD,CACA,OAAA4C,GACI,MAAO,GAAG3E,KAAK+E,SAASJ,qBAC5B,CACA,UAAM7C,GAMF,OADA9B,KAAKgF,SAAWhF,KAAKgF,SAASpI,MAAK,IAAMoD,KAAKiF,eACvCjF,KAAKgF,QAChB,CACA,gBAAMC,GACF,OAAOjF,KAAK+E,SAASjD,MACzB,EAEJ,MAAMoC,UAAqBtC,EACvB,WAAAhH,CAAYmK,EAAUG,GAClBjE,QACAjB,KAAK+E,SAAWA,EAChB/E,KAAKkF,SAAWA,EAEhBlF,KAAK+D,MAAQ,EACb/D,KAAKgF,SAAW1N,QAAQC,QAAQ,CAAE8C,MAAO,KAAM0H,MAAM,GACzD,CACA,OAAA4C,GACI,MAAO,GAAG3E,KAAK+E,SAASJ,mBAC5B,CACA,UAAM7C,GAMF,OADA9B,KAAKgF,SAAWhF,KAAKgF,SAASpI,MAAK,IAAMoD,KAAKiF,eACvCjF,KAAKgF,QAChB,CACA,gBAAMC,GAKF,KAAOjF,KAAK+D,QAAU/D,KAAKkF,UAAU,CACjC,MAAMC,QAAgBnF,KAAK+E,SAASjD,OAEpC,GAAIqD,EAAQpD,KACR,OAAOoD,EAEX,UAAWA,EAAQ9K,MACvB,CACA,OAAO2F,KAAK+E,SAASjD,MACzB,EAEJ,MAAMkC,UAAqBpC,EACvB,WAAAhH,CAAYmK,EAAUG,GAClBjE,QACAjB,KAAK+E,SAAWA,EAChB/E,KAAKkF,SAAWA,EAChBlF,KAAK+D,MAAQ,CACjB,CACA,OAAAY,GACI,MAAO,GAAG3E,KAAK+E,SAASJ,mBAC5B,CACA,UAAM7C,GACF,OAAI9B,KAAK+D,SAAW/D,KAAKkF,SACd,CAAE7K,MAAO,KAAM0H,MAAM,GAEzB/B,KAAK+E,SAASjD,MACzB,EAKJ,MAAM2B,UAA8B7B,EAChC,WAAAhH,CAAYmK,EAAUxB,EAAW6B,GAAuB,GACpDnE,QACAjB,KAAK+E,SAAWA,EAChB/E,KAAKuD,UAAYA,EACjBvD,KAAKoF,qBAAuBA,EAC5BpF,KAAKgF,SAAW1N,QAAQC,QAAQ,CAAE8C,MAAO,KAAM0H,MAAM,GACzD,CACA,OAAA4C,GACI,MAAO,GAAG3E,KAAK+E,SAASJ,4BAC5B,CACA,UAAM7C,GAMF,OADA9B,KAAKgF,SAAWhF,KAAKgF,SAASpI,MAAK,IAAMoD,KAAKiF,eACvCjF,KAAKgF,QAChB,CACA,gBAAMC,GACF,MAAMI,EAAQ,GACd,KAAOA,EAAMjS,OAAS4M,KAAKuD,WAAW,CAClC,MAAM3D,QAAaI,KAAK+E,SAASjD,OACjC,GAAIlC,EAAKmC,KACL,OAAI/B,KAAKoF,sBAAwBC,EAAMjS,OAAS,EACrC,CAAEiH,MAAOgL,EAAOtD,MAAM,GAE1B,CAAE1H,MAAO,KAAM0H,MAAM,GAEhCsD,EAAMzP,KAAKgK,EAAKvF,MACpB,CACA,MAAO,CAAEA,MAAOgL,EAAOtD,MAAM,EACjC,EAEJ,MAAMY,UAAuBf,EACzB,WAAAhH,CAAYmK,EAAU1C,GAClBpB,QACAjB,KAAK+E,SAAWA,EAChB/E,KAAKqC,UAAYA,EACjBrC,KAAKgF,SAAW1N,QAAQC,QAAQ,CAAE8C,MAAO,KAAM0H,MAAM,GACzD,CACA,OAAA4C,GACI,MAAO,GAAG3E,KAAK+E,SAASJ,qBAC5B,CACA,UAAM7C,GAMF,OADA9B,KAAKgF,SAAWhF,KAAKgF,SAASpI,MAAK,IAAMoD,KAAKiF,eACvCjF,KAAKgF,QAChB,CACA,gBAAMC,GACF,OAAa,CACT,MAAMrF,QAAaI,KAAK+E,SAASjD,OACjC,GAAIlC,EAAKmC,MAAQ/B,KAAKqC,UAAUzC,EAAKvF,OACjC,OAAOuF,EAEX,UAAWA,EAAKvF,MACpB,CACJ,EAEJ,MAAMwI,UAAoBjB,EACtB,WAAAhH,CAAYmK,EAAUnC,GAClB3B,QACAjB,KAAK+E,SAAWA,EAChB/E,KAAK4C,UAAYA,CACrB,CACA,OAAA+B,GACI,MAAO,GAAG3E,KAAK+E,SAASJ,kBAC5B,CACA,UAAM7C,GACF,MAAMlC,QAAaI,KAAK+E,SAASjD,OACjC,GAAIlC,EAAKmC,KACL,MAAO,CAAE1H,MAAO,KAAM0H,MAAM,GAEhC,MAAMuD,EAAe,oCAAqC1F,EAAKvF,OAOzDkL,EAASvF,KAAK4C,UAAUhD,EAAKvF,OAC7BmL,EAAgB,oCAAqCD,GAG3D,IAAK,MAAME,KAAKH,EACP,6BAA8BG,EAAGD,IAClCC,EAAEC,UAGV,MAAO,CAAErL,MAAOkL,EAAQxD,MAAM,EAClC,EAEJ,MAAMU,UAAkCb,EACpC,WAAAhH,CAAYmK,EAAUvC,GAClBvB,QACAjB,KAAK+E,SAAWA,EAChB/E,KAAKwC,QAAUA,EACfxC,KAAK+D,MAAQ,EACb/D,KAAKgF,SAAW1N,QAAQC,QAAQ,CAAE8C,MAAO,KAAM0H,MAAM,GACzD,CACA,OAAA4C,GACI,MAAO,GAAG3E,KAAK+E,SAASJ,2BAC5B,CACA,UAAM7C,GAMF,OADA9B,KAAKgF,SAAWhF,KAAKgF,SAASpI,MAAK,IAAMoD,KAAKiF,eACvCjF,KAAKgF,QAChB,CACA,gBAAMC,GACF,OACI,IACI,aAAajF,KAAK+E,SAASjD,MAC/B,CACA,MAAO+C,GACH,IAAK7E,KAAKwC,QAAQqC,GACd,MAAO,CAAExK,MAAO,KAAM0H,MAAM,EAMpC,CAER,EAEJ,MAAMgB,UAAyBnB,EAC3B,WAAAhH,CAAYmK,EAAUnC,GAClB3B,QACAjB,KAAK+E,SAAWA,EAChB/E,KAAK4C,UAAYA,CACrB,CACA,OAAA+B,GACI,MAAO,GAAG3E,KAAK+E,SAASJ,uBAC5B,CACA,UAAM7C,GACF,MAAMlC,QAAaI,KAAK+E,SAASjD,OACjC,GAAIlC,EAAKmC,KACL,MAAO,CAAE1H,MAAO,KAAM0H,MAAM,GAEhC,MAAMuD,EAAe,oCAAqC1F,EAAKvF,OAOzDkL,QAAevF,KAAK4C,UAAUhD,EAAKvF,OACnCmL,EAAgB,oCAAqCD,GAG3D,IAAK,MAAME,KAAKH,EACP,6BAA8BG,EAAGD,IAClCC,EAAEC,UAGV,MAAO,CAAErL,MAAOkL,EAAQxD,MAAM,EAClC,EAYG,MAAM4D,UAA0B/D,EACnC,WAAAhH,GACIqG,QACAjB,KAAK4F,YAAc,IAAI5E,EACvBhB,KAAKgF,SAAW1N,QAAQC,QAAQ,CAAE8C,MAAO,KAAM0H,MAAM,GACzD,CACA,UAAMD,GAMF,OADA9B,KAAKgF,SAAWhF,KAAKgF,SAASpI,MAAK,IAAMoD,KAAKiF,eACvCjF,KAAKgF,QAChB,CACA,gBAAMC,GAIF,KAAqC,IAA9BjF,KAAK4F,YAAYxS,UAEpB,UAAW4M,KAAK6F,OACZ,MAAO,CAAExL,MAAO,KAAM0H,MAAM,GAGpC,MAAO,CAAE1H,MAAO2F,KAAK4F,YAAY/E,QAASkB,MAAM,EACpD,EAEJ,MAAMoB,UAAwBwC,EAC1B,WAAA/K,CAAYmK,EAAUnC,GAClB3B,QACAjB,KAAK+E,SAAWA,EAChB/E,KAAK4C,UAAYA,CACrB,CACA,OAAA+B,GACI,MAAO,GAAG3E,KAAK+E,SAASJ,sBAC5B,CACA,UAAMkB,GACF,MAAMjG,QAAaI,KAAK+E,SAASjD,OACjC,GAAIlC,EAAKmC,KACL,OAAO,EAEX,MAAMuD,EAAe,oCAAqC1F,EAAKvF,OAMzDyL,EAAc9F,KAAK4C,UAAUhD,EAAKvF,OAClCmL,EAAgB,oCAAqCM,GAC3D9F,KAAK4F,YAAYnF,QAAQqF,GAGzB,IAAK,MAAML,KAAKH,EACP,6BAA8BG,EAAGD,IAClCC,EAAEC,UAGV,OAAO,CACX,EAWG,MAAM/D,UAAwBC,EACjC,WAAAhH,CAAYmL,EAAWrE,GACnBT,QACAjB,KAAK0B,iBAAmBA,EAGxB1B,KAAKgF,SAAW,KAEhBhF,KAAK4D,SAAW,KAChB5D,KAAKgG,cAAgBD,CACzB,CACA,OAAApB,GAEI,MAAO,wDACX,CACA,UAAM7C,GAEF,OADA9B,KAAKgF,SAAWhF,KAAKiG,cAAcjG,KAAKgF,UACjChF,KAAKgF,QAChB,CACA,mBAAMiB,CAAcjB,GAOhB,SADMA,EACe,MAAjBhF,KAAK4D,SAAkB,CACvB,MAAMsC,QAAuBlG,KAAKgG,cAAclE,OAChD,GAAIoE,EAAenE,KAEf,MAAO,CAAE1H,MAAO,KAAM0H,MAAM,GAEhC/B,KAAK4D,SAAWsC,EAAe7L,MACF,MAAzB2F,KAAK0B,mBACL1B,KAAK4D,SAAW5D,KAAK4D,SAASrB,aAAavC,KAAK0B,kBAExD,CACA,MAAMyE,QAAmBnG,KAAK4D,SAAS9B,OACvC,OAAIqE,EAAWpE,MACX/B,KAAK4D,SAAW,KACT5D,KAAKiG,cAAcjB,IAEvBmB,CACX,GAGJ,SAAWC,GACPA,EAAgBA,EAAsB,KAAI,GAAK,OAC/CA,EAAgBA,EAA0B,SAAI,GAAK,WACnDA,EAAgBA,EAAyB,QAAI,GAAK,SACrD,CAJD,CAIG,IAAoB,EAAkB,CAAC,IAsGnC,MAAMhC,UAAyBxC,EAClC,WAAAhH,CAAYmK,EAAUZ,GAClBlD,QACAjB,KAAK+E,SAAWA,EAChB/E,KAAKmE,WAAaA,EAClBnE,KAAKqG,OAAS,IAAIvG,EAAWqE,EACjC,CACA,OAAAQ,GACI,MAAO,GAAG3E,KAAK+E,SAASJ,uBAC5B,CAKA,MAAA2B,GACI,MAAQtG,KAAKqG,OAAO9F,UAAU,CAC1B,MAAMgG,EAAIvG,KAAK+E,SAASjD,OACxB9B,KAAKqG,OAAOzQ,KAAK2Q,EACrB,CACJ,CACA,IAAAzE,GAKI,OAJA9B,KAAKsG,SAIEtG,KAAKqG,OAAOxF,OACvB,EAQG,MAAM0D,UAAwBH,EACjC,WAAAxJ,CAAYmK,EAAUV,EAAYC,GAC9BrD,MAAM8D,EAAUV,GAChBrE,KAAK+E,SAAWA,EAChB/E,KAAKqE,WAAaA,EAElBrE,KAAKwG,mBAAoB,EACzBxG,KAAKxM,OAASiT,EAAWC,KAAKpC,GAAQ,aAAcqC,YACpD3G,KAAKgF,SAAW1N,QAAQC,QAAQ,CAAE8C,MAAO,KAAM0H,MAAM,GACzD,CACA,UAAMD,GAMF,OADA9B,KAAKgF,SAAWhF,KAAKgF,SAASpI,MAAK,IAAMoD,KAAKiF,eACvCjF,KAAKgF,QAChB,CACA,SAAA4B,CAAU5S,GACN,OAAOT,KAAKkJ,MAAMuD,KAAKxM,SAAWQ,EACtC,CACA,WAAA6S,GACI,OAAO7G,KAAK4G,UAAU5G,KAAKqG,OAAOjT,SACtC,CACA,gBAAM6R,GAKF,IAHKjF,KAAKwG,mBACNxG,KAAKsG,UAEDtG,KAAKqG,OAAO7F,WAAW,CAC3B,MAAMsG,EAAc9G,KAAK6G,cACnBlS,QAAeqL,KAAKqG,OAAOvF,cAAcgG,GAC/C,IAAInS,EAAOoN,KAKP,OADA/B,KAAKsG,SACE3R,EAJPqL,KAAKwG,mBAAoB,CAMjC,CACA,MAAO,CAAEnM,MAAO,KAAM0H,MAAM,EAChC,EKz7BG,MAAMgF,EACT,WAAAnM,GACIoF,KAAKjK,KAAO,IAChB,CA6DA,KAAAsP,CAAM9B,EAAWC,GAAiB,GAC9B,MAAMwD,EAAOhH,KAGb,IAAIjK,EAgBJ,OAlBA,cAAewN,EAAY,GAAG,IAAM,oDACpCA,MAKIxN,EAHAiK,KAAKjK,OAASM,KAAyB,MAAb2J,KAAKjK,KAGxBiK,KAAKjK,KAEPyN,EAGEjQ,KAAKmD,KAAKsJ,KAAKjK,KAAOwN,GAKtBhQ,KAAKkJ,MAAMuD,KAAKjK,KAAOwN,GAE3B,GAAsB0D,gBACXD,EAAKpD,YACdF,iBAAiBH,EAAWC,EAAgB0D,IAClDnR,EACP,CAgBA,WAAA4N,CAAYwD,GACR,MAAMH,EAAOhH,KACb,IAAIjK,EAgBJ,OAZIA,EAHAiK,KAAKjK,OAASM,KAAY8Q,EAAQpR,OAASM,IAGpCA,IAEW,MAAb2J,KAAKjK,MAAgC,MAAhBoR,EAAQpR,KAG3BiK,KAAKjK,KAAOoR,EAAQpR,KAKpB,KAEJ,GAAsBkR,gBAAmBD,EAAKpD,YAAYD,kBAAkBwD,EAAQvD,aAAa7N,EAC5G,CAiBA,MAAA2M,CAAOL,GACH,MAAM2E,EAAOhH,KACb,IAAIjK,EAUJ,OAPIA,EAFAiK,KAAKjK,OAASM,IAEPA,IAKA,KAEJ,GAAsB4Q,gBACXD,EAAKpD,YAAYlB,QAAO3O,GAAK,QAAQ,IAAMsO,EAAUtO,QACpEgC,EACP,CAiBA,kBAAMqN,CAAazI,GACf,aAAcqF,KAAK4D,YAAYR,aAAazI,EAChD,CAgBA,GAAAtC,CAAIuK,GACA,MAAMoE,EAAOhH,KACb,OAAO,GAAsBiH,gBACXD,EAAKpD,YAAYvL,KAAItE,GAAK,QAAQ,IAAM6O,EAAU7O,QACjEiM,KAAKjK,KACZ,CAwBA,QAAA+M,CAASF,GACL,MAAMoE,EAAOhH,KACb,OAAO,GAAsBiH,gBACXD,EAAKpD,YAAYd,SAASF,IACzC5C,KAAKjK,KACZ,CAUA,QAAAmM,CAASiC,GACL,GAAkB,MAAdA,EACA,MAAM,IAAIhE,WAAW,6DAEzB,MAAM6G,EAAOhH,KACb,OAAO,GAAsBiH,gBAAmBD,EAAKpD,YAAY1B,SAASiC,IAAanE,KAAKjK,KAChG,CAmBA,MAAAkB,CAAO8M,GACH,MAAMiD,EAAOhH,KACb,IAAIjK,EAoBJ,OAfIA,EAJa,MAAbiK,KAAKjK,MAAgBgO,EAAQ,EAItB/D,KAAKjK,KAAOgO,EAEJ,IAAVA,EAEE,EAEW,MAAb/D,KAAKjK,YAA2B4K,IAAVoD,GAAuBA,EAAQ,GAGnD1N,IAIA,KAEJ,GAAsB4Q,SAElBzF,EADkB,GAAqByF,UAAY,CAAG5M,YAAa2M,EAAKpD,WAAY7B,MAAM,MAChD+B,KAAKC,KACvDhO,EACP,CAkBA,IAAAkO,CAAKF,GACD,MAAMiD,EAAOhH,KACb,IAAIjK,EAiBJ,OAZIA,EAJa,MAAbiK,KAAKjK,MAAgBgO,GAAS,GAAK/D,KAAKjK,MAAQgO,EAIzC/D,KAAKjK,KAAOgO,EAED,MAAb/D,KAAKjK,OACTiK,KAAKjK,KAAOgO,QAAmBpD,IAAVoD,GAAuBA,EAAQ,GAG9C,EAIA,KAEJ,GAAsBkD,gBAAmBD,EAAKpD,YAAYK,KAAKF,IAAQhO,EAClF,CAsBA,OAAA9C,CAAQkR,EAAYG,EAAM8C,GAAyB,GAC/C,GAAkB,MAAdjD,GAAsBA,EAAa,EACnC,MAAiB,MAAbnE,KAAKjK,KACC,IAAIoK,WAAW,4DAGf,IAAIA,WAGN,mNAAmCH,KAAKjK,kBAGpD,MAAMiR,EAAOhH,KACPxM,EAASiT,EAAWC,KAAKpC,GAAQ,aAAcqC,YACrD,OAAO,GAAsBM,UACzB,IAAII,EAAQ7T,EAAO8T,QAInB,OAHIF,IACAC,GAAS7T,EAAO8T,gBAENN,EAAKpD,YAAY3Q,QAAQkR,EAAYkD,EAAMV,WAAW,GACrE3G,KAAKjK,KACZ,CAkBA,IAAA+N,CAAKC,GACD,MAAMiD,EAAOhH,KACb,IAAIjK,EAeJ,OAXIA,EAHa,MAAbiK,KAAKjK,MAAgBiK,KAAKjK,KAAOgO,EAG1BA,EAEW,MAAb/D,KAAKjK,MAAgBiK,KAAKjK,MAAQgO,EAGhC/D,KAAKjK,KAIL,KAEJ,GAAsBkR,gBAAmBD,EAAKpD,YAAYE,KAAKC,IAAQhO,EAClF,CAiBA,aAAM8L,GACF,GAAI7B,KAAKjK,OAASM,IACd,MAAM,IAAI1C,MAAM,kDAEpB,aAAcqM,KAAK4D,YAAY/B,SACnC,CAYA,oBAAMG,GACF,GAAIhC,KAAKjK,OAASM,IACd,MAAM,IAAI1C,MAAM,kDAEpB,aAAcqM,KAAK4D,YAAY5B,gBACnC,EAgBG,SAAS,EAAsBuF,EAAYxR,EAAO,MACrD,OAAO,IAAI,cAAcgR,EACrB,WAAAnM,GACIqG,SAASuG,WACTxH,KAAKjK,KAAOA,CAChB,CAKA,cAAM6N,GACF,OAAO2D,GACX,EAER,CAyGA,SAASL,EAAgBO,GACrB,GAAa,OAATA,EACA,OAAO,KAIX,GJjaG,SAAsBnI,GACzB,OAAc,MAAPA,GASW,QADDjF,EARiBiF,IAUZ,kBAAVjF,GAAuC,oBAAVA,GAVC5E,MAAMC,QAAQ4J,IACpC,kBAARA,GAAqBA,aAAe,UAC5C,oBAAqBA,GAM7B,IAAqBjF,CALrB,CI6ZQqN,CADeD,EAAK,IACM,CAG1B,MAAO,CAAEpN,MASjB,SAAqBsN,GACjB,GAAsB,IAAlBA,EAAOvU,OAEP,MAAM,IAAIO,MAAM,wCAEpB,OAAIgU,EAAO,aAAc,SAEd,QAASA,GAIT,SAAUA,EAEzB,CAvBsBC,CAAYH,GACV/I,SAAS,EAC7B,CAEA,MAAO,CAAErE,MAAO,KAAMqE,SAAS,EACnC,CAjJAqI,EAAQc,gBAAkB,ICzdRC,OAAO,OACLA,OAAO,SACPA,OAAO,SACKA,OAAO,mBACHA,OAAO,gB,YCvB3C,MAAM9U,EAAU,O,m+DCoBT,SAAS+U,EAAkB1N,EAAOpB,GACrC,MAAc,WAAVA,EACO+O,EAAa3N,GAEjB4N,EAAa,CAAC5N,GAAQpB,EACjC,CAMO,SAASgP,EAAa1T,EAAG0E,GAC5B,GAAc,WAAVA,EACA,MAAM,IAAItF,MAAM,6CAQpB,GANI8B,MAAMC,QAAQnB,KACdA,EAAI,KAAaA,KAEjB,UAAM2T,QAAQ,UACd,KAA8B3T,EAAG0E,GAbzC,SAA4B1E,EAAG0E,GAC3B,OAAQ1E,aAAa4E,cAA0B,YAAVF,GAChC1E,aAAa6E,YAAwB,UAAVH,GAC3B1E,aAAa8E,YAAwB,SAAVJ,CACpC,CAWQkP,CAAmB5T,EAAG0E,GACtB,OAAO1E,EAEX,GAAa,MAAT0E,GAA2B,YAAVA,GAAiC,cAAVA,EACxC,OAAO,IAAIE,aAAa5E,GAEvB,GAAc,UAAV0E,EACL,OAAO,IAAIG,WAAW7E,GAErB,GAAc,SAAV0E,EAAkB,CACvB,MAAMmP,EAAO,IAAI/O,WAAW9E,EAAEnB,QAC9B,IAAK,IAAIiB,EAAI,EAAGA,EAAI+T,EAAKhV,SAAUiB,EACN,IAArBd,KAAK8U,MAAM9T,EAAEF,MACb+T,EAAK/T,GAAK,GAGlB,OAAO+T,CACX,CAEI,MAAM,IAAIzU,MAAM,qBAAqBsF,IAE7C,CAYO,SAASqP,IACZ,OAAO,UAAMC,SAASD,KAC1B,CAiBO,SAASE,EAAMC,EAAMC,GACxB,OAAO,UAAMH,SAASC,MAAMC,EAAMC,EACtC,CASO,SAASV,EAAa1P,EAAGqQ,EAAW,SAEvC,OADAA,EAAWA,GAAY,SAChB,UAAMJ,SAASK,OAAOtQ,EAAGqQ,EACpC,CASO,SAASE,EAAa3O,EAAOyO,EAAW,SAE3C,OADAA,EAAWA,GAAY,SAChB,UAAMJ,SAASO,OAAO5O,EAAOyO,EACxC,C","sources":["webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/version.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/util_base.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/types.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/train.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-data/dist/iterators/lazy_iterator.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-data/dist/util/deep_map.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-data/dist/util/deep_clone.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-data/dist/util/ring_buffer.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-data/dist/util/growing_ring_buffer.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-data/dist/dataset.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-data/dist/datasets/csv_dataset.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-data/dist/version.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/util.js"],"sourcesContent":["/** @license See the LICENSE file. */\n// This code is auto-generated, do not modify this file!\nconst version = '3.6.0';\nexport { version };\n//# sourceMappingURL=version.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n/**\n * Shuffles the array in-place using Fisher-Yates algorithm.\n *\n * ```js\n * const a = [1, 2, 3, 4, 5];\n * tf.util.shuffle(a);\n * console.log(a);\n * ```\n *\n * @param array The array to shuffle in-place.\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\n// tslint:disable-next-line:no-any\nexport function shuffle(array) {\n    let counter = array.length;\n    let temp = 0;\n    let index = 0;\n    // While there are elements in the array\n    while (counter > 0) {\n        // Pick a random index\n        index = (Math.random() * counter) | 0;\n        // Decrease counter by 1\n        counter--;\n        // And swap the last element with it\n        temp = array[counter];\n        array[counter] = array[index];\n        array[index] = temp;\n    }\n}\n/**\n * Shuffles two arrays in-place the same way using Fisher-Yates algorithm.\n *\n * ```js\n * const a = [1,2,3,4,5];\n * const b = [11,22,33,44,55];\n * tf.util.shuffleCombo(a, b);\n * console.log(a, b);\n * ```\n *\n * @param array The first array to shuffle in-place.\n * @param array2 The second array to shuffle in-place with the same permutation\n *     as the first array.\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function shuffleCombo(\n// tslint:disable-next-line:no-any\narray, \n// tslint:disable-next-line:no-any\narray2) {\n    if (array.length !== array2.length) {\n        throw new Error(`Array sizes must match to be shuffled together ` +\n            `First array length was ${array.length}` +\n            `Second array length was ${array2.length}`);\n    }\n    let counter = array.length;\n    let temp, temp2;\n    let index = 0;\n    // While there are elements in the array\n    while (counter > 0) {\n        // Pick a random index\n        index = (Math.random() * counter) | 0;\n        // Decrease counter by 1\n        counter--;\n        // And swap the last element of each array with it\n        temp = array[counter];\n        temp2 = array2[counter];\n        array[counter] = array[index];\n        array2[counter] = array2[index];\n        array[index] = temp;\n        array2[index] = temp2;\n    }\n}\n/** Clamps a value to a specified range. */\nexport function clamp(min, x, max) {\n    return Math.max(min, Math.min(x, max));\n}\nexport function nearestLargerEven(val) {\n    return val % 2 === 0 ? val : val + 1;\n}\nexport function sum(arr) {\n    let sum = 0;\n    for (let i = 0; i < arr.length; i++) {\n        sum += arr[i];\n    }\n    return sum;\n}\n/**\n * Returns a sample from a uniform [a, b) distribution.\n *\n * @param a The minimum support (inclusive).\n * @param b The maximum support (exclusive).\n * @return A pseudorandom number on the half-open interval [a,b).\n */\nexport function randUniform(a, b) {\n    const r = Math.random();\n    return (b * r) + (1 - r) * a;\n}\n/** Returns the squared Euclidean distance between two vectors. */\nexport function distSquared(a, b) {\n    let result = 0;\n    for (let i = 0; i < a.length; i++) {\n        const diff = Number(a[i]) - Number(b[i]);\n        result += diff * diff;\n    }\n    return result;\n}\n/**\n * Asserts that the expression is true. Otherwise throws an error with the\n * provided message.\n *\n * ```js\n * const x = 2;\n * tf.util.assert(x === 2, 'x is not 2');\n * ```\n *\n * @param expr The expression to assert (as a boolean).\n * @param msg A function that returns the message to report when throwing an\n *     error. We use a function for performance reasons.\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function assert(expr, msg) {\n    if (!expr) {\n        throw new Error(typeof msg === 'string' ? msg : msg());\n    }\n}\nexport function assertShapesMatch(shapeA, shapeB, errorMessagePrefix = '') {\n    assert(arraysEqual(shapeA, shapeB), () => errorMessagePrefix + ` Shapes ${shapeA} and ${shapeB} must match`);\n}\nexport function assertNonNull(a) {\n    assert(a != null, () => `The input to the tensor constructor must be a non-null value.`);\n}\n// NOTE: We explicitly type out what T extends instead of any so that\n// util.flatten on a nested array of number doesn't try to infer T as a\n// number[][], causing us to explicitly type util.flatten<number>().\n/**\n *  Flattens an arbitrarily nested array.\n *\n * ```js\n * const a = [[1, 2], [3, 4], [5, [6, [7]]]];\n * const flat = tf.util.flatten(a);\n * console.log(flat);\n * ```\n *\n *  @param arr The nested array to flatten.\n *  @param result The destination array which holds the elements.\n *  @param skipTypedArray If true, avoids flattening the typed arrays. Defaults\n *      to false.\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function flatten(arr, result = [], skipTypedArray = false) {\n    if (result == null) {\n        result = [];\n    }\n    if (Array.isArray(arr) || isTypedArray(arr) && !skipTypedArray) {\n        for (let i = 0; i < arr.length; ++i) {\n            flatten(arr[i], result, skipTypedArray);\n        }\n    }\n    else {\n        result.push(arr);\n    }\n    return result;\n}\n/**\n * Returns the size (number of elements) of the tensor given its shape.\n *\n * ```js\n * const shape = [3, 4, 2];\n * const size = tf.util.sizeFromShape(shape);\n * console.log(size);\n * ```\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function sizeFromShape(shape) {\n    if (shape.length === 0) {\n        // Scalar.\n        return 1;\n    }\n    let size = shape[0];\n    for (let i = 1; i < shape.length; i++) {\n        size *= shape[i];\n    }\n    return size;\n}\nexport function isScalarShape(shape) {\n    return shape.length === 0;\n}\nexport function arraysEqual(n1, n2) {\n    if (n1 === n2) {\n        return true;\n    }\n    if (n1 == null || n2 == null) {\n        return false;\n    }\n    if (n1.length !== n2.length) {\n        return false;\n    }\n    for (let i = 0; i < n1.length; i++) {\n        if (n1[i] !== n2[i]) {\n            return false;\n        }\n    }\n    return true;\n}\nexport function isInt(a) {\n    return a % 1 === 0;\n}\nexport function tanh(x) {\n    // tslint:disable-next-line:no-any\n    if (Math.tanh != null) {\n        // tslint:disable-next-line:no-any\n        return Math.tanh(x);\n    }\n    if (x === Infinity) {\n        return 1;\n    }\n    else if (x === -Infinity) {\n        return -1;\n    }\n    else {\n        const e2x = Math.exp(2 * x);\n        return (e2x - 1) / (e2x + 1);\n    }\n}\nexport function sizeToSquarishShape(size) {\n    const width = Math.ceil(Math.sqrt(size));\n    return [width, Math.ceil(size / width)];\n}\n/**\n * Creates a new array with randomized indicies to a given quantity.\n *\n * ```js\n * const randomTen = tf.util.createShuffledIndices(10);\n * console.log(randomTen);\n * ```\n *\n * @param number Quantity of how many shuffled indicies to create.\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function createShuffledIndices(n) {\n    const shuffledIndices = new Uint32Array(n);\n    for (let i = 0; i < n; ++i) {\n        shuffledIndices[i] = i;\n    }\n    shuffle(shuffledIndices);\n    return shuffledIndices;\n}\nexport function rightPad(a, size) {\n    if (size <= a.length) {\n        return a;\n    }\n    return a + ' '.repeat(size - a.length);\n}\nexport function repeatedTry(checkFn, delayFn = (counter) => 0, maxCounter) {\n    return new Promise((resolve, reject) => {\n        let tryCount = 0;\n        const tryFn = () => {\n            if (checkFn()) {\n                resolve();\n                return;\n            }\n            tryCount++;\n            const nextBackoff = delayFn(tryCount);\n            if (maxCounter != null && tryCount >= maxCounter) {\n                reject();\n                return;\n            }\n            setTimeout(tryFn, nextBackoff);\n        };\n        tryFn();\n    });\n}\n/**\n * Given the full size of the array and a shape that may contain -1 as the\n * implicit dimension, returns the inferred shape where -1 is replaced.\n * E.g. For shape=[2, -1, 3] and size=24, it will return [2, 4, 3].\n *\n * @param shape The shape, which may contain -1 in some dimension.\n * @param size The full size (number of elements) of the array.\n * @return The inferred shape where -1 is replaced with the inferred size.\n */\nexport function inferFromImplicitShape(shape, size) {\n    let shapeProd = 1;\n    let implicitIdx = -1;\n    for (let i = 0; i < shape.length; ++i) {\n        if (shape[i] >= 0) {\n            shapeProd *= shape[i];\n        }\n        else if (shape[i] === -1) {\n            if (implicitIdx !== -1) {\n                throw Error(`Shapes can only have 1 implicit size. ` +\n                    `Found -1 at dim ${implicitIdx} and dim ${i}`);\n            }\n            implicitIdx = i;\n        }\n        else if (shape[i] < 0) {\n            throw Error(`Shapes can not be < 0. Found ${shape[i]} at dim ${i}`);\n        }\n    }\n    if (implicitIdx === -1) {\n        if (size > 0 && size !== shapeProd) {\n            throw Error(`Size(${size}) must match the product of shape ${shape}`);\n        }\n        return shape;\n    }\n    if (shapeProd === 0) {\n        throw Error(`Cannot infer the missing size in [${shape}] when ` +\n            `there are 0 elements`);\n    }\n    if (size % shapeProd !== 0) {\n        throw Error(`The implicit shape can't be a fractional number. ` +\n            `Got ${size} / ${shapeProd}`);\n    }\n    const newShape = shape.slice();\n    newShape[implicitIdx] = size / shapeProd;\n    return newShape;\n}\nexport function parseAxisParam(axis, shape) {\n    const rank = shape.length;\n    // Normalize input\n    axis = axis == null ? shape.map((s, i) => i) : [].concat(axis);\n    // Check for valid range\n    assert(axis.every(ax => ax >= -rank && ax < rank), () => `All values in axis param must be in range [-${rank}, ${rank}) but ` +\n        `got axis ${axis}`);\n    // Check for only integers\n    assert(axis.every(ax => isInt(ax)), () => `All values in axis param must be integers but ` +\n        `got axis ${axis}`);\n    // Handle negative axis.\n    return axis.map(a => a < 0 ? rank + a : a);\n}\n/** Reduces the shape by removing all dimensions of shape 1. */\nexport function squeezeShape(shape, axis) {\n    const newShape = [];\n    const keptDims = [];\n    const isEmptyArray = axis != null && Array.isArray(axis) && axis.length === 0;\n    const axes = (axis == null || isEmptyArray) ?\n        null :\n        parseAxisParam(axis, shape).sort();\n    let j = 0;\n    for (let i = 0; i < shape.length; ++i) {\n        if (axes != null) {\n            if (axes[j] === i && shape[i] !== 1) {\n                throw new Error(`Can't squeeze axis ${i} since its dim '${shape[i]}' is not 1`);\n            }\n            if ((axes[j] == null || axes[j] > i) && shape[i] === 1) {\n                newShape.push(shape[i]);\n                keptDims.push(i);\n            }\n            if (axes[j] <= i) {\n                j++;\n            }\n        }\n        if (shape[i] !== 1) {\n            newShape.push(shape[i]);\n            keptDims.push(i);\n        }\n    }\n    return { newShape, keptDims };\n}\nexport function getTypedArrayFromDType(dtype, size) {\n    let values = null;\n    if (dtype == null || dtype === 'float32') {\n        values = new Float32Array(size);\n    }\n    else if (dtype === 'int32') {\n        values = new Int32Array(size);\n    }\n    else if (dtype === 'bool') {\n        values = new Uint8Array(size);\n    }\n    else {\n        throw new Error(`Unknown data type ${dtype}`);\n    }\n    return values;\n}\nexport function getArrayFromDType(dtype, size) {\n    let values = null;\n    if (dtype == null || dtype === 'float32') {\n        values = new Float32Array(size);\n    }\n    else if (dtype === 'int32') {\n        values = new Int32Array(size);\n    }\n    else if (dtype === 'bool') {\n        values = new Uint8Array(size);\n    }\n    else if (dtype === 'string') {\n        values = new Array(size);\n    }\n    else {\n        throw new Error(`Unknown data type ${dtype}`);\n    }\n    return values;\n}\nexport function checkConversionForErrors(vals, dtype) {\n    for (let i = 0; i < vals.length; i++) {\n        const num = vals[i];\n        if (isNaN(num) || !isFinite(num)) {\n            throw Error(`A tensor of type ${dtype} being uploaded contains ${num}.`);\n        }\n    }\n}\n/** Returns true if the dtype is valid. */\nexport function isValidDtype(dtype) {\n    return dtype === 'bool' || dtype === 'complex64' || dtype === 'float32' ||\n        dtype === 'int32' || dtype === 'string';\n}\n/**\n * Returns true if the new type can't encode the old type without loss of\n * precision.\n */\nexport function hasEncodingLoss(oldType, newType) {\n    if (newType === 'complex64') {\n        return false;\n    }\n    if (newType === 'float32' && oldType !== 'complex64') {\n        return false;\n    }\n    if (newType === 'int32' && oldType !== 'float32' && oldType !== 'complex64') {\n        return false;\n    }\n    if (newType === 'bool' && oldType === 'bool') {\n        return false;\n    }\n    return true;\n}\nexport function isTypedArray(a) {\n    return a instanceof Float32Array || a instanceof Int32Array ||\n        a instanceof Uint8Array;\n}\nexport function bytesPerElement(dtype) {\n    if (dtype === 'float32' || dtype === 'int32') {\n        return 4;\n    }\n    else if (dtype === 'complex64') {\n        return 8;\n    }\n    else if (dtype === 'bool') {\n        return 1;\n    }\n    else {\n        throw new Error(`Unknown dtype ${dtype}`);\n    }\n}\n/**\n * Returns the approximate number of bytes allocated in the string array - 2\n * bytes per character. Computing the exact bytes for a native string in JS is\n * not possible since it depends on the encoding of the html page that serves\n * the website.\n */\nexport function bytesFromStringArray(arr) {\n    if (arr == null) {\n        return 0;\n    }\n    let bytes = 0;\n    arr.forEach(x => bytes += x.length);\n    return bytes;\n}\n/** Returns true if the value is a string. */\nexport function isString(value) {\n    return typeof value === 'string' || value instanceof String;\n}\nexport function isBoolean(value) {\n    return typeof value === 'boolean';\n}\nexport function isNumber(value) {\n    return typeof value === 'number';\n}\nexport function inferDtype(values) {\n    if (Array.isArray(values)) {\n        return inferDtype(values[0]);\n    }\n    if (values instanceof Float32Array) {\n        return 'float32';\n    }\n    else if (values instanceof Int32Array || values instanceof Uint8Array) {\n        return 'int32';\n    }\n    else if (isNumber(values)) {\n        return 'float32';\n    }\n    else if (isString(values)) {\n        return 'string';\n    }\n    else if (isBoolean(values)) {\n        return 'bool';\n    }\n    return 'float32';\n}\nexport function isFunction(f) {\n    return !!(f && f.constructor && f.call && f.apply);\n}\nexport function nearestDivisor(size, start) {\n    for (let i = start; i < size; ++i) {\n        if (size % i === 0) {\n            return i;\n        }\n    }\n    return size;\n}\nexport function computeStrides(shape) {\n    const rank = shape.length;\n    if (rank < 2) {\n        return [];\n    }\n    // Last dimension has implicit stride of 1, thus having D-1 (instead of D)\n    // strides.\n    const strides = new Array(rank - 1);\n    strides[rank - 2] = shape[rank - 1];\n    for (let i = rank - 3; i >= 0; --i) {\n        strides[i] = strides[i + 1] * shape[i + 1];\n    }\n    return strides;\n}\nfunction createNestedArray(offset, shape, a, isComplex = false) {\n    const ret = new Array();\n    if (shape.length === 1) {\n        const d = shape[0] * (isComplex ? 2 : 1);\n        for (let i = 0; i < d; i++) {\n            ret[i] = a[offset + i];\n        }\n    }\n    else {\n        const d = shape[0];\n        const rest = shape.slice(1);\n        const len = rest.reduce((acc, c) => acc * c) * (isComplex ? 2 : 1);\n        for (let i = 0; i < d; i++) {\n            ret[i] = createNestedArray(offset + i * len, rest, a, isComplex);\n        }\n    }\n    return ret;\n}\n// Provide a nested array of TypedArray in given shape.\nexport function toNestedArray(shape, a, isComplex = false) {\n    if (shape.length === 0) {\n        // Scalar type should return a single number.\n        return a[0];\n    }\n    const size = shape.reduce((acc, c) => acc * c) * (isComplex ? 2 : 1);\n    if (size === 0) {\n        // A tensor with shape zero should be turned into empty list.\n        return [];\n    }\n    if (size !== a.length) {\n        throw new Error(`[${shape}] does not match the input size ${a.length}${isComplex ? ' for a complex tensor' : ''}.`);\n    }\n    return createNestedArray(0, shape, a, isComplex);\n}\nexport function makeOnesTypedArray(size, dtype) {\n    const array = makeZerosTypedArray(size, dtype);\n    for (let i = 0; i < array.length; i++) {\n        array[i] = 1;\n    }\n    return array;\n}\nexport function makeZerosTypedArray(size, dtype) {\n    if (dtype == null || dtype === 'float32' || dtype === 'complex64') {\n        return new Float32Array(size);\n    }\n    else if (dtype === 'int32') {\n        return new Int32Array(size);\n    }\n    else if (dtype === 'bool') {\n        return new Uint8Array(size);\n    }\n    else {\n        throw new Error(`Unknown data type ${dtype}`);\n    }\n}\n/**\n * Make nested `TypedArray` filled with zeros.\n * @param shape The shape information for the nested array.\n * @param dtype dtype of the array element.\n */\nexport function makeZerosNestedTypedArray(shape, dtype) {\n    const size = shape.reduce((prev, curr) => prev * curr, 1);\n    if (dtype == null || dtype === 'float32') {\n        return toNestedArray(shape, new Float32Array(size));\n    }\n    else if (dtype === 'int32') {\n        return toNestedArray(shape, new Int32Array(size));\n    }\n    else if (dtype === 'bool') {\n        return toNestedArray(shape, new Uint8Array(size));\n    }\n    else {\n        throw new Error(`Unknown data type ${dtype}`);\n    }\n}\nexport function assertNonNegativeIntegerDimensions(shape) {\n    shape.forEach(dimSize => {\n        assert(Number.isInteger(dimSize) && dimSize >= 0, () => `Tensor must have a shape comprised of positive integers but got ` +\n            `shape [${shape}].`);\n    });\n}\n/**\n * Computes flat index for a given location (multidimentionsal index) in a\n * Tensor/multidimensional array.\n *\n * @param locs Location in the tensor.\n * @param rank Rank of the tensor.\n * @param strides Tensor strides.\n */\nexport function locToIndex(locs, rank, strides) {\n    if (rank === 0) {\n        return 0;\n    }\n    else if (rank === 1) {\n        return locs[0];\n    }\n    let index = locs[locs.length - 1];\n    for (let i = 0; i < locs.length - 1; ++i) {\n        index += strides[i] * locs[i];\n    }\n    return index;\n}\n/**\n * Computes the location (multidimensional index) in a tensor/multidimentional\n * array for a given flat index.\n *\n * @param index Index in flat array.\n * @param rank Rank of tensor.\n * @param strides Strides of tensor.\n */\nexport function indexToLoc(index, rank, strides) {\n    if (rank === 0) {\n        return [];\n    }\n    else if (rank === 1) {\n        return [index];\n    }\n    const locs = new Array(rank);\n    for (let i = 0; i < locs.length - 1; ++i) {\n        locs[i] = Math.floor(index / strides[i]);\n        index -= locs[i] * strides[i];\n    }\n    locs[locs.length - 1] = index;\n    return locs;\n}\n/**\n * This method asserts whether an object is a Promise instance.\n * @param object\n */\n// tslint:disable-next-line: no-any\nexport function isPromise(object) {\n    //  We chose to not use 'obj instanceOf Promise' for two reasons:\n    //  1. It only reliably works for es6 Promise, not other Promise\n    //  implementations.\n    //  2. It doesn't work with framework that uses zone.js. zone.js monkey patch\n    //  the async calls, so it is possible the obj (patched) is comparing to a\n    //  pre-patched Promise.\n    return object && object.then && typeof object.then === 'function';\n}\n//# sourceMappingURL=util_base.js.map","/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nexport var Rank;\n(function (Rank) {\n    Rank[\"R0\"] = \"R0\";\n    Rank[\"R1\"] = \"R1\";\n    Rank[\"R2\"] = \"R2\";\n    Rank[\"R3\"] = \"R3\";\n    Rank[\"R4\"] = \"R4\";\n    Rank[\"R5\"] = \"R5\";\n    Rank[\"R6\"] = \"R6\";\n})(Rank || (Rank = {}));\n// Looks for upcasting types. Used, for example, in operations with mixed dtype\n// inputs.\nvar UpcastInt32AndMap;\n(function (UpcastInt32AndMap) {\n    UpcastInt32AndMap[\"float32\"] = \"float32\";\n    UpcastInt32AndMap[\"int32\"] = \"int32\";\n    UpcastInt32AndMap[\"bool\"] = \"int32\";\n    UpcastInt32AndMap[\"complex64\"] = \"complex64\";\n})(UpcastInt32AndMap || (UpcastInt32AndMap = {}));\nvar UpcastBoolAndMap;\n(function (UpcastBoolAndMap) {\n    UpcastBoolAndMap[\"float32\"] = \"float32\";\n    UpcastBoolAndMap[\"int32\"] = \"int32\";\n    UpcastBoolAndMap[\"bool\"] = \"bool\";\n    UpcastBoolAndMap[\"complex64\"] = \"complex64\";\n})(UpcastBoolAndMap || (UpcastBoolAndMap = {}));\nvar UpcastFloat32AndMap;\n(function (UpcastFloat32AndMap) {\n    UpcastFloat32AndMap[\"float32\"] = \"float32\";\n    UpcastFloat32AndMap[\"int32\"] = \"float32\";\n    UpcastFloat32AndMap[\"bool\"] = \"float32\";\n    UpcastFloat32AndMap[\"complex64\"] = \"complex64\";\n})(UpcastFloat32AndMap || (UpcastFloat32AndMap = {}));\nvar UpcastComplex64AndMap;\n(function (UpcastComplex64AndMap) {\n    UpcastComplex64AndMap[\"float32\"] = \"complex64\";\n    UpcastComplex64AndMap[\"int32\"] = \"complex64\";\n    UpcastComplex64AndMap[\"bool\"] = \"complex64\";\n    UpcastComplex64AndMap[\"complex64\"] = \"complex64\";\n})(UpcastComplex64AndMap || (UpcastComplex64AndMap = {}));\nconst upcastTypeMap = {\n    'float32': UpcastFloat32AndMap,\n    'int32': UpcastInt32AndMap,\n    'bool': UpcastBoolAndMap,\n    'complex64': UpcastComplex64AndMap\n};\nexport function upcastType(typeA, typeB) {\n    if (typeA === 'string' || typeB === 'string') {\n        if (typeA === 'string' && typeB === 'string') {\n            return 'string';\n        }\n        throw new Error(`Can not upcast ${typeA} with ${typeB}`);\n    }\n    return upcastTypeMap[typeA][typeB];\n}\n/** Returns the output type after summation. */\nexport function sumOutType(type) {\n    return upcastType(type, 'int32');\n}\n//# sourceMappingURL=types.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// So typings can propagate.\nimport { AdadeltaOptimizer } from './optimizers/adadelta_optimizer';\nimport { AdagradOptimizer } from './optimizers/adagrad_optimizer';\nimport { AdamOptimizer } from './optimizers/adam_optimizer';\nimport { AdamaxOptimizer } from './optimizers/adamax_optimizer';\nimport { MomentumOptimizer } from './optimizers/momentum_optimizer';\nimport { OptimizerConstructors } from './optimizers/optimizer_constructors';\nimport { RMSPropOptimizer } from './optimizers/rmsprop_optimizer';\nimport { SGDOptimizer } from './optimizers/sgd_optimizer';\n// tslint:disable-next-line:no-unused-expression\n[MomentumOptimizer, SGDOptimizer, AdadeltaOptimizer, AdagradOptimizer,\n    RMSPropOptimizer, AdamaxOptimizer, AdamOptimizer];\nexport const train = {\n    sgd: OptimizerConstructors.sgd,\n    momentum: OptimizerConstructors.momentum,\n    adadelta: OptimizerConstructors.adadelta,\n    adagrad: OptimizerConstructors.adagrad,\n    rmsprop: OptimizerConstructors.rmsprop,\n    adamax: OptimizerConstructors.adamax,\n    adam: OptimizerConstructors.adam\n};\n//# sourceMappingURL=train.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\nimport { deepClone } from '../util/deep_clone';\nimport { deepMapAndAwaitAll, deepZip, zipToList } from '../util/deep_map';\nimport { GrowingRingBuffer } from '../util/growing_ring_buffer';\nimport { RingBuffer } from '../util/ring_buffer';\n// Here we implement a simple asynchronous iterator.\n// This lets us avoid using either third-party stream libraries or\n// recent TypeScript language support requiring polyfills.\n/**\n * Create a `LazyIterator` from an array of items.\n */\nexport function iteratorFromItems(items) {\n    return new ArrayIterator(items);\n}\n/**\n * Create a `LazyIterator` of incrementing integers.\n */\nexport function iteratorFromIncrementing(start) {\n    let i = start;\n    return iteratorFromFunction(() => ({ value: i++, done: false }));\n}\n/**\n * Create a `LazyIterator` from a function.\n *\n * ```js\n * let i = -1;\n * const func = () =>\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\n * const iter = tf.data.iteratorFromFunction(func);\n * await iter.forEachAsync(e => console.log(e));\n * ```\n *\n * @param func A function that produces data on each call.\n */\nexport function iteratorFromFunction(func) {\n    return new FunctionCallIterator(func);\n}\n/**\n * Create a `LazyIterator` by concatenating underlying streams, which are\n * themselves provided as a stream.\n *\n * This can also be thought of as a \"stream flatten\" operation.\n *\n * @param baseIterators A stream of streams to be concatenated.\n * @param baseErrorHandler An optional function that can intercept `Error`s\n *   raised during a `next()` call on the base stream.  This function can decide\n *   whether the error should be propagated, whether the error should be\n *   ignored, or whether the base stream should be terminated.\n */\nexport function iteratorFromConcatenated(baseIterators, baseErrorHandler) {\n    return new ChainedIterator(baseIterators, baseErrorHandler);\n}\n/**\n * Create a `LazyIterator` by concatenating streams produced by calling a\n * stream-generating function a given number of times.\n *\n * Since a `LazyIterator` is read-once, it cannot be repeated, but this\n * function can be used to achieve a similar effect:\n *\n *   LazyIterator.ofConcatenatedFunction(() => new MyIterator(), 6);\n *\n * @param iteratorFunc: A function that produces a new stream on each call.\n * @param count: The number of times to call the function.\n * @param baseErrorHandler An optional function that can intercept `Error`s\n *   raised during a `next()` call on the base stream.  This function can decide\n *   whether the error should be propagated, whether the error should be\n *   ignored, or whether the base stream should be terminated.\n */\nexport function iteratorFromConcatenatedFunction(iteratorFunc, count, baseErrorHandler) {\n    return iteratorFromConcatenated(iteratorFromFunction(iteratorFunc).take(count), baseErrorHandler);\n}\n/**\n * Create a `LazyIterator` by zipping together an array, dict, or nested\n * structure of `LazyIterator`s (and perhaps additional constants).\n *\n * The underlying streams must provide elements in a consistent order such\n * that they correspond.\n *\n * Typically, the underlying streams should have the same number of\n * elements. If they do not, the behavior is determined by the\n * `mismatchMode` argument.\n *\n * The nested structure of the `iterators` argument determines the\n * structure of elements in the resulting iterator.\n *\n * @param iterators: An array or object containing LazyIterators at the\n * leaves.\n * @param mismatchMode: Determines what to do when one underlying iterator\n * is exhausted before the others.  `ZipMismatchMode.FAIL` (the default)\n * causes an error to be thrown in this case.  `ZipMismatchMode.SHORTEST`\n * causes the zipped iterator to terminate with the furst underlying\n * streams, so elements remaining on the longer streams are ignored.\n * `ZipMismatchMode.LONGEST` causes the zipped stream to continue, filling\n * in nulls for the exhausted streams, until all streams are exhausted.\n */\nexport function iteratorFromZipped(iterators, mismatchMode = ZipMismatchMode.FAIL) {\n    return new ZipIterator(iterators, mismatchMode);\n}\n/**\n * An asynchronous iterator, providing lazy access to a potentially\n * unbounded stream of elements.\n *\n * Iterator can be obtained from a dataset:\n * `const iter = await dataset.iterator();`\n */\nexport class LazyIterator {\n    /**\n     * Collect all remaining elements of a bounded stream into an array.\n     * Obviously this will succeed only for small streams that fit in memory.\n     * Useful for testing.\n     *\n     * @returns A Promise for an array of stream elements, which will resolve\n     *   when the stream is exhausted.\n     */\n    async toArray() {\n        const result = [];\n        let x = await this.next();\n        while (!x.done) {\n            result.push(x.value);\n            x = await this.next();\n        }\n        return result;\n    }\n    /**\n     * Collect all elements of this dataset into an array with prefetching 100\n     * elements. This is useful for testing, because the prefetch changes the\n     * order in which the Promises are resolved along the processing pipeline.\n     * This may help expose bugs where results are dependent on the order of\n     * Promise resolution rather than on the logical order of the stream (i.e.,\n     * due to hidden mutable state).\n     *\n     * @returns A Promise for an array of stream elements, which will resolve\n     *   when the stream is exhausted.\n     */\n    async toArrayForTest() {\n        const stream = this.prefetch(100);\n        const result = [];\n        let x = await stream.next();\n        while (!x.done) {\n            result.push(x.value);\n            x = await stream.next();\n        }\n        return result;\n    }\n    /**\n     * Draw items from the stream until it is exhausted.\n     *\n     * This can be useful when the stream has side effects but no output.  In\n     * that case, calling this function guarantees that the stream will be\n     * fully processed.\n     */\n    async resolveFully() {\n        let x = await this.next();\n        while (!x.done) {\n            x = await this.next();\n        }\n    }\n    /**\n     * Draw items from the stream until it is exhausted, or a predicate fails.\n     *\n     * This can be useful when the stream has side effects but no output.  In\n     * that case, calling this function guarantees that the stream will be\n     * fully processed.\n     */\n    async resolveWhile(predicate) {\n        let x = await this.next();\n        let shouldContinue = predicate(x.value);\n        while ((!x.done) && shouldContinue) {\n            x = await this.next();\n            shouldContinue = predicate(x.value);\n        }\n    }\n    /**\n     * Handles errors thrown on this stream using a provided handler function.\n     *\n     * @param handler A function that handles any `Error` thrown during a `next()`\n     *   call and returns true if the stream should continue (dropping the failed\n     *   call) or false if the stream should quietly terminate.  If the handler\n     *   itself throws (or rethrows) an `Error`, that will be propagated.\n     *\n     * @returns A `LazyIterator` of elements passed through from upstream,\n     *   possibly filtering or terminating on upstream `next()` calls that\n     *   throw an `Error`.\n     */\n    handleErrors(handler) {\n        return new ErrorHandlingLazyIterator(this, handler);\n    }\n    // TODO(soergel): Implement reduce() etc.\n    /**\n     * Filters this stream according to `predicate`.\n     *\n     * @param predicate A function mapping a stream element to a boolean or a\n     * `Promise` for one.\n     *\n     * @returns A `LazyIterator` of elements for which the predicate was true.\n     */\n    filter(predicate) {\n        return new FilterIterator(this, predicate);\n    }\n    /**\n     * Maps this stream through a 1-to-1 transform.\n     *\n     * @param transform A function mapping a stream element to a transformed\n     *   element.\n     *\n     * @returns A `LazyIterator` of transformed elements.\n     */\n    map(transform) {\n        return new MapIterator(this, transform);\n    }\n    /**\n     * Maps this stream through an async 1-to-1 transform.\n     *\n     * @param transform A function mapping a stream element to a `Promise` for a\n     *   transformed stream element.\n     *\n     * @returns A `LazyIterator` of transformed elements.\n     */\n    mapAsync(transform) {\n        return new AsyncMapIterator(this, transform);\n    }\n    /**\n     * Maps this stream through a 1-to-1 transform, forcing serial execution.\n     *\n     * @param transform A function mapping a stream element to a transformed\n     *   element.\n     *\n     * @returns A `LazyIterator` of transformed elements.\n     */\n    serialMapAsync(transform) {\n        return new AsyncMapIterator(this, transform).serial();\n    }\n    /**\n     * Maps this stream through a 1-to-many transform.\n     *\n     * @param transform A function mapping a stream element to an array of\n     *   transformed elements.\n     *\n     * @returns A `DataStream` of transformed elements.\n     */\n    flatmap(transform) {\n        return new FlatmapIterator(this, transform);\n    }\n    /**\n     * Apply a function to every element of the stream.\n     *\n     * @param f A function to apply to each stream element.\n     */\n    async forEachAsync(f) {\n        return this.map(f).resolveFully();\n    }\n    /**\n     * Apply a function to every element of the stream, forcing serial execution.\n     *\n     * @param f A function to apply to each stream element.  Should return 'true'\n     *   to indicate that the stream should continue, or 'false' to cause it to\n     *   terminate.\n     */\n    async serialForEach(f) {\n        return this.serialMapAsync(f).resolveWhile(x => (x === true));\n    }\n    /**\n     * Groups elements into batches, represented as arrays of elements.\n     *\n     * We can think of the elements of this iterator as 'rows' (even if they are\n     * nested structures).  By the same token, consecutive values for a given\n     * key within the elements form a 'column'.  This matches the usual sense of\n     * 'row' and 'column' when processing tabular data (e.g., parsing a CSV).\n     *\n     * Thus, \"Row-major\" means that the resulting batch is simply a collection of\n     * rows: `[row1, row2, row3, ...]`.  This is contrast to the column-major\n     * form, which is needed for vectorized computation.\n     *\n     * @param batchSize The number of elements desired per batch.\n     * @param smallLastBatch Whether to emit the final batch when it has fewer\n     *   than batchSize elements. Default true.\n     * @returns A `LazyIterator` of batches of elements, represented as arrays\n     *   of the original element type.\n     */\n    rowMajorBatch(batchSize, smallLastBatch = true) {\n        return new RowMajorBatchIterator(this, batchSize, smallLastBatch);\n    }\n    /**\n     * Groups elements into batches, represented in column-major form.\n     *\n     * We can think of the elements of this iterator as 'rows' (even if they are\n     * nested structures).  By the same token, consecutive values for a given\n     * key within the elements form a 'column'.  This matches the usual sense of\n     * 'row' and 'column' when processing tabular data (e.g., parsing a CSV).\n     *\n     * Thus, \"column-major\" means that the resulting batch is a (potentially\n     * nested) structure representing the columns.  Each column entry, then,\n     * contains a collection of the values found in that column for a range of\n     * input elements.  This representation allows for vectorized computation, in\n     * contrast to the row-major form.\n     *\n     * The inputs should all have the same nested structure (i.e., of arrays and\n     * dicts).  The result is a single object with the same nested structure,\n     * where the leaves are arrays collecting the values of the inputs at that\n     * location (or, optionally, the result of a custom function applied to those\n     * arrays).\n     *\n     * @param batchSize The number of elements desired per batch.\n     * @param smallLastBatch Whether to emit the final batch when it has fewer\n     *   than batchSize elements. Default true.\n     * @param zipFn: (optional) A function that expects an array of elements at a\n     *   single node of the object tree, and returns a `DeepMapResult`.  The\n     *   `DeepMapResult` either provides a result value for that node (i.e.,\n     *   representing the subtree), or indicates that the node should be processed\n     *   recursively.  The default zipFn recurses as far as possible and places\n     *   arrays at the leaves.\n     * @returns A `LazyIterator` of batches of elements, represented as an object\n     *   with collections at the leaves.\n     */\n    columnMajorBatch(batchSize, smallLastBatch = true, \n    // tslint:disable-next-line:no-any\n    zipFn = zipToList) {\n        // First collect the desired number of input elements as a row-major batch.\n        const rowBatches = this.rowMajorBatch(batchSize, smallLastBatch);\n        // Now 'rotate' or 'pivot' the data, collecting all values from each column\n        // in the batch (i.e., for each key within the elements) into an array.\n        return rowBatches.map(x => deepZip(x, zipFn));\n    }\n    /**\n     * Concatenate this `LazyIterator` with another.\n     *\n     * @param iterator A `LazyIterator` to be concatenated onto this one.\n     * @param baseErrorHandler An optional function that can intercept `Error`s\n     *   raised during a `next()` call on the base stream.  This function can\n     *   decide whether the error should be propagated, whether the error should\n     *   be ignored, or whether the base stream should be terminated.\n     * @returns A `LazyIterator`.\n     */\n    concatenate(iterator, baseErrorHandler) {\n        return new ChainedIterator(iteratorFromItems([this, iterator]), baseErrorHandler);\n    }\n    /**\n     * Limits this stream to return at most `count` items.\n     *\n     * @param count The maximum number of items to provide from the stream. If\n     * a negative or undefined value is given, the entire stream is returned\n     *   unaltered.\n     */\n    take(count) {\n        if (count < 0 || count == null) {\n            return this;\n        }\n        return new TakeIterator(this, count);\n    }\n    /**\n     * Skips the first `count` items in this stream.\n     *\n     * @param count The number of items to skip.  If a negative or undefined\n     * value is given, the entire stream is returned unaltered.\n     */\n    skip(count) {\n        if (count < 0 || count == null) {\n            return this;\n        }\n        return new SkipIterator(this, count);\n    }\n    /**\n     * Prefetch the first `bufferSize` items in this stream.\n     *\n     * Note this prefetches Promises, but makes no guarantees about when those\n     * Promises resolve.\n     *\n     * @param bufferSize: An integer specifying the number of elements to be\n     *   prefetched.\n     */\n    prefetch(bufferSize) {\n        return new PrefetchIterator(this, bufferSize);\n    }\n    // TODO(soergel): deep sharded shuffle, where supported\n    /**\n     * Randomly shuffles the elements of this stream.\n     *\n     * @param bufferSize: An integer specifying the number of elements from\n     * this stream from which the new stream will sample.\n     * @param seed: (Optional.) An integer specifying the random seed that\n     * will be used to create the distribution.\n     */\n    shuffle(windowSize, seed) {\n        return new ShuffleIterator(this, windowSize, seed);\n    }\n    /**\n     * Force an iterator to execute serially: each next() call will await the\n     * prior one, so that they cannot execute concurrently.\n     */\n    serial() {\n        return new SerialIterator(this);\n    }\n}\n// ============================================================================\n// The following private classes serve to implement the chainable methods\n// on LazyIterator.  Unfortunately they can't be placed in separate files,\n// due to resulting trouble with circular imports.\n// ============================================================================\n// Iterators that just extend LazyIterator directly\n// ============================================================================\nclass ArrayIterator extends LazyIterator {\n    constructor(items) {\n        super();\n        this.items = items;\n        this.trav = 0;\n    }\n    summary() {\n        return `Array of ${this.items.length} items`;\n    }\n    async next() {\n        if (this.trav >= this.items.length) {\n            return { value: null, done: true };\n        }\n        const item = this.items[this.trav];\n        this.trav++;\n        return { value: deepClone(item), done: false };\n    }\n}\nclass FunctionCallIterator extends LazyIterator {\n    constructor(nextFn) {\n        super();\n        this.nextFn = nextFn;\n    }\n    summary() {\n        return `Function call`;\n    }\n    async next() {\n        try {\n            return this.nextFn();\n        }\n        catch (e) {\n            // Modify the error message but leave the stack trace intact\n            e.message =\n                `Error thrown while iterating through a dataset: ${e.message}`;\n            throw e;\n        }\n    }\n}\nclass SerialIterator extends LazyIterator {\n    constructor(upstream) {\n        super();\n        this.upstream = upstream;\n        this.lastRead = Promise.resolve({ value: null, done: false });\n    }\n    summary() {\n        return `${this.upstream.summary()} -> Serial`;\n    }\n    async next() {\n        // This sets this.lastRead to a new Promise right away, as opposed to\n        // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n        // would not work because this.nextRead would be updated only after the\n        // promise resolves.\n        this.lastRead = this.lastRead.then(() => this.serialNext());\n        return this.lastRead;\n    }\n    async serialNext() {\n        return this.upstream.next();\n    }\n}\nclass SkipIterator extends LazyIterator {\n    constructor(upstream, maxCount) {\n        super();\n        this.upstream = upstream;\n        this.maxCount = maxCount;\n        // Local state that should not be clobbered by out-of-order execution.\n        this.count = 0;\n        this.lastRead = Promise.resolve({ value: null, done: false });\n    }\n    summary() {\n        return `${this.upstream.summary()} -> Skip`;\n    }\n    async next() {\n        // This sets this.lastRead to a new Promise right away, as opposed to\n        // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n        // would not work because this.nextRead would be updated only after the\n        // promise resolves.\n        this.lastRead = this.lastRead.then(() => this.serialNext());\n        return this.lastRead;\n    }\n    async serialNext() {\n        // TODO(soergel): consider tradeoffs of reading in parallel, eg.\n        // collecting next() promises in an Array and then waiting for\n        // Promise.all() of those. Benefit: pseudo-parallel execution.  Drawback:\n        // maybe delayed GC.\n        while (this.count++ < this.maxCount) {\n            const skipped = await this.upstream.next();\n            // short-circuit if upstream is already empty\n            if (skipped.done) {\n                return skipped;\n            }\n            tf.dispose(skipped.value);\n        }\n        return this.upstream.next();\n    }\n}\nclass TakeIterator extends LazyIterator {\n    constructor(upstream, maxCount) {\n        super();\n        this.upstream = upstream;\n        this.maxCount = maxCount;\n        this.count = 0;\n    }\n    summary() {\n        return `${this.upstream.summary()} -> Take`;\n    }\n    async next() {\n        if (this.count++ >= this.maxCount) {\n            return { value: null, done: true };\n        }\n        return this.upstream.next();\n    }\n}\n// Note this batch just groups items into row-wise element arrays.\n// Rotating these to a column-wise representation happens only at the dataset\n// level.\nclass RowMajorBatchIterator extends LazyIterator {\n    constructor(upstream, batchSize, enableSmallLastBatch = true) {\n        super();\n        this.upstream = upstream;\n        this.batchSize = batchSize;\n        this.enableSmallLastBatch = enableSmallLastBatch;\n        this.lastRead = Promise.resolve({ value: null, done: false });\n    }\n    summary() {\n        return `${this.upstream.summary()} -> RowMajorBatch`;\n    }\n    async next() {\n        // This sets this.lastRead to a new Promise right away, as opposed to\n        // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n        // would not work because this.nextRead would be updated only after the\n        // promise resolves.\n        this.lastRead = this.lastRead.then(() => this.serialNext());\n        return this.lastRead;\n    }\n    async serialNext() {\n        const batch = [];\n        while (batch.length < this.batchSize) {\n            const item = await this.upstream.next();\n            if (item.done) {\n                if (this.enableSmallLastBatch && batch.length > 0) {\n                    return { value: batch, done: false };\n                }\n                return { value: null, done: true };\n            }\n            batch.push(item.value);\n        }\n        return { value: batch, done: false };\n    }\n}\nclass FilterIterator extends LazyIterator {\n    constructor(upstream, predicate) {\n        super();\n        this.upstream = upstream;\n        this.predicate = predicate;\n        this.lastRead = Promise.resolve({ value: null, done: false });\n    }\n    summary() {\n        return `${this.upstream.summary()} -> Filter`;\n    }\n    async next() {\n        // This sets this.lastRead to a new Promise right away, as opposed to\n        // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n        // would not work because this.nextRead would be updated only after the\n        // promise resolves.\n        this.lastRead = this.lastRead.then(() => this.serialNext());\n        return this.lastRead;\n    }\n    async serialNext() {\n        while (true) {\n            const item = await this.upstream.next();\n            if (item.done || this.predicate(item.value)) {\n                return item;\n            }\n            tf.dispose(item.value);\n        }\n    }\n}\nclass MapIterator extends LazyIterator {\n    constructor(upstream, transform) {\n        super();\n        this.upstream = upstream;\n        this.transform = transform;\n    }\n    summary() {\n        return `${this.upstream.summary()} -> Map`;\n    }\n    async next() {\n        const item = await this.upstream.next();\n        if (item.done) {\n            return { value: null, done: true };\n        }\n        const inputTensors = tf.tensor_util.getTensorsInContainer(item.value);\n        // Careful: the transform may mutate the item in place.\n        // That's why we have to remember the input Tensors above, and then\n        // below dispose only those that were not passed through to the output.\n        // Note too that the transform function is responsible for tidying\n        // any intermediate Tensors.  Here we are concerned only about the\n        // inputs.\n        const mapped = this.transform(item.value);\n        const outputTensors = tf.tensor_util.getTensorsInContainer(mapped);\n        // TODO(soergel) faster intersection\n        // TODO(soergel) move to tf.disposeExcept(in, out)?\n        for (const t of inputTensors) {\n            if (!tf.tensor_util.isTensorInList(t, outputTensors)) {\n                t.dispose();\n            }\n        }\n        return { value: mapped, done: false };\n    }\n}\nclass ErrorHandlingLazyIterator extends LazyIterator {\n    constructor(upstream, handler) {\n        super();\n        this.upstream = upstream;\n        this.handler = handler;\n        this.count = 0;\n        this.lastRead = Promise.resolve({ value: null, done: false });\n    }\n    summary() {\n        return `${this.upstream.summary()} -> handleErrors`;\n    }\n    async next() {\n        // This sets this.lastRead to a new Promise right away, as opposed to\n        // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n        // would not work because this.nextRead would be updated only after the\n        // promise resolves.\n        this.lastRead = this.lastRead.then(() => this.serialNext());\n        return this.lastRead;\n    }\n    async serialNext() {\n        while (true) {\n            try {\n                return await this.upstream.next();\n            }\n            catch (e) {\n                if (!this.handler(e)) {\n                    return { value: null, done: true };\n                }\n                // If the handler returns true, loop and fetch the next upstream item.\n                // If the upstream iterator throws an endless stream of errors, and if\n                // the handler says to ignore them, then we loop forever here.  That is\n                // the correct behavior-- it's up to the handler to decide when to stop.\n            }\n        }\n    }\n}\nclass AsyncMapIterator extends LazyIterator {\n    constructor(upstream, transform) {\n        super();\n        this.upstream = upstream;\n        this.transform = transform;\n    }\n    summary() {\n        return `${this.upstream.summary()} -> AsyncMap`;\n    }\n    async next() {\n        const item = await this.upstream.next();\n        if (item.done) {\n            return { value: null, done: true };\n        }\n        const inputTensors = tf.tensor_util.getTensorsInContainer(item.value);\n        // Careful: the transform may mutate the item in place.\n        // That's why we have to remember the input Tensors above, and then\n        // below dispose only those that were not passed through to the output.\n        // Note too that the transform function is responsible for tidying\n        // any intermediate Tensors.  Here we are concerned only about the\n        // inputs.\n        const mapped = await this.transform(item.value);\n        const outputTensors = tf.tensor_util.getTensorsInContainer(mapped);\n        // TODO(soergel) faster intersection\n        // TODO(soergel) move to tf.disposeExcept(in, out)?\n        for (const t of inputTensors) {\n            if (!tf.tensor_util.isTensorInList(t, outputTensors)) {\n                t.dispose();\n            }\n        }\n        return { value: mapped, done: false };\n    }\n}\n// Iterators that maintain a queue of pending items\n// ============================================================================\n/**\n * A base class for transforming streams that operate by maintaining an\n * output queue of elements that are ready to return via next().  This is\n * commonly required when the transformation is 1-to-many:  A call to next()\n * may trigger a call to the underlying stream, which will produce many\n * mapped elements of this stream-- of which we need to return only one, so\n * we have to queue the rest.\n */\nexport class OneToManyIterator extends LazyIterator {\n    constructor() {\n        super();\n        this.outputQueue = new GrowingRingBuffer();\n        this.lastRead = Promise.resolve({ value: null, done: false });\n    }\n    async next() {\n        // This sets this.lastRead to a new Promise right away, as opposed to\n        // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n        // would not work because this.nextRead would be updated only after the\n        // promise resolves.\n        this.lastRead = this.lastRead.then(() => this.serialNext());\n        return this.lastRead;\n    }\n    async serialNext() {\n        // Fetch so that the queue contains at least one item if possible.\n        // If the upstream source is exhausted, AND there are no items left in\n        // the output queue, then this stream is also exhausted.\n        while (this.outputQueue.length() === 0) {\n            // TODO(soergel): consider parallel reads.\n            if (!await this.pump()) {\n                return { value: null, done: true };\n            }\n        }\n        return { value: this.outputQueue.shift(), done: false };\n    }\n}\nclass FlatmapIterator extends OneToManyIterator {\n    constructor(upstream, transform) {\n        super();\n        this.upstream = upstream;\n        this.transform = transform;\n    }\n    summary() {\n        return `${this.upstream.summary()} -> Flatmap`;\n    }\n    async pump() {\n        const item = await this.upstream.next();\n        if (item.done) {\n            return false;\n        }\n        const inputTensors = tf.tensor_util.getTensorsInContainer(item.value);\n        // Careful: the transform may mutate the item in place.\n        // that's why we have to remember the input Tensors above, and then\n        // below dispose only those that were not passed through to the output.\n        // Note too that the transform function is responsible for tidying any\n        // intermediate Tensors.  Here we are concerned only about the inputs.\n        const mappedArray = this.transform(item.value);\n        const outputTensors = tf.tensor_util.getTensorsInContainer(mappedArray);\n        this.outputQueue.pushAll(mappedArray);\n        // TODO(soergel) faster intersection, and deduplicate outputTensors\n        // TODO(soergel) move to tf.disposeExcept(in, out)?\n        for (const t of inputTensors) {\n            if (!tf.tensor_util.isTensorInList(t, outputTensors)) {\n                t.dispose();\n            }\n        }\n        return true;\n    }\n}\n/**\n * Provides a `LazyIterator` that concatenates a stream of underlying\n * streams.\n *\n * Doing this in a concurrency-safe way requires some trickery.  In\n * particular, we want this stream to return the elements from the\n * underlying streams in the correct order according to when next() was\n * called, even if the resulting Promises resolve in a different order.\n */\nexport class ChainedIterator extends LazyIterator {\n    constructor(iterators, baseErrorHandler) {\n        super();\n        this.baseErrorHandler = baseErrorHandler;\n        // Strict Promise execution order:\n        // a next() call may not even begin until the previous one completes.\n        this.lastRead = null;\n        // Local state that should not be clobbered by out-of-order execution.\n        this.iterator = null;\n        this.moreIterators = iterators;\n    }\n    summary() {\n        const upstreamSummaries = 'TODO: fill in upstream of chained summaries';\n        return `${upstreamSummaries} -> Chained`;\n    }\n    async next() {\n        this.lastRead = this.readFromChain(this.lastRead);\n        return this.lastRead;\n    }\n    async readFromChain(lastRead) {\n        // Must await on the previous read since the previous read may have advanced\n        // the stream of streams, from which we need to read.\n        // This is unfortunate since we can't parallelize reads. Which means\n        // prefetching of chained streams is a no-op.\n        // One solution is to prefetch immediately upstream of this.\n        await lastRead;\n        if (this.iterator == null) {\n            const iteratorResult = await this.moreIterators.next();\n            if (iteratorResult.done) {\n                // No more streams to stream from.\n                return { value: null, done: true };\n            }\n            this.iterator = iteratorResult.value;\n            if (this.baseErrorHandler != null) {\n                this.iterator = this.iterator.handleErrors(this.baseErrorHandler);\n            }\n        }\n        const itemResult = await this.iterator.next();\n        if (itemResult.done) {\n            this.iterator = null;\n            return this.readFromChain(lastRead);\n        }\n        return itemResult;\n    }\n}\nexport var ZipMismatchMode;\n(function (ZipMismatchMode) {\n    ZipMismatchMode[ZipMismatchMode[\"FAIL\"] = 0] = \"FAIL\";\n    ZipMismatchMode[ZipMismatchMode[\"SHORTEST\"] = 1] = \"SHORTEST\";\n    ZipMismatchMode[ZipMismatchMode[\"LONGEST\"] = 2] = \"LONGEST\"; // use nulls for exhausted streams; use up the longest stream.\n})(ZipMismatchMode || (ZipMismatchMode = {}));\n/**\n * Provides a `LazyIterator` that zips together an array, dict, or nested\n * structure of `LazyIterator`s (and perhaps additional constants).\n *\n * The underlying streams must provide elements in a consistent order such\n * that they correspond.\n *\n * Typically, the underlying streams should have the same number of\n * elements. If they do not, the behavior is determined by the\n * `mismatchMode` argument.\n *\n * The nested structure of the `iterators` argument determines the\n * structure of elements in the resulting iterator.\n *\n * Doing this in a concurrency-safe way requires some trickery.  In\n * particular, we want this stream to return the elements from the\n * underlying streams in the correct order according to when next() was\n * called, even if the resulting Promises resolve in a different order.\n *\n * @param iterators: An array or object containing LazyIterators at the\n * leaves.\n * @param mismatchMode: Determines what to do when one underlying iterator\n * is exhausted before the others.  `ZipMismatchMode.FAIL` (the default)\n * causes an error to be thrown in this case.  `ZipMismatchMode.SHORTEST`\n * causes the zipped iterator to terminate with the furst underlying\n * streams, so elements remaining on the longer streams are ignored.\n * `ZipMismatchMode.LONGEST` causes the zipped stream to continue, filling\n * in nulls for the exhausted streams, until all streams are exhausted.\n */\nclass ZipIterator extends LazyIterator {\n    constructor(iterators, mismatchMode = ZipMismatchMode.FAIL) {\n        super();\n        this.iterators = iterators;\n        this.mismatchMode = mismatchMode;\n        this.count = 0;\n        this.currentPromise = null;\n    }\n    summary() {\n        const upstreamSummaries = 'TODO: fill in upstream of zip summaries';\n        return `{${upstreamSummaries}} -> Zip`;\n    }\n    async nextState(afterState) {\n        // This chaining ensures that the underlying next() are not even called\n        // before the previous ones have resolved.\n        await afterState;\n        // Collect underlying iterator \"done\" signals as a side effect in\n        // getNext()\n        let numIterators = 0;\n        let iteratorsDone = 0;\n        function getNext(container) {\n            if (container instanceof LazyIterator) {\n                const result = container.next();\n                return {\n                    value: result.then(x => {\n                        numIterators++;\n                        if (x.done) {\n                            iteratorsDone++;\n                        }\n                        return x.value;\n                    }),\n                    recurse: false\n                };\n            }\n            else {\n                return { value: null, recurse: true };\n            }\n        }\n        const mapped = await deepMapAndAwaitAll(this.iterators, getNext);\n        if (numIterators === iteratorsDone) {\n            // The streams have all ended.\n            return { value: null, done: true };\n        }\n        if (iteratorsDone > 0) {\n            switch (this.mismatchMode) {\n                case ZipMismatchMode.FAIL:\n                    throw new Error('Zipped streams should have the same length. ' +\n                        `Mismatched at element ${this.count}.`);\n                case ZipMismatchMode.SHORTEST:\n                    return { value: null, done: true };\n                case ZipMismatchMode.LONGEST:\n                default:\n                // Continue.  The exhausted streams already produced value: null.\n            }\n        }\n        this.count++;\n        return { value: mapped, done: false };\n    }\n    async next() {\n        this.currentPromise = this.nextState(this.currentPromise);\n        return this.currentPromise;\n    }\n}\n// Iterators that maintain a ring buffer of pending promises\n// ============================================================================\n/**\n * A stream that prefetches a given number of items from an upstream source,\n * returning them in FIFO order.\n *\n * Note this prefetches Promises, but makes no guarantees about when those\n * Promises resolve.\n */\nexport class PrefetchIterator extends LazyIterator {\n    constructor(upstream, bufferSize) {\n        super();\n        this.upstream = upstream;\n        this.bufferSize = bufferSize;\n        this.buffer = new RingBuffer(bufferSize);\n    }\n    summary() {\n        return `${this.upstream.summary()} -> Prefetch`;\n    }\n    /**\n     * Refill the prefetch buffer.  Returns only after the buffer is full, or\n     * the upstream source is exhausted.\n     */\n    refill() {\n        while (!this.buffer.isFull()) {\n            const v = this.upstream.next();\n            this.buffer.push(v);\n        }\n    }\n    next() {\n        this.refill();\n        // This shift will never throw an error because the buffer is always\n        // full after a refill. If the stream is exhausted, the buffer will be\n        // full of Promises that will resolve to the end-of-stream signal.\n        return this.buffer.shift();\n    }\n}\n/**\n * A stream that performs a sliding-window random shuffle on an upstream\n * source. This is like a `PrefetchIterator` except that the items are\n * returned in randomized order.  Mixing naturally improves as the buffer\n * size increases.\n */\nexport class ShuffleIterator extends PrefetchIterator {\n    constructor(upstream, windowSize, seed) {\n        super(upstream, windowSize);\n        this.upstream = upstream;\n        this.windowSize = windowSize;\n        // Local state that should not be clobbered by out-of-order execution.\n        this.upstreamExhausted = false;\n        this.random = seedrandom.alea(seed || tf.util.now().toString());\n        this.lastRead = Promise.resolve({ value: null, done: false });\n    }\n    async next() {\n        // This sets this.lastRead to a new Promise right away, as opposed to\n        // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n        // would not work because this.nextRead would be updated only after the\n        // promise resolves.\n        this.lastRead = this.lastRead.then(() => this.serialNext());\n        return this.lastRead;\n    }\n    randomInt(max) {\n        return Math.floor(this.random() * max);\n    }\n    chooseIndex() {\n        return this.randomInt(this.buffer.length());\n    }\n    async serialNext() {\n        // TODO(soergel): consider performance\n        if (!this.upstreamExhausted) {\n            this.refill();\n        }\n        while (!this.buffer.isEmpty()) {\n            const chosenIndex = this.chooseIndex();\n            const result = await this.buffer.shuffleExcise(chosenIndex);\n            if (result.done) {\n                this.upstreamExhausted = true;\n            }\n            else {\n                this.refill();\n                return result;\n            }\n        }\n        return { value: null, done: true };\n    }\n}\n//# sourceMappingURL=lazy_iterator.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\n/**\n * Apply a mapping function to a nested structure in a recursive manner.\n *\n * The result of the mapping is an object with the same nested structure (i.e.,\n * of arrays and dicts) as the input, except that some subtrees are replaced,\n * according to the results of the mapping function.\n *\n * Mappings are memoized.  Thus, if the nested structure contains the same\n * object in multiple positions, the output will contain the same mapped object\n * in those positions.  Cycles are not supported, however.\n *\n * @param input: The object to which to apply the mapping function.\n * @param mapFn: A function that expects a single node of the object tree, and\n *   returns a `DeepMapResult`.  The `DeepMapResult` either provides a\n *   replacement value for that node (i.e., replacing the subtree), or indicates\n *   that the node should be processed recursively.\n */\nexport function deepMap(input, mapFn) {\n    return deepMapInternal(input, mapFn);\n}\n/**\n * @param seen: A Map of known object mappings (i.e., memoized results of\n *   `mapFn()`)\n * @param containedIn: An set containing objects on the reference path currently\n *   being processed (used to detect cycles).\n */\nfunction deepMapInternal(input, mapFn, seen = new Map(), containedIn = new Set()) {\n    if (input == null) {\n        return null;\n    }\n    if (containedIn.has(input)) {\n        throw new Error('Circular references are not supported.');\n    }\n    if (seen.has(input)) {\n        return seen.get(input);\n    }\n    const result = mapFn(input);\n    if (result.recurse && result.value !== null) {\n        throw new Error('A deep map function may not return both a value and recurse=true.');\n    }\n    if (!result.recurse) {\n        seen.set(input, result.value);\n        return result.value;\n    }\n    else if (isIterable(input)) {\n        // tslint:disable-next-line:no-any\n        const mappedIterable = Array.isArray(input) ? [] : {};\n        containedIn.add(input);\n        for (const k in input) {\n            const child = input[k];\n            const childResult = deepMapInternal(child, mapFn, seen, containedIn);\n            mappedIterable[k] = childResult;\n        }\n        containedIn.delete(input);\n        return mappedIterable;\n    }\n    else {\n        throw new Error(`Can't recurse into non-iterable type: ${input}`);\n    }\n}\n// TODO(soergel, kangyizhang) Reconsider naming of deepZip() to avoid confusion\n// with zip()\n/**\n * Zip nested structures together in a recursive manner.\n *\n * This has the effect of transposing or pivoting data, e.g. converting it from\n * a row-major representation to a column-major representation.\n *\n * For example, `deepZip([{a: 1, b: 2}, {a: 3, b: 4}])` returns\n * `{a: [1, 3], b: [2, 4]}`.\n *\n * The inputs should all have the same nested structure (i.e., of arrays and\n * dicts).  The result is a single object with the same nested structure, where\n * the leaves are arrays collecting the values of the inputs at that location\n * (or, optionally, the result of a custom function applied to those arrays).\n *\n * @param inputs: An array of the objects to zip together.\n * @param zipFn: (optional) A function that expects an array of elements at a\n *   single node of the object tree, and returns a `DeepMapResult`.  The\n *   `DeepMapResult` either provides a result value for that node (i.e.,\n *   representing the subtree), or indicates that the node should be processed\n *   recursively.  The default zipFn recurses as far as possible and places\n *   arrays at the leaves.\n */\nexport function deepZip(inputs, zipFn = zipToList) {\n    return deepZipInternal(inputs, zipFn);\n}\n/**\n * @param containedIn: An set containing objects on the reference path currently\n *   being processed (used to detect cycles).\n */\nfunction deepZipInternal(inputs, zipFn, containedIn = new Set()) {\n    // The recursion follows the structure of input 0; it's assumed that all the\n    // other inputs have the same structure.\n    const input = inputs[0];\n    if (containedIn.has(input)) {\n        throw new Error('Circular references are not supported.');\n    }\n    const result = zipFn(inputs);\n    if (result.recurse && result.value !== null) {\n        throw new Error('A deep zip function may not return both a value and recurse=true.');\n    }\n    if (!result.recurse) {\n        return result.value;\n    }\n    else if (isIterable(input)) {\n        // tslint:disable-next-line:no-any\n        const mappedIterable = Array.isArray(input) ? [] : {};\n        containedIn.add(input);\n        for (const k in input) {\n            const children = inputs.map(x => x[k]);\n            const childResult = deepZipInternal(children, zipFn, containedIn);\n            mappedIterable[k] = childResult;\n        }\n        containedIn.delete(input);\n        return mappedIterable;\n    }\n    else {\n        throw new Error(`Can't recurse into non-iterable type: ${input}`);\n    }\n}\n// tslint:disable-next-line:no-any\nexport function zipToList(x) {\n    if (x === null) {\n        return null;\n    }\n    // TODO(soergel): validate array type?\n    if (isIterable(x[0])) {\n        return { value: null, recurse: true };\n    }\n    else {\n        return { value: x, recurse: false };\n    }\n}\n/**\n * Apply an async mapping function to a nested structure in a recursive manner.\n *\n * This first creates a nested structure of Promises, and then awaits all of\n * those, resulting in a single Promise for a resolved nested structure.\n *\n * The result of the mapping is an object with the same nested structure (i.e.,\n * of arrays and dicts) as the input, except that some subtrees are replaced,\n * according to the results of the mapping function.\n *\n * Mappings are memoized.  Thus, if the nested structure contains the same\n * object in multiple positions, the output will contain the same mapped object\n * in those positions.  Cycles are not supported, however.\n *\n * @param input: The object to which to apply the mapping function.\n * @param mapFn: A function that expects a single node of the object tree, and\n *   returns a `DeepMapAsyncResult`.  The `DeepMapAsyncResult` either provides\n *   a `Promise` for a replacement value for that node (i.e., replacing the\n *   subtree), or indicates that the node should be processed recursively.  Note\n *   that the decision whether or not to recurse must be made immediately; only\n *   the mapped value may be promised.\n */\nexport async function deepMapAndAwaitAll(input, mapFn) {\n    const seen = new Map();\n    // First do a normal deepMap, collecting Promises in 'seen' as a side effect.\n    deepMapInternal(input, mapFn, seen);\n    // Replace the Promises in 'seen' in place.\n    // Note TypeScript provides no async map iteration, and regular map iteration\n    // is broken too, so sadly we have to do Array.from() to make it work.\n    // (There's no advantage to Promise.all(), and that would be tricky anyway.)\n    for (const key of Array.from(seen.keys())) {\n        const value = seen.get(key);\n        if (tf.util.isPromise(value)) {\n            const mappedValue = await value;\n            seen.set(key, mappedValue);\n        }\n    }\n    // Normal deepMap again, this time filling in the resolved values.\n    // It's unfortunate that we have to do two passes.\n    // TODO(soergel): test performance and think harder about a fast solution.\n    const result = deepMapInternal(input, mapFn, seen);\n    return result;\n}\n/**\n * Determine whether the argument is iterable.\n *\n * @returns true if the argument is an array or any non-Tensor object.\n */\n// tslint:disable-next-line:no-any\nexport function isIterable(obj) {\n    return obj != null && (!ArrayBuffer.isView(obj)) &&\n        (Array.isArray(obj) ||\n            (typeof obj === 'object' && !(obj instanceof tf.Tensor)));\n}\n/**\n * Determine whether the argument can be converted to Tensor.\n *\n * Tensors, primitives, arrays, and TypedArrays all qualify; anything else does\n * not.\n *\n * @returns true if the argument can be converted to Tensor.\n */\n// tslint:disable-next-line:no-any\nexport function canTensorify(obj) {\n    return obj == null || isPrimitive(obj) || Array.isArray(obj) ||\n        (typeof obj === 'object' && (obj instanceof tf.Tensor)) ||\n        tf.util.isTypedArray(obj);\n}\n/**\n * Returns true if the given `value` is a primitive type. Otherwise returns\n * false. This is equivalant to node util.isPrimitive\n */\nfunction isPrimitive(value) {\n    return (value === null ||\n        (typeof value !== 'object' && typeof value !== 'function'));\n}\n//# sourceMappingURL=deep_map.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport { deepMap, isIterable } from './deep_map';\nexport function deepClone(container) {\n    return deepMap(container, cloneIfTensor);\n}\n// tslint:disable-next-line: no-any\nfunction cloneIfTensor(item) {\n    if (item instanceof tf.Tensor) {\n        return ({ value: item.clone(), recurse: false });\n    }\n    else if (isIterable(item)) {\n        return { value: null, recurse: true };\n    }\n    else {\n        return { value: item, recurse: false };\n    }\n}\n//# sourceMappingURL=deep_clone.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n/**\n * A ring buffer, providing O(1) FIFO, LIFO, and related operations.\n */\nexport class RingBuffer {\n    /**\n     * Constructs a `RingBuffer`.\n     * @param capacity The number of items that the buffer can accomodate.\n     */\n    constructor(capacity) {\n        this.capacity = capacity;\n        // Note we store the indices in the range 0 <= index < 2*capacity.\n        // This allows us to distinguish the full from the empty case.\n        // See https://www.snellman.net/blog/archive/2016-12-13-ring-buffers/\n        this.begin = 0; // inclusive\n        this.end = 0; // exclusive\n        if (capacity == null) {\n            throw new RangeError('Can\\'t create a ring buffer of unknown capacity.');\n        }\n        if (capacity < 1) {\n            throw new RangeError('Can\\'t create ring buffer of capacity < 1.');\n        }\n        this.data = new Array(capacity);\n        this.doubledCapacity = 2 * capacity;\n    }\n    /**\n     * Map any index into the range 0 <= index < 2*capacity.\n     */\n    wrap(index) {\n        // don't trust % on negative numbers\n        while (index < 0) {\n            index += this.doubledCapacity;\n        }\n        return index % this.doubledCapacity;\n    }\n    get(index) {\n        if (index < 0) {\n            throw new RangeError('Can\\'t get item at a negative index.');\n        }\n        return this.data[index % this.capacity];\n    }\n    set(index, value) {\n        if (index < 0) {\n            throw new RangeError('Can\\'t set item at a negative index.');\n        }\n        this.data[index % this.capacity] = value;\n    }\n    /**\n     * Returns the current number of items in the buffer.\n     */\n    length() {\n        let length = this.end - this.begin;\n        if (length < 0) {\n            length = this.doubledCapacity + length;\n        }\n        return length;\n    }\n    /**\n     * Reports whether the buffer is full.\n     * @returns true if the number of items in the buffer equals its capacity, and\n     *   false otherwise.\n     */\n    isFull() {\n        return this.length() === this.capacity;\n    }\n    /**\n     * Reports whether the buffer is empty.\n     * @returns true if the number of items in the buffer equals zero, and\n     *   false otherwise.\n     */\n    isEmpty() {\n        return this.length() === 0;\n    }\n    /**\n     * Adds an item to the end of the buffer.\n     */\n    push(value) {\n        if (this.isFull()) {\n            throw new RangeError('Ring buffer is full.');\n        }\n        this.set(this.end, value);\n        this.end = this.wrap(this.end + 1);\n    }\n    /**\n     * Adds many items to the end of the buffer, in order.\n     */\n    pushAll(values) {\n        for (const value of values) {\n            this.push(value);\n        }\n    }\n    /**\n     * Removes and returns the last item in the buffer.\n     */\n    pop() {\n        if (this.isEmpty()) {\n            throw new RangeError('Ring buffer is empty.');\n        }\n        this.end = this.wrap(this.end - 1);\n        const result = this.get(this.end);\n        this.set(this.end, undefined);\n        return result;\n    }\n    /**\n     * Adds an item to the beginning of the buffer.\n     */\n    unshift(value) {\n        if (this.isFull()) {\n            throw new RangeError('Ring buffer is full.');\n        }\n        this.begin = this.wrap(this.begin - 1);\n        this.set(this.begin, value);\n    }\n    /**\n     * Removes and returns the first item in the buffer.\n     */\n    shift() {\n        if (this.isEmpty()) {\n            throw new RangeError('Ring buffer is empty.');\n        }\n        const result = this.get(this.begin);\n        this.set(this.begin, undefined);\n        this.begin = this.wrap(this.begin + 1);\n        return result;\n    }\n    /**\n     * Removes and returns a specific item in the buffer, and moves the last item\n     * to the vacated slot.  This is useful for implementing a shuffling stream.\n     * Note that this operation necessarily scrambles the original order.\n     *\n     * @param relativeIndex: the index of the item to remove, relative to the\n     *   first item in the buffer (e.g., hiding the ring nature of the underlying\n     *   storage).\n     */\n    shuffleExcise(relativeIndex) {\n        if (this.isEmpty()) {\n            throw new RangeError('Ring buffer is empty.');\n        }\n        const index = this.wrap(this.begin + relativeIndex);\n        const result = this.get(index);\n        this.set(index, this.pop());\n        return result;\n    }\n}\n//# sourceMappingURL=ring_buffer.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport { RingBuffer } from './ring_buffer';\nexport class GrowingRingBuffer extends RingBuffer {\n    /**\n     * Constructs a `GrowingRingBuffer`.\n     */\n    constructor() {\n        super(GrowingRingBuffer.INITIAL_CAPACITY);\n    }\n    isFull() {\n        return false;\n    }\n    push(value) {\n        if (super.isFull()) {\n            this.expand();\n        }\n        super.push(value);\n    }\n    unshift(value) {\n        if (super.isFull()) {\n            this.expand();\n        }\n        super.unshift(value);\n    }\n    /**\n     * Doubles the capacity of the buffer.\n     */\n    expand() {\n        const newCapacity = this.capacity * 2;\n        const newData = new Array(newCapacity);\n        const len = this.length();\n        // Rotate the buffer to start at index 0 again, since we can't just\n        // allocate more space at the end.\n        for (let i = 0; i < len; i++) {\n            newData[i] = this.get(this.wrap(this.begin + i));\n        }\n        this.data = newData;\n        this.capacity = newCapacity;\n        this.doubledCapacity = 2 * this.capacity;\n        this.begin = 0;\n        this.end = len;\n    }\n}\nGrowingRingBuffer.INITIAL_CAPACITY = 32;\n//# sourceMappingURL=growing_ring_buffer.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\nimport { iteratorFromConcatenated, iteratorFromFunction, iteratorFromItems, iteratorFromZipped, ZipMismatchMode } from './iterators/lazy_iterator';\nimport { canTensorify, deepMapAndAwaitAll, isIterable } from './util/deep_map';\n// TODO(soergel): consider vectorized operations within the pipeline.\n/**\n * Represents a potentially large list of independent data elements (typically\n * 'samples' or 'examples').\n *\n * A 'data example' may be a primitive, an array, a map from string keys to\n * values, or any nested structure of these.\n *\n * A `Dataset` represents an ordered collection of elements, together with a\n * chain of transformations to be performed on those elements. Each\n * transformation is a method of `Dataset` that returns another `Dataset`, so\n * these may be chained, e.g.\n * `const processedDataset = rawDataset.filter(...).map(...).batch(...)`.\n *\n * Data loading and transformation is done in a lazy, streaming fashion.  The\n * dataset may be iterated over multiple times; each iteration starts the data\n * loading anew and recapitulates the transformations.\n *\n * A `Dataset` is typically processed as a stream of unbatched examples --i.e.,\n * its transformations are applied one example at a time. Batching produces a\n * new `Dataset` where each element is a batch. Batching should usually come\n * last in a pipeline, because data transformations are easier to express on a\n * per-example basis than on a per-batch basis.\n *\n * The following code examples are calling `await dataset.forEachAsync(...)` to\n * iterate once over the entire dataset in order to print out the data.\n *\n * @doc {heading: 'Data', subheading: 'Classes', namespace: 'data'}\n */\nexport class Dataset {\n    constructor() {\n        this.size = null;\n    }\n    // TODO(soergel): Make Datasets report whether repeated iterator() calls\n    // produce the same result (e.g., reading from a file) or different results\n    // (e.g., from the webcam).  Currently we don't make this distinction but it\n    // could be important for the user to know.\n    // abstract isDeterministic(): boolean;\n    /**\n     * Groups elements into batches.\n     *\n     * It is assumed that each of the incoming dataset elements has the same\n     * structure-- i.e. the same set of keys at each location in an object\n     * hierarchy.  For each key, the resulting `Dataset` provides a batched\n     * element collecting all of the incoming values for that key.\n     *\n     *  * Incoming primitives are grouped into a 1-D Tensor.\n     *  * Incoming Tensors are grouped into a new Tensor where the 0'th axis is\n     *    the batch dimension.\n     *  * Incoming arrays are converted to Tensor and then batched.\n     *  * A nested array is interpreted as an n-D Tensor, so the batched result\n     *    has n+1 dimensions.\n     *  * An array that cannot be converted to Tensor produces an error.\n     *\n     * If an array should not be batched as a unit, it should first be converted\n     * to an object with integer keys.\n     *\n     * Here are a few examples:\n     *\n     * Batch a dataset of numbers:\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8]).batch(4);\n     * await a.forEachAsync(e => e.print());\n     * ```\n     *\n     * Batch a dataset of arrays:\n     * ```js\n     * const b = tf.data.array([[1], [2], [3], [4], [5], [6], [7], [8]]).batch(4);\n     * await b.forEachAsync(e => e.print());\n     * ```\n     *\n     * Batch a dataset of objects:\n     * ```js\n     * const c = tf.data.array([{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13},\n     *   {a: 4, b: 14}, {a: 5, b: 15}, {a: 6, b: 16}, {a: 7, b: 17},\n     *   {a: 8, b: 18}]).batch(4);\n     * await c.forEachAsync(e => {\n     *   console.log('{');\n     *   for(var key in e) {\n     *     console.log(key+':');\n     *     e[key].print();\n     *   }\n     *   console.log('}');\n     * })\n     * ```\n     *\n     * @param batchSize The number of elements desired per batch.\n     * @param smallLastBatch Whether to emit the final batch when it has fewer\n     *   than batchSize elements. Default true.\n     * @returns A `Dataset`, from which a stream of batches can be obtained.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    batch(batchSize, smallLastBatch = true) {\n        const base = this;\n        tf.util.assert(batchSize > 0, () => `batchSize needs to be positive, but it is\n      ${batchSize}`);\n        let size;\n        if (this.size === Infinity || this.size == null) {\n            // If the size of this dataset is infinity or null, the new size keeps the\n            // same.\n            size = this.size;\n        }\n        else if (smallLastBatch) {\n            // If the size of this dataset is known and include small last batch, the\n            // new size is full batch count plus last batch.\n            size = Math.ceil(this.size / batchSize);\n        }\n        else {\n            // If the size of this dataset is known and not include small last batch,\n            // the new size is full batch count.\n            size = Math.floor(this.size / batchSize);\n        }\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator())\n                .columnMajorBatch(batchSize, smallLastBatch, deepBatchConcat);\n        }, size);\n    }\n    /**\n     * Concatenates this `Dataset` with another.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]);\n     * const b = tf.data.array([4, 5, 6]);\n     * const c = a.concatenate(b);\n     * await c.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param dataset A `Dataset` to be concatenated onto this one.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    concatenate(dataset) {\n        const base = this;\n        let size;\n        if (this.size === Infinity || dataset.size === Infinity) {\n            // If the size of any of these two dataset is infinity, new size is\n            // infinity.\n            size = Infinity;\n        }\n        else if (this.size != null && dataset.size != null) {\n            // If the size of both datasets are known and not infinity, new size is\n            // sum the size of these two datasets.\n            size = this.size + dataset.size;\n        }\n        else {\n            // If neither of these two datasets has infinite size and any of these two\n            // datasets' size is null, the new size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => (await base.iterator()).concatenate(await dataset.iterator()), size);\n    }\n    /**\n     * Filters this dataset according to `predicate`.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n     *   .filter(x => x%2 === 0);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param predicate A function mapping a dataset element to a boolean or a\n     * `Promise` for one.\n     *\n     * @returns A `Dataset` of elements for which the predicate was true.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    filter(predicate) {\n        const base = this;\n        let size;\n        if (this.size === Infinity) {\n            // If the size of this dataset is infinity, new size is infinity\n            size = Infinity;\n        }\n        else {\n            // If this dataset has limited elements, new size is null because it might\n            // exhausted randomly.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator()).filter(x => tf.tidy(() => predicate(x)));\n        }, size);\n    }\n    /**\n     * Apply a function to every element of the dataset.\n     *\n     * After the function is applied to a dataset element, any Tensors contained\n     * within that element are disposed.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param f A function to apply to each dataset element.\n     * @returns A `Promise` that resolves after all elements have been processed.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    async forEachAsync(f) {\n        return (await this.iterator()).forEachAsync(f);\n    }\n    /**\n     * Maps this dataset through a 1-to-1 transform.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]).map(x => x*x);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param transform A function mapping a dataset element to a transformed\n     *   dataset element.\n     *\n     * @returns A `Dataset` of transformed elements.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    map(transform) {\n        const base = this;\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator()).map(x => tf.tidy(() => transform(x)));\n        }, this.size);\n    }\n    /**\n     * Maps this dataset through an async 1-to-1 transform.\n     *\n     * ```js\n     * const a =\n     *  tf.data.array([1, 2, 3]).mapAsync(x => new Promise(function(resolve){\n     *    setTimeout(() => {\n     *      resolve(x * x);\n     *    }, Math.random()*1000 + 500);\n     *  }));\n     * console.log(await a.toArray());\n     * ```\n     *\n     * @param transform A function mapping a dataset element to a `Promise` for a\n     *   transformed dataset element.  This transform is responsible for disposing\n     *   any intermediate `Tensor`s, i.e. by wrapping its computation in\n     *   `tf.tidy()`; that cannot be automated here (as it is in the synchronous\n     *   `map()` case).\n     *\n     * @returns A `Dataset` of transformed elements.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    mapAsync(transform) {\n        const base = this;\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator()).mapAsync(transform);\n        }, this.size);\n    }\n    /**\n     *  Creates a `Dataset` that prefetches elements from this dataset.\n     *\n     * @param bufferSize: An integer specifying the number of elements to be\n     *   prefetched.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    prefetch(bufferSize) {\n        if (bufferSize == null) {\n            throw new RangeError('`Dataset.prefetch()` requires bufferSize to be specified.');\n        }\n        const base = this;\n        return datasetFromIteratorFn(async () => (await base.iterator()).prefetch(bufferSize), this.size);\n    }\n    /**\n     * Repeats this dataset `count` times.\n     *\n     * NOTE: If this dataset is a function of global state (e.g. a random number\n     * generator), then different repetitions may produce different elements.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]).repeat(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param count: (Optional) An integer, representing the number of times\n     *   the dataset should be repeated. The default behavior (if `count` is\n     *   `undefined` or negative) is for the dataset be repeated indefinitely.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    repeat(count) {\n        const base = this;\n        let size;\n        if (this.size != null && count > 0) {\n            // If this dataset has size and count is positive, new size is current\n            // size multiply count. This also covers the case that current size is\n            // infinity.\n            size = this.size * count;\n        }\n        else if (count === 0) {\n            // If count is 0, new size is 0.\n            size = 0;\n        }\n        else if (this.size != null && (count === undefined || count < 0)) {\n            // If this dataset has size and count is undefined or negative, the\n            // dataset will be repeated indefinitely and new size is infinity.\n            size = Infinity;\n        }\n        else {\n            // If the size of this dataset is null, the new dataset's size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => {\n            const iteratorIterator = iteratorFromFunction(async () => ({ value: await base.iterator(), done: false }));\n            return iteratorFromConcatenated(iteratorIterator.take(count));\n        }, size);\n    }\n    /**\n     * Creates a `Dataset` that skips `count` initial elements from this dataset.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).skip(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param count: The number of elements of this dataset that should be skipped\n     *   to form the new dataset.  If `count` is greater than the size of this\n     *   dataset, the new dataset will contain no elements.  If `count`\n     *   is `undefined` or negative, skips the entire dataset.\n     *\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    skip(count) {\n        const base = this;\n        let size;\n        if (this.size != null && count >= 0 && this.size >= count) {\n            // If the size of this dataset is greater than count, the new dataset's\n            // size is current size minus skipped size.This also covers the case that\n            // current size is infinity.\n            size = this.size - count;\n        }\n        else if (this.size != null &&\n            (this.size < count || count === undefined || count < 0)) {\n            // If the size of this dataset is smaller than count, or count is\n            // undefined or negative, skips the entire dataset and the new size is 0.\n            size = 0;\n        }\n        else {\n            // If the size of this dataset is null, the new dataset's size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => (await base.iterator()).skip(count), size);\n    }\n    /**\n     * Pseudorandomly shuffles the elements of this dataset. This is done in a\n     * streaming manner, by sampling from a given number of prefetched elements.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).shuffle(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param bufferSize: An integer specifying the number of elements from this\n     *   dataset from which the new dataset will sample.\n     * @param seed: (Optional) An integer specifying the random seed that will\n     *   be used to create the distribution.\n     * @param reshuffleEachIteration: (Optional) A boolean, which if true\n     *   indicates that the dataset should be pseudorandomly reshuffled each time\n     *   it is iterated over. If false, elements will be returned in the same\n     *   shuffled order on each iteration. (Defaults to `true`.)\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    shuffle(bufferSize, seed, reshuffleEachIteration = true) {\n        if (bufferSize == null || bufferSize < 0) {\n            if (this.size == null) {\n                throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.');\n            }\n            else {\n                throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.  ' +\n                    'If your data fits in main memory (for regular JS objects), ' +\n                    'and/or GPU memory (for `tf.Tensor`s), consider setting ' +\n                    `bufferSize to the dataset size (${this.size} elements)`);\n            }\n        }\n        const base = this;\n        const random = seedrandom.alea(seed || tf.util.now().toString());\n        return datasetFromIteratorFn(async () => {\n            let seed2 = random.int32();\n            if (reshuffleEachIteration) {\n                seed2 += random.int32();\n            }\n            return (await base.iterator()).shuffle(bufferSize, seed2.toString());\n        }, this.size);\n    }\n    /**\n     * Creates a `Dataset` with at most `count` initial elements from this\n     * dataset.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).take(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param count: The number of elements of this dataset that should be taken\n     *   to form the new dataset.  If `count` is `undefined` or negative, or if\n     *   `count` is greater than the size of this dataset, the new dataset will\n     *   contain all elements of this dataset.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    take(count) {\n        const base = this;\n        let size;\n        if (this.size != null && this.size > count) {\n            // If the size of this dataset is greater than count, the new dataset's\n            // size is count.\n            size = count;\n        }\n        else if (this.size != null && this.size <= count) {\n            // If the size of this dataset is equal or smaller than count, the new\n            // dataset's size is the size of this dataset.\n            size = this.size;\n        }\n        else {\n            // If the size of this dataset is null, the new dataset's size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => (await base.iterator()).take(count), size);\n    }\n    /**\n     * Collect all elements of this dataset into an array.\n     *\n     * Obviously this will succeed only for small datasets that fit in memory.\n     * Useful for testing and generally should be avoided if possible.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]);\n     * console.log(await a.toArray());\n     * ```\n     *\n     * @returns A Promise for an array of elements, which will resolve\n     *   when a new stream has been obtained and fully consumed.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    async toArray() {\n        if (this.size === Infinity) {\n            throw new Error('Can not convert infinite data stream to array.');\n        }\n        return (await this.iterator()).toArray();\n    }\n    /**\n     * Collect all elements of this dataset into an array with prefetching 100\n     * elements. This is useful for testing, because the prefetch changes the\n     * order in which the Promises are resolved along the processing pipeline.\n     * This may help expose bugs where results are dependent on the order of\n     * Promise resolution rather than on the logical order of the stream (i.e.,\n     * due to hidden mutable state).\n     *\n     * @returns A Promise for an array of elements, which will resolve\n     *   when a new stream has been obtained and fully consumed.\n     */\n    async toArrayForTest() {\n        if (this.size === Infinity) {\n            throw new Error('Can not convert infinite data stream to array.');\n        }\n        return (await this.iterator()).toArrayForTest();\n    }\n}\n// TODO(soergel): deep sharded shuffle, where supported\nDataset.MAX_BUFFER_SIZE = 10000;\n/**\n * Create a `Dataset` defined by a provided iterator() function.\n *\n * ```js\n * let i = -1;\n * const func = () =>\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\n * const iter = tf.data.iteratorFromFunction(func);\n * const ds = tf.data.datasetFromIteratorFn(iter);\n * await ds.forEachAsync(e => console.log(e));\n * ```\n */\nexport function datasetFromIteratorFn(iteratorFn, size = null) {\n    return new class extends Dataset {\n        constructor() {\n            super(...arguments);\n            this.size = size;\n        }\n        /*\n         * Provide a new stream of elements.  Note this will also start new streams\n         * from any underlying `Dataset`s.\n         */\n        async iterator() {\n            return iteratorFn();\n        }\n    }();\n}\n/**\n * Create a `Dataset` from an array of elements.\n *\n * Create a Dataset from an array of objects:\n * ```js\n * const a = tf.data.array([{'item': 1}, {'item': 2}, {'item': 3}]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n *\n * Create a Dataset from an array of numbers:\n * ```js\n * const a = tf.data.array([4, 5, 6]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n * @param items An array of elements that will be parsed as items in a dataset.\n *\n * @doc {heading: 'Data', subheading: 'Creation', namespace: 'data'}\n */\nexport function array(items) {\n    return datasetFromIteratorFn(async () => iteratorFromItems(items), items.length);\n}\n/**\n * Create a `Dataset` by zipping together an array, dict, or nested\n * structure of `Dataset`s (and perhaps additional constants).\n * The underlying datasets must provide elements in a consistent order such that\n * they correspond.\n *\n * The number of elements in the resulting dataset is the same as the size of\n * the smallest dataset in datasets.\n *\n * The nested structure of the `datasets` argument determines the\n * structure of elements in the resulting iterator.\n *\n * Note this means that, given an array of two datasets that produce dict\n * elements, the result is a dataset that produces elements that are arrays\n * of two dicts:\n *\n * Zip an array of datasets:\n * ```js\n * console.log('Zip two datasets of objects:');\n * const ds1 = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const ds2 = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const ds3 = tf.data.zip([ds1, ds2]);\n * await ds3.forEachAsync(e => console.log(JSON.stringify(e)));\n *\n * // If the goal is to merge the dicts in order to produce elements like\n * // {a: ..., b: ...}, this requires a second step such as:\n * console.log('Merge the objects:');\n * const ds4 = ds3.map(x => {return {a: x[0].a, b: x[1].b}});\n * await ds4.forEachAsync(e => console.log(e));\n * ```\n *\n * Zip a dict of datasets:\n * ```js\n * const a = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const b = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const c = tf.data.zip({c: a, d: b});\n * await c.forEachAsync(e => console.log(JSON.stringify(e)));\n * ```\n *\n * @doc {heading: 'Data', subheading: 'Operations', namespace: 'data'}\n */\nexport function zip(datasets) {\n    // manually type-check the argument for JS users\n    if (!isIterable(datasets)) {\n        throw new Error('The argument to zip() must be an object or array.');\n    }\n    let size;\n    if (Array.isArray(datasets)) {\n        for (let i = 0; i < datasets.length; i++) {\n            size = size == null ? datasets[i].size :\n                Math.min(size, datasets[i].size);\n        }\n    }\n    else if (datasets instanceof Object) {\n        for (const ds in datasets) {\n            size = size == null ? datasets[ds].size :\n                Math.min(size, datasets[ds].size);\n        }\n    }\n    return datasetFromIteratorFn(async () => {\n        const streams = await deepMapAndAwaitAll(datasets, d => {\n            if (d instanceof Dataset) {\n                return { value: d.iterator(), recurse: false };\n            }\n            else if (isIterable(d)) {\n                return { value: null, recurse: true };\n            }\n            else {\n                throw new Error('Leaves of the structure passed to zip() must be Datasets, ' +\n                    'not primitives.');\n            }\n        });\n        return iteratorFromZipped(streams, ZipMismatchMode.SHORTEST);\n    }, size);\n}\n/**\n * A zip function for use with deepZip, passed via the columnMajorBatch call.\n *\n * Accepts an array of identically-structured nested elements and either batches\n * them (if they are primitives, numeric arrays, or Tensors) or requests\n * recursion (if not).\n */\n// tslint:disable-next-line:no-any\nfunction deepBatchConcat(rows) {\n    if (rows === null) {\n        return null;\n    }\n    // use the first item to decide whether to recurse or batch here.\n    const exampleRow = rows[0];\n    if (canTensorify(exampleRow)) {\n        // rows is an array of primitives, Tensors, or arrays.  Batch them.\n        const value = batchConcat(rows);\n        return { value, recurse: false };\n    }\n    // the example row is an object, so recurse into it.\n    return { value: null, recurse: true };\n}\n/**\n * Assembles a list of same-shaped numbers, number arrays, or Tensors\n * into a single new Tensor where axis 0 is the batch dimension.\n */\nfunction batchConcat(arrays) {\n    if (arrays.length === 0) {\n        // We can't return an empty Tensor because we don't know the element shape.\n        throw new Error('Can\\'t make a batch of zero elements.');\n    }\n    if (arrays[0] instanceof tf.Tensor) {\n        // Input is an array of Tensors\n        return tf.stack(arrays);\n    }\n    else {\n        // Input is a possibly-nested array of numbers.\n        return tf.tensor(arrays);\n    }\n}\n//# sourceMappingURL=dataset.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport { util } from '@tensorflow/tfjs-core';\nimport { Dataset } from '../dataset';\nimport { TextLineDataset } from './text_line_dataset';\nconst CODE_QUOTE = '\"';\nconst STATE_OUT = Symbol('out');\nconst STATE_FIELD = Symbol('field');\nconst STATE_QUOTE = Symbol('quote');\nconst STATE_QUOTE_AFTER_QUOTE = Symbol('quoteafterquote');\nconst STATE_WITHIN_QUOTE_IN_QUOTE = Symbol('quoteinquote');\n/**\n * Represents a potentially large collection of delimited text records.\n *\n * The produced `TensorContainer`s each contain one key-value pair for\n * every column of the table.  When a field is empty in the incoming data, the\n * resulting value is `undefined`, or throw error if it is required.  Values\n * that can be parsed as numbers are emitted as type `number`, other values\n * are parsed as `string`.\n *\n * The results are not batched.\n *\n * @doc {heading: 'Data', subheading: 'Classes', namespace: 'data'}\n */\nexport class CSVDataset extends Dataset {\n    /**\n     * Create a `CSVDataset`.\n     *\n     * @param input A `DataSource` providing a chunked, UTF8-encoded byte stream.\n     * @param csvConfig (Optional) A CSVConfig object that contains configurations\n     *     of reading and decoding from CSV file(s).\n     *\n     *     hasHeader: (Optional) A boolean value that indicates whether the first\n     *     row of provided CSV file is a header line with column names, and should\n     *     not be included in the data. Defaults to `true`.\n     *\n     *     columnNames: (Optional) A list of strings that corresponds to\n     *     the CSV column names, in order. If provided, it ignores the column\n     *     names inferred from the header row. If not provided, infers the column\n     *     names from the first row of the records. If hasHeader is false and\n     *     columnNames is not provided, this method throws an error.\n     *\n     *     columnConfigs: (Optional) A dictionary whose key is column names, value\n     *     is an object stating if this column is required, column's data type,\n     *     default value, and if this column is label. If provided, keys must\n     *     correspond to names provided in columnNames or inferred from the file\n     *     header lines. If isLabel is true any column, returns an array of two\n     *     items: the first item is a dict of features key/value pairs, the second\n     *     item is a dict of labels key/value pairs. If no feature is marked as\n     *     label, returns a dict of features only.\n     *\n     *     configuredColumnsOnly (Optional) If true, only columns provided in\n     *     columnConfigs will be parsed and provided during iteration.\n     *\n     *     delimiter (Optional) The string used to parse each line of the input\n     *     file. Defaults to `,`.\n     */\n    constructor(input, csvConfig) {\n        super();\n        this.input = input;\n        this.hasHeader = true;\n        this.fullColumnNames = null;\n        this.columnNamesValidated = false;\n        this.columnConfigs = null;\n        this.configuredColumnsOnly = false;\n        this.delimiter = ',';\n        this.delimWhitespace = false;\n        this.base = new TextLineDataset(input);\n        if (!csvConfig) {\n            csvConfig = {};\n        }\n        this.hasHeader = csvConfig.hasHeader === false ? false : true;\n        this.fullColumnNames = csvConfig.columnNames;\n        this.columnConfigs = csvConfig.columnConfigs;\n        this.configuredColumnsOnly = csvConfig.configuredColumnsOnly;\n        if (csvConfig.delimWhitespace) {\n            util.assert(csvConfig.delimiter == null, () => 'Delimiter should not be provided when delimWhitespace is true.');\n            this.delimWhitespace = true;\n            this.delimiter = ' ';\n        }\n        else {\n            this.delimiter = csvConfig.delimiter ? csvConfig.delimiter : ',';\n        }\n    }\n    /**\n     * Returns column names of the csv dataset. If `configuredColumnsOnly` is\n     * true, return column names in `columnConfigs`. If `configuredColumnsOnly` is\n     * false and `columnNames` is provided, `columnNames`. If\n     * `configuredColumnsOnly` is false and `columnNames` is not provided, return\n     * all column names parsed from the csv file. For example usage please go to\n     * `tf.data.csv`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    async columnNames() {\n        if (!this.columnNamesValidated) {\n            await this.setColumnNames();\n        }\n        return this.configuredColumnsOnly ? Object.keys(this.columnConfigs) :\n            this.fullColumnNames;\n    }\n    /* 1) If `columnNames` is provided as string[], use this string[] as output\n     * keys in corresponding order. The length must match the number of inferred\n     * columns if `hasHeader` is true .\n     * 2) If `columnNames` is not provided, parse header line as `columnNames` if\n     * hasHeader is true. If `hasHeader` is false, throw an error.\n     * 3) If `columnConfigs` is provided, all the keys in `columnConfigs` must\n     * exist in parsed `columnNames`.\n     */\n    async setColumnNames() {\n        const columnNamesFromFile = await this.maybeReadHeaderLine();\n        if (!this.fullColumnNames && !columnNamesFromFile) {\n            // Throw an error if columnNames is not provided and no header line.\n            throw new Error('Column names must be provided if there is no header line.');\n        }\n        else if (this.fullColumnNames && columnNamesFromFile) {\n            // Check provided columnNames match header line.\n            util.assert(columnNamesFromFile.length === this.fullColumnNames.length, () => 'The length of provided columnNames (' +\n                this.fullColumnNames.length.toString() +\n                ') does not match the length of the header line read from ' +\n                'file (' + columnNamesFromFile.length.toString() + ').');\n        }\n        if (!this.fullColumnNames) {\n            this.fullColumnNames = columnNamesFromFile;\n        }\n        // Check if there are duplicate column names.\n        const counts = this.fullColumnNames.reduce((countAcc, name) => {\n            countAcc[name] = (countAcc[name] + 1) || 1;\n            return countAcc;\n        }, {});\n        const duplicateNames = Object.keys(counts).filter((name) => (counts[name] > 1));\n        util.assert(duplicateNames.length === 0, () => 'Duplicate column names found: ' + duplicateNames.toString());\n        // Check if keys in columnConfigs match columnNames.\n        if (this.columnConfigs) {\n            for (const key of Object.keys(this.columnConfigs)) {\n                const index = this.fullColumnNames.indexOf(key);\n                if (index === -1) {\n                    throw new Error('The key \"' + key +\n                        '\" provided in columnConfigs does not match any of the column ' +\n                        'names (' + this.fullColumnNames.toString() + ').');\n                }\n            }\n        }\n        this.columnNamesValidated = true;\n    }\n    async maybeReadHeaderLine() {\n        if (this.hasHeader) {\n            const iter = await this.base.iterator();\n            const firstElement = await iter.next();\n            if (firstElement.done) {\n                throw new Error('No data was found for CSV parsing.');\n            }\n            const firstLine = firstElement.value;\n            const headers = this.parseRow(firstLine, false);\n            return headers;\n        }\n        else {\n            return null;\n        }\n    }\n    async iterator() {\n        if (!this.columnNamesValidated) {\n            await this.setColumnNames();\n        }\n        let lines = await this.base.iterator();\n        if (this.hasHeader) {\n            // We previously read the first line to get the columnNames.\n            // Now that we're providing data, skip it.\n            lines = lines.skip(1);\n        }\n        return lines.map(x => this.makeDataElement(x));\n    }\n    makeDataElement(line) {\n        const values = this.parseRow(line);\n        const features = {};\n        const labels = {};\n        for (let i = 0; i < this.fullColumnNames.length; i++) {\n            const key = this.fullColumnNames[i];\n            const config = this.columnConfigs ? this.columnConfigs[key] : null;\n            if (this.configuredColumnsOnly && !config) {\n                // This column is not selected.\n                continue;\n            }\n            else {\n                const value = values[i];\n                let parsedValue = null;\n                if (value === '') {\n                    // If default value is provided, use it. If default value is not\n                    // provided, set as undefined.\n                    if (config && config.default !== undefined) {\n                        parsedValue = config.default;\n                    }\n                    else if (config && (config.required || config.isLabel)) {\n                        throw new Error(`Required column ${key} is empty in this line: ${line}`);\n                    }\n                    else {\n                        parsedValue = undefined;\n                    }\n                }\n                else {\n                    // A value is present, so parse it based on type\n                    const valueAsNum = Number(value);\n                    if (isNaN(valueAsNum)) {\n                        // The value is a string and this column is declared as boolean\n                        // in config, parse it as boolean.\n                        if (config && config.dtype === 'bool') {\n                            parsedValue = this.getBoolean(value);\n                        }\n                        else {\n                            // Set value as string\n                            parsedValue = value;\n                        }\n                    }\n                    else if (!config || !config.dtype) {\n                        // If this value is a number and no type config is provided, return\n                        // it as number.\n                        parsedValue = valueAsNum;\n                    }\n                    else {\n                        // If this value is a number and data type is provided, parse it\n                        // according to provided data type.\n                        switch (config.dtype) {\n                            case 'float32':\n                                parsedValue = valueAsNum;\n                                break;\n                            case 'int32':\n                                parsedValue = Math.floor(valueAsNum);\n                                break;\n                            case 'bool':\n                                parsedValue = this.getBoolean(value);\n                                break;\n                            default:\n                                parsedValue = valueAsNum;\n                        }\n                    }\n                }\n                // Check if this column is label.\n                (config && config.isLabel) ? labels[key] = parsedValue :\n                    features[key] = parsedValue;\n            }\n        }\n        // If label exists, return an object of features and labels as {xs:features,\n        // ys:labels}, otherwise return features only.\n        if (Object.keys(labels).length === 0) {\n            return features;\n        }\n        else {\n            return { xs: features, ys: labels };\n        }\n    }\n    getBoolean(value) {\n        if (value === '1' || value.toLowerCase() === 'true') {\n            return 1;\n        }\n        else {\n            return 0;\n        }\n    }\n    // adapted from https://beta.observablehq.com/@mbostock/streaming-csv\n    parseRow(line, validateElementCount = true) {\n        const result = [];\n        let readOffset = 0;\n        const readLength = line.length;\n        let currentState = STATE_OUT;\n        // Goes through the line to parse quote.\n        for (let i = 0; i < readLength; i++) {\n            switch (currentState) {\n                // Before enter a new field\n                case STATE_OUT:\n                    switch (line.charAt(i)) {\n                        // Enter a quoted field\n                        case CODE_QUOTE:\n                            readOffset = i + 1;\n                            currentState = STATE_QUOTE;\n                            break;\n                        // Read an empty field\n                        case this.delimiter:\n                            readOffset = i + 1;\n                            // If delimiter is white space and configured to collapse\n                            // multiple white spaces, ignore this white space.\n                            if (this.delimiter === ' ' && this.delimWhitespace) {\n                                break;\n                            }\n                            result.push('');\n                            currentState = STATE_OUT;\n                            break;\n                        // Enter an unquoted field\n                        default:\n                            currentState = STATE_FIELD;\n                            readOffset = i;\n                            break;\n                    }\n                    break;\n                // In an unquoted field\n                case STATE_FIELD:\n                    switch (line.charAt(i)) {\n                        // Exit an unquoted field, add it to result\n                        case this.delimiter:\n                            result.push(line.substring(readOffset, i));\n                            currentState = STATE_OUT;\n                            readOffset = i + 1;\n                            break;\n                        default:\n                    }\n                    break;\n                // In a quoted field\n                case STATE_QUOTE:\n                    switch (line.charAt(i)) {\n                        // Read a quote after a quote\n                        case CODE_QUOTE:\n                            currentState = STATE_QUOTE_AFTER_QUOTE;\n                            break;\n                        default:\n                    }\n                    break;\n                // This state means it's right after a second quote in a field\n                case STATE_QUOTE_AFTER_QUOTE:\n                    switch (line.charAt(i)) {\n                        // Finished a quoted field\n                        case this.delimiter:\n                            result.push(line.substring(readOffset, i - 1));\n                            currentState = STATE_OUT;\n                            readOffset = i + 1;\n                            break;\n                        // Finished a quoted part in a quoted field\n                        case CODE_QUOTE:\n                            currentState = STATE_QUOTE;\n                            break;\n                        // In a quoted part in a quoted field\n                        default:\n                            currentState = STATE_WITHIN_QUOTE_IN_QUOTE;\n                            break;\n                    }\n                    break;\n                case STATE_WITHIN_QUOTE_IN_QUOTE:\n                    switch (line.charAt(i)) {\n                        // Exit a quoted part in a quoted field\n                        case CODE_QUOTE:\n                            currentState = STATE_QUOTE;\n                            break;\n                        default:\n                    }\n                    break;\n                default:\n            }\n        }\n        // Adds last item based on if it is quoted.\n        if (currentState === STATE_QUOTE_AFTER_QUOTE) {\n            result.push(line.substring(readOffset, readLength - 1));\n        }\n        else {\n            result.push(line.substring(readOffset));\n        }\n        // Check if each row has the same number of elements as column names.\n        if (validateElementCount && result.length !== this.fullColumnNames.length) {\n            throw new Error(`Invalid row in csv file. Should have ${this.fullColumnNames.length} elements in a row, but got ${result}`);\n        }\n        return result;\n    }\n}\n// TODO(soergel): add more basic datasets for parity with tf.data\n// tf.data.FixedLengthRecordDataset()\n// tf.data.TFRecordDataset()\n//# sourceMappingURL=csv_dataset.js.map","/** @license See the LICENSE file. */\n// This code is auto-generated, do not modify this file!\nconst version = '3.6.0';\nexport { version };\n//# sourceMappingURL=version.js.map","/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env } from './environment';\nimport * as base from './util_base';\nexport * from './util_base';\n/**\n * Create typed array for scalar value. Used for storing in `DataStorage`.\n */\nexport function createScalarValue(value, dtype) {\n    if (dtype === 'string') {\n        return encodeString(value);\n    }\n    return toTypedArray([value], dtype);\n}\nfunction noConversionNeeded(a, dtype) {\n    return (a instanceof Float32Array && dtype === 'float32') ||\n        (a instanceof Int32Array && dtype === 'int32') ||\n        (a instanceof Uint8Array && dtype === 'bool');\n}\nexport function toTypedArray(a, dtype) {\n    if (dtype === 'string') {\n        throw new Error('Cannot convert a string[] to a TypedArray');\n    }\n    if (Array.isArray(a)) {\n        a = base.flatten(a);\n    }\n    if (env().getBool('DEBUG')) {\n        base.checkConversionForErrors(a, dtype);\n    }\n    if (noConversionNeeded(a, dtype)) {\n        return a;\n    }\n    if (dtype == null || dtype === 'float32' || dtype === 'complex64') {\n        return new Float32Array(a);\n    }\n    else if (dtype === 'int32') {\n        return new Int32Array(a);\n    }\n    else if (dtype === 'bool') {\n        const bool = new Uint8Array(a.length);\n        for (let i = 0; i < bool.length; ++i) {\n            if (Math.round(a[i]) !== 0) {\n                bool[i] = 1;\n            }\n        }\n        return bool;\n    }\n    else {\n        throw new Error(`Unknown data type ${dtype}`);\n    }\n}\n/**\n * Returns the current high-resolution time in milliseconds relative to an\n * arbitrary time in the past. It works across different platforms (node.js,\n * browsers).\n *\n * ```js\n * console.log(tf.util.now());\n * ```\n *\n * @doc {heading: 'Util', namespace: 'util'}\n */\nexport function now() {\n    return env().platform.now();\n}\n/**\n * Returns a platform-specific implementation of\n * [`fetch`](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API).\n *\n * If `fetch` is defined on the global object (`window`, `process`, etc.),\n * `tf.util.fetch` returns that function.\n *\n * If not, `tf.util.fetch` returns a platform-specific solution.\n *\n * ```js\n * const resource = await tf.util.fetch('https://unpkg.com/@tensorflow/tfjs');\n * // handle response\n * ```\n *\n * @doc {heading: 'Util'}\n */\nexport function fetch(path, requestInits) {\n    return env().platform.fetch(path, requestInits);\n}\n/**\n * Encodes the provided string into bytes using the provided encoding scheme.\n *\n * @param s The string to encode.\n * @param encoding The encoding scheme. Defaults to utf-8.\n *\n * @doc {heading: 'Util'}\n */\nexport function encodeString(s, encoding = 'utf-8') {\n    encoding = encoding || 'utf-8';\n    return env().platform.encode(s, encoding);\n}\n/**\n * Decodes the provided bytes into a string using the provided encoding scheme.\n * @param bytes The bytes to decode.\n *\n * @param encoding The encoding scheme. Defaults to utf-8.\n *\n * @doc {heading: 'Util'}\n */\nexport function decodeString(bytes, encoding = 'utf-8') {\n    encoding = encoding || 'utf-8';\n    return env().platform.decode(bytes, encoding);\n}\n//# sourceMappingURL=util.js.map"],"names":["version","shuffle","array","counter","length","temp","index","Math","random","shuffleCombo","array2","Error","temp2","clamp","min","x","max","nearestLargerEven","val","sum","arr","i","randUniform","a","b","r","distSquared","result","diff","Number","assert","expr","msg","assertShapesMatch","shapeA","shapeB","errorMessagePrefix","arraysEqual","assertNonNull","flatten","skipTypedArray","Array","isArray","isTypedArray","push","sizeFromShape","shape","size","isScalarShape","n1","n2","isInt","tanh","Infinity","e2x","exp","sizeToSquarishShape","width","ceil","sqrt","createShuffledIndices","n","shuffledIndices","Uint32Array","rightPad","repeat","repeatedTry","checkFn","delayFn","maxCounter","Promise","resolve","reject","tryCount","tryFn","nextBackoff","setTimeout","inferFromImplicitShape","shapeProd","implicitIdx","newShape","slice","parseAxisParam","axis","rank","map","s","concat","every","ax","squeezeShape","keptDims","isEmptyArray","axes","sort","j","getTypedArrayFromDType","dtype","values","Float32Array","Int32Array","Uint8Array","getArrayFromDType","checkConversionForErrors","vals","num","isNaN","isFinite","isValidDtype","hasEncodingLoss","oldType","newType","bytesPerElement","bytesFromStringArray","bytes","forEach","isString","value","String","isBoolean","isNumber","inferDtype","isFunction","f","constructor","call","apply","nearestDivisor","start","computeStrides","strides","createNestedArray","offset","isComplex","ret","d","rest","len","reduce","acc","c","toNestedArray","makeOnesTypedArray","makeZerosTypedArray","makeZerosNestedTypedArray","prev","curr","assertNonNegativeIntegerDimensions","dimSize","isInteger","locToIndex","locs","indexToLoc","floor","isPromise","object","then","Rank","UpcastInt32AndMap","UpcastBoolAndMap","UpcastFloat32AndMap","UpcastComplex64AndMap","upcastTypeMap","upcastType","typeA","typeB","sumOutType","type","S","train","sgd","momentum","adadelta","adagrad","rmsprop","adamax","adam","deepMapInternal","input","mapFn","seen","Map","containedIn","Set","has","get","recurse","mappedIterable","add","k","childResult","delete","set","deepZip","inputs","zipFn","zipToList","deepZipInternal","obj","ArrayBuffer","isView","deepClone","container","cloneIfTensor","item","clone","RingBuffer","capacity","this","begin","end","RangeError","data","doubledCapacity","wrap","isFull","isEmpty","pushAll","pop","undefined","unshift","shift","shuffleExcise","relativeIndex","GrowingRingBuffer","super","INITIAL_CAPACITY","expand","newCapacity","newData","func","FunctionCallIterator","iteratorFromConcatenated","baseIterators","baseErrorHandler","ChainedIterator","LazyIterator","toArray","next","done","toArrayForTest","stream","prefetch","resolveFully","resolveWhile","predicate","shouldContinue","handleErrors","handler","ErrorHandlingLazyIterator","filter","FilterIterator","transform","MapIterator","mapAsync","AsyncMapIterator","serialMapAsync","serial","flatmap","FlatmapIterator","forEachAsync","serialForEach","rowMajorBatch","batchSize","smallLastBatch","RowMajorBatchIterator","columnMajorBatch","concatenate","iterator","ArrayIterator","take","count","TakeIterator","skip","SkipIterator","bufferSize","PrefetchIterator","windowSize","seed","ShuffleIterator","SerialIterator","items","trav","summary","nextFn","e","message","upstream","lastRead","serialNext","maxCount","skipped","enableSmallLastBatch","batch","inputTensors","mapped","outputTensors","t","dispose","OneToManyIterator","outputQueue","pump","mappedArray","iterators","moreIterators","readFromChain","iteratorResult","itemResult","ZipMismatchMode","buffer","refill","v","upstreamExhausted","seedrandom","alea","toString","randomInt","chooseIndex","chosenIndex","Dataset","base","async","deepBatchConcat","dataset","reshuffleEachIteration","seed2","int32","iteratorFn","arguments","rows","canTensorify","arrays","batchConcat","MAX_BUFFER_SIZE","Symbol","createScalarValue","encodeString","toTypedArray","getBool","noConversionNeeded","bool","round","now","platform","fetch","path","requestInits","encoding","encode","decodeString","decode"],"sourceRoot":""}