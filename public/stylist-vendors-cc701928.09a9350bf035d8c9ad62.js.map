{"version":3,"file":"stylist-vendors-cc701928.09a9350bf035d8c9ad62.js","mappings":"2LAUA,IAAIA,EAIG,SAASC,IAIZ,OAHgB,MAAZD,IACAA,GAAW,IAAAE,WAAUD,WAElBD,CACX,CAWO,SAASG,IACZ,MAAO,cACX,C,0jBC4BO,SAASC,EAAKC,EAAGC,GACpB,OAAOD,EAAEE,OAAOD,EACpB,CAOO,SAASE,EAAWH,EAAGI,GAAO,GACjC,MAAMC,EAAWL,EAAEM,MAAMC,QAKzB,OAJIH,EAAO,IACPA,EAAOC,EAASG,OAASJ,EAAO,GAEpCC,EAASI,OAAOL,EAAM,EAAG,GAClBJ,EAAEU,QAAQL,EACrB,CAYO,SAASM,EAAOX,EAAGY,GACtB,OAAO,IAAAC,OAAK,KACR,GAAuB,IAAnBb,EAAEM,MAAME,OACR,MAAM,IAAI,KACN,yDAAQR,EAAEM,MAAME,kBAGxB,OAAOM,EADGX,EAAWH,EAAG,GACT,CAAC,EAAGY,EAAG,GAAG,GAEjC,CAMO,SAASG,EAAQf,GACpB,MAAMgB,EAAW,CAAC,KAAqBhB,EAAEM,QACzC,OAAON,EAAEU,QAAQM,EACrB,CASO,SAASC,EAAajB,GACzB,GAAIA,EAAEkB,MAAQ,EACV,MAAM,IAAI,KAAW,wDAAwDlB,EAAEkB,SAEnF,MAAMF,EAAW,CAAChB,EAAEM,MAAM,GAAI,KAAqBN,EAAEM,MAAO,IAC5D,OAAON,EAAEU,QAAQM,EACrB,CASO,SAASG,EAAoBC,EAAOC,EAAOC,GAC9C,OAAO,IAAAT,OAAK,KACR,OAAQO,EAAMF,MACV,KAAK,EACD,OAAO,UAAYE,EAAOC,EAAOC,GACrC,KAAK,EACD,OAAO,UAAYF,EAAO,CAACC,EAAO,GAAI,CAACC,EAAMF,EAAMd,MAAM,KAC7D,KAAK,EACD,OAAO,UAAYc,EAAO,CAACC,EAAO,EAAG,GAAI,CAACC,EAAMF,EAAMd,MAAM,GAAIc,EAAMd,MAAM,KAChF,KAAK,EACD,OAAO,UAAYc,EAAO,CAACC,EAAO,EAAG,EAAG,GAAI,CAACC,EAAMF,EAAMd,MAAM,GAAIc,EAAMd,MAAM,GAAIc,EAAMd,MAAM,KACnG,KAAK,EACD,OAAO,QAAUc,EAAO,CAACC,EAAO,EAAG,EAAG,EAAG,GAAI,CACzCC,EAAMF,EAAMd,MAAM,GAAIc,EAAMd,MAAM,GAAIc,EAAMd,MAAM,GAAIc,EAAMd,MAAM,KAE1E,KAAK,EACD,OAAO,QAAUc,EAAO,CAACC,EAAO,EAAG,EAAG,EAAG,EAAG,GAAI,CAC5CC,EAAMF,EAAMd,MAAM,GAAIc,EAAMd,MAAM,GAAIc,EAAMd,MAAM,GAAIc,EAAMd,MAAM,GAClEc,EAAMd,MAAM,KAEpB,QACI,MAAM,IAAI,KACN,8DAAGc,EAAMF,QACrB,GAER,CASO,SAASK,EAAmBH,EAAOC,EAAOC,GAC7C,OAAO,IAAAT,OAAK,KACR,OAAQO,EAAMF,MACV,KAAK,EACD,OAAO,UAAYE,EAAOC,EAAOC,GACrC,KAAK,EACD,OAAO,UAAYF,EAAO,CAAC,EAAGC,GAAQ,CAACD,EAAMd,MAAM,GAAIgB,IAC3D,KAAK,EACD,OAAO,UAAYF,EAAO,CAAC,EAAG,EAAGC,GAAQ,CAACD,EAAMd,MAAM,GAAIc,EAAMd,MAAM,GAAIgB,IAC9E,KAAK,EACD,OAAO,UAAYF,EAAO,CAAC,EAAG,EAAG,EAAGC,GAAQ,CAACD,EAAMd,MAAM,GAAIc,EAAMd,MAAM,GAAIc,EAAMd,MAAM,GAAIgB,IACjG,QACI,MAAM,IAAI,KACN,6DAAGF,EAAMF,QACrB,GAER,CAUO,SAASM,EAAeJ,EAAOC,EAAOC,EAAMlB,GAC/C,OAAO,IAAAS,OAAK,KACR,OAAQO,EAAMF,MACV,KAAK,EACD,OAAO,UAAYE,EAAOC,EAAOC,GACrC,KAAK,EACD,OAAQlB,GACJ,KAAK,EACD,OAAOe,EAAoBC,EAAOC,EAAOC,GAC7C,KAAK,EACD,OAAOC,EAAmBH,EAAOC,EAAOC,GAC5C,QACI,MAAM,IAAI,KACN,iDAAGlB,KAEnB,KAAK,EACD,OAAQA,GACJ,KAAK,EACD,OAAOe,EAAoBC,EAAOC,EAAOC,GAC7C,KAAK,EACD,OAAO,UAAYF,EAAO,CAAC,EAAGC,EAAO,GAAI,CAACD,EAAMd,MAAM,GAAIgB,EAAMF,EAAMd,MAAM,KAChF,KAAK,EACD,OAAOiB,EAAmBH,EAAOC,EAAOC,GAC5C,QACI,MAAM,IAAI,KACN,iDAAGlB,KAEnB,KAAK,EACD,OAAQA,GACJ,KAAK,EACD,OAAOe,EAAoBC,EAAOC,EAAOC,GAC7C,KAAK,EACD,OAAO,UAAYF,EAAO,CAAC,EAAGC,EAAO,EAAG,GAAI,CAACD,EAAMd,MAAM,GAAIgB,EAAMF,EAAMd,MAAM,GAAIc,EAAMd,MAAM,KACnG,KAAK,EACD,OAAO,UAAYc,EAAO,CAAC,EAAG,EAAGC,EAAO,GAAI,CAACD,EAAMd,MAAM,GAAIc,EAAMd,MAAM,GAAIgB,EAAMF,EAAMd,MAAM,KACnG,KAAK,EACD,OAAOiB,EAAmBH,EAAOC,EAAOC,GAC5C,QACI,MAAM,IAAI,KACN,iDAAGlB,KAEnB,QACI,MAAM,IAAI,KACN,6DAAGgB,EAAMF,QACrB,GAER,CAOO,SAASO,EAAYC,EAAStB,GAAO,GACxC,IAAIc,EAgBJ,OAfId,EAAO,IACPc,EAAOQ,EAAQ,GAAGR,KAEdd,EADS,IAATc,EACOA,EAGA,GAGXd,IAASsB,EAAQ,GAAGR,OAGpBd,GAAQ,GAGL,SAAWsB,EAAStB,EAC/B,CAQO,SAASuB,EAAqBC,EAAGC,GACpC,OAAQD,EAAEV,MACN,KAAK,EACD,OAAO,WAAa,CAACU,EAAGC,IAC5B,KAAK,EACD,OAAO,WAAa,CAACD,EAAGC,GAAI,GAChC,KAAK,EACD,OAAO,WAAa,CAACD,EAAGC,GAAI,GAChC,KAAK,EACD,OAAO,WAAa,CAACD,EAAGC,GAAI,GAChC,QACI,MAAM,IAAI,KACN,+DAAgBD,EAAEV,QAElC,CAQO,SAASJ,EAAKd,EAAGY,GAIpB,GAHKkB,MAAMC,QAAQnB,KACfA,EAAI,CAACA,IAELZ,EAAEkB,OAASN,EAAEJ,OACb,MAAM,IAAI,KAAW,0BAA0BI,EAAEJ,+DACLR,EAAEkB,SAElD,OAAO,OAASlB,EAAGY,EACvB,CAYO,SAASoB,EAAa1B,EAAO2B,EAAO,EAAKC,EAAS,EAAKjC,EAAOkC,GACjE,OAAO,eAAiB7B,EAAO2B,EAAMC,EAAQjC,EAAOkC,EACxD,CAkBO,SAASC,EAAIR,EAAGC,EAAGQ,EAAYC,GAClC,GAAKV,EAAEV,KAAO,GAAOW,EAAEX,KAAO,EAC1B,MAAM,IAAI,KACN,8DAAsBU,EAAEtB,uBAAuBuB,EAAEvB,SAEzD,GAAIuB,EAAEX,MAAQ,EAAG,CAGb,GAFiBU,EAAEtB,MAAMC,OAAO,GAAG,KACZsB,EAAEvB,MAAMC,OAAO,GAAG,GAErC,MAAM,IAAI,KACN,gGAAwDqB,EAAEtB,wBAC5CuB,EAAEvB,QAE5B,CAEA,GAAgB,IAAXsB,EAAEV,MAA2B,IAAXW,EAAEX,KAAa,CAClC,MAAMqB,GAAa,EACbC,GAAa,EAInB,OAAO,eAAiB,CACpBZ,IACAC,EAAGA,EACHU,aACAC,aACAF,KAAMA,EAAOG,EAAYb,EAAEV,KAAMoB,GAAM,WAAqB,KAC5DD,cAER,CACK,CAED,MAAMK,EAAad,EAAEtB,MAAMC,QACrBoC,EAAWD,EAAWE,MAC5BhB,EAAIA,EAAElB,QAAQ,EAAE,EAAGiC,IAGnB,MAAME,EAAShB,EAAEvB,MAAMC,QACjBuC,EAAWD,EAAOD,MAClBG,EAAiBF,EAAOD,MACxBI,EAAa,IAAIH,EAAQC,GAGzBG,EAAOnB,MAAMoB,KAAK,CAAE1C,OAAQqB,EAAEX,OAAQ,CAACiC,EAAGC,IAClC,IAANA,EACOvB,EAAEX,KAAO,EAEXkC,GAAKvB,EAAEX,KAAO,EACZkC,EAAI,EAERA,IAEXvB,EAAIA,EAAEwB,UAAUJ,GAAMvC,QAAQ,CAACqC,GAAiB,IAEhD,MAAMO,EAAc,IAAIZ,KAAeM,GACjCT,GAAa,EACbC,GAAa,EACnB,OAAO,eACK,CACRZ,IACAC,IACAU,aACAC,aACAF,KAAMA,EAAOG,EAAYb,EAAEV,KAAMoB,GAAM,WAAqB,KAC5DD,eAEC3B,QAAQ4C,EACjB,CACJ,CA6CO,SAASC,EAAOC,EAAWC,EAASrD,GACvC,OAAO,IAAAS,OAAK,KAEJ4C,EADA3B,MAAMC,QAAQ0B,IACJ,IAAAC,UAASD,EAAS,SAGlBA,EAAQE,QAEf,SAAWH,EAAWC,EAASrD,KAE9C,CAMO,SAASwD,EAAO5D,GACnB,OAAO,MAAQA,EAAGA,EACtB,CA2BA,SAASyC,EAAYoB,EAAOvB,EAAMwB,GAC9B,MAAMC,EAAYzB,EAAKhC,MACvB,GAAkB,IAAdgC,EAAKpB,MAAcoB,EAAKpB,OAAS2C,EACjC,MAAM,IAAI,KAAW,+BAA+BvB,EAAKpB,gCACzB2C,KAEpC,GAAc,IAAVA,EAAa,CACb,GAAmB,kBAAfC,EACA,OAAyB,IAArBC,EAAUvD,OACH8B,EAAK5B,QAAQ,CAAC,EAAGqD,EAAU,GAAI,EAAG,EAAG,IAGrCzB,EAAK5B,QAAQ,CAAC,EAAGqD,EAAU,GAAIA,EAAU,GAAIA,EAAU,GAAIA,EAAU,KAG/E,GAAmB,iBAAfD,EACL,OAAyB,IAArBC,EAAUvD,OACH8B,EAAK5B,QAAQ,CAAC,EAAG,EAAG,EAAG,EAAGqD,EAAU,KAGpCzB,EAAK5B,QAAQ,CAAC,GAAGsD,OAAOD,GAG3C,MACK,GAAc,IAAVF,EAAa,CAClB,GAAmB,kBAAfC,EACA,OAAyB,IAArBC,EAAUvD,OACH8B,EAAK5B,QAAQ,CAAC,EAAGqD,EAAU,GAAI,EAAG,IAGlCzB,EAAK5B,QAAQ,CAAC,EAAGqD,EAAU,GAAIA,EAAU,GAAIA,EAAU,KAGjE,GAAmB,iBAAfD,EACL,OAAyB,IAArBC,EAAUvD,OACH8B,EAAK5B,QAAQ,CAAC,EAAG,EAAG,EAAGqD,EAAU,KAGjCzB,EAAK5B,QAAQ,CAAC,GAAGsD,OAAOD,GAG3C,MACK,GAAc,IAAVF,EAAa,CAClB,GAAmB,kBAAfC,EACA,OAAyB,IAArBC,EAAUvD,OACH8B,EAAK5B,QAAQ,CAAC,EAAGqD,EAAU,GAAI,IAG/BzB,EAAK5B,QAAQ,CAAC,EAAGqD,EAAU,GAAIA,EAAU,KAGnD,GAAmB,iBAAfD,EACL,OAAyB,IAArBC,EAAUvD,OACH8B,EAAK5B,QAAQ,CAAC,EAAG,EAAGqD,EAAU,KAG9BzB,EAAK5B,QAAQ,CAAC,GAAGsD,OAAOD,GAG3C,MACK,GAAIF,EAAQ,EACb,OAAOvB,EAEX,MAAM,IAAI,KAAW,sCAAsCA,EAAKpB,OACpE,CAUO,SAAS+C,EAAQjE,EAAGsC,EAAMwB,GAC7B,OAAO,IAAAjD,OAAK,KACU,MAAdiD,IACAA,GAAa,YAEjB,QAAgBA,GACT9D,EAAEkE,IAAIzB,EAAYzC,EAAEkB,KAAMoB,EAAMwB,MAE/C,CAOO,SAASK,EAAInE,EAAGoE,EAAQ,GAE3B,GAAc,IAAVA,EACA,MAAM,IAAI,KAAoB,0CAA0CA,8BAG5E,OAAO,MAAQpE,EACnB,CASO,SAASqE,EAASrE,GACrB,OAAO,IAAAa,OAAK,IAAM,MAAQb,EAAG,MAAQA,GAAGkE,IAAI,KAChD,CAWO,SAASI,EAAQtE,EAAGuE,EAAOC,EAAYrC,GAC1C,OAAO,IAAAtB,OAAK,IAAM,UAAYb,EAAGuE,EAAOC,EAAYrC,IACxD,CAUO,SAASsC,EAAYzE,GACxB,OAAO,IAAAa,OAAK,KACR,MAAM6D,EAAI,MAAQ,GAAI,MAAQ,GAAI1E,IAClC,OAAO,cAAgB0E,EAAG,EAAG,EAAE,GAEvC,CAcO,SAASC,EAAa3E,EAAG4E,EAAKC,GAAW,GAC5C,OAAOA,EAAW7E,IAAM4E,GAC5B,C,6IC5mBO,MAAME,UAAmB,EAAAC,cAAA,aAC5B,SAAAC,GACI,MAAO,CAAC,CACZ,EAMG,MAAMC,UAAYH,EAQrB,KAAAI,CAAMlF,EAAGoE,EAAQ,GACb,OAAO,KAAMpE,EAAGoE,EACpB,EAGJa,EAAIE,UAAY,MAChB,EAAAJ,cAAA,cAA4BE,GAQrB,MAAMG,UAAaN,EACtB,KAAAI,CAAMlF,GACF,OAAO,OAASA,EACpB,EAGJoF,EAAKD,UAAY,OACjB,EAAAJ,cAAA,cAA4BK,GAIrB,MAAMC,UAAaP,EACtB,KAAAI,CAAMlF,GACF,OAAO,OAASA,EACpB,EAGJqF,EAAKF,UAAY,OACjB,EAAAJ,cAAA,cAA4BM,GAIrB,MAAMC,UAAcR,EACvB,KAAAI,CAAMlF,GACF,OAAO,IAAAa,OAAK,IAAM,UAAY,EAAK,OAASb,KAChD,EAGJsF,EAAMH,UAAY,QAClB,EAAAJ,cAAA,cAA4BO,GAErB,MAAMC,UAAeT,EACxB,KAAAI,CAAMlF,GACF,OAAOA,CACX,EAGJuF,EAAOJ,UAAY,SACnB,EAAAJ,cAAA,cAA4BQ,GAIrB,MAAMC,UAAgBV,EACzB,KAAAI,CAAMlF,GACF,OAAO,UAAYA,EACvB,EAGJwF,EAAQL,UAAY,UACpB,EAAAJ,cAAA,cAA4BS,GAIrB,MAAMC,UAAoBX,EAC7B,KAAAI,CAAMlF,GACF,OAAO,KAAcA,EACzB,EAGJyF,EAAYN,UAAY,cACxB,EAAAJ,cAAA,cAA4BU,GAIrB,MAAMC,UAAiBZ,EAC1B,KAAAI,CAAMlF,GACF,OAAO,WAAaA,EACxB,EAGJ0F,EAASP,UAAY,WACrB,EAAAJ,cAAA,cAA4BW,GAIrB,MAAMC,UAAiBb,EAC1B,KAAAI,CAAMlF,GACF,OAAO,KAAWA,EACtB,EAGJ2F,EAASR,UAAY,WACrB,EAAAJ,cAAA,cAA4BY,GAIrB,MAAMC,UAAad,EACtB,KAAAI,CAAMlF,GACF,OAAO,OAASA,EACpB,EAGJ4F,EAAKT,UAAY,OACjB,EAAAJ,cAAA,cAA4Ba,GAIrB,MAAMC,UAAgBf,EAazB,KAAAI,CAAMlF,EAAGI,GAAO,GACZ,OAAO,UAAYJ,EAAGI,EAC1B,EAGJyF,EAAQV,UAAY,UACpB,EAAAJ,cAAA,cAA4Bc,GAIrB,MAAMC,UAAmBhB,EAc5B,KAAAI,CAAMlF,EAAGI,GAAO,GACZ,OAAO,aAAeJ,EAAGI,EAC7B,EAGJ0F,EAAWX,UAAY,aACvB,EAAAJ,cAAA,cAA4Be,GAIrB,MAAMC,UAAcjB,EAQvB,KAAAI,CAAMlF,EAAGoE,EAAQ,GACb,OAAO,IAAAvD,OAAK,IAAM,UAAYb,EAAEgG,IAAI5B,IAAQ4B,IAAIhG,IACpD,EAGJ+F,EAAMZ,UAAY,QAClB,EAAAJ,cAAA,cAA4BgB,GAIrB,MAAME,UAAanB,EAOtB,KAAAI,CAAMlF,GACF,OAAO,IAAAa,OAAK,IAAM,MAAQb,EAAG,OAAS,WAAaA,MACvD,EAKG,SAASkG,EAAoB7D,GAChC,OAAOA,EAAW8D,cACtB,CACO,SAASC,EAAsBC,EAAQC,EAAgB,CAAC,GAC3D,OAAO,QAAuBD,EAAQ,EAAAtB,cAAA,iBAA+BwB,SAASC,aAAcF,EAAe,aAC/G,CACO,SAASG,EAAcC,GAC1B,GAAkB,MAAdA,EAAoB,CACpB,MAAML,EAAS,CACfA,UAAsB,SACtBA,OAAmB,CAAC,GACpB,OAAOD,EAAsBC,EACjC,CACA,GAA0B,kBAAfK,EAAyB,CAChC,MAAML,EAAS,CAAC,EAGhB,OAFAA,EAAkB,UAAIK,EACtBL,EAAe,OAAI,CAAC,EACbD,EAAsBC,EACjC,CACK,OAAIK,aAAsB5B,EACpB4B,EAGAN,EAAsBM,EAErC,CA3BAT,EAAKd,UAAY,OACjB,EAAAJ,cAAA,cAA4BkB,E,+EClN5B,IAAIU,EAAsB,EACnB,SAASC,IACZ,OAAOD,GACX,CACA,MAAME,EAAe,CAAC,EAMf,SAASC,EAAOC,EAAS,IAK5B,OAJMA,KAAUF,IACZA,EAAaE,GAAU,GAE3BF,EAAaE,IAAW,EACjBA,EAASF,EAAaE,GAAQC,UACzC,C","sources":["webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/backend/common.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/backend/tfjs_backend.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/activations.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/backend/state.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport { backend } from '@tensorflow/tfjs-core';\nlet _epsilon;\n/**\n * Returns the value of the fuzz factor used in numeric expressions.\n */\nexport function epsilon() {\n    if (_epsilon == null) {\n        _epsilon = backend().epsilon();\n    }\n    return _epsilon;\n}\n/**\n * Sets the value of the fuzz factor used in numeric expressions.\n * @param e New value of epsilon.\n */\nexport function setEpsilon(e) {\n    _epsilon = e;\n}\n/**\n * Returns the default image data format convention.\n */\nexport function imageDataFormat() {\n    return 'channelsLast';\n}\n//# sourceMappingURL=common.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * deeplearn.js backend.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { onesLike as coreOnesLike, scalar, tensor1d, tidy, where, zerosLike as coreZerosLike } from '@tensorflow/tfjs-core';\nimport { checkDataFormat } from '../common';\nimport { NotImplementedError, ValueError } from '../errors';\nimport * as math_utils from '../utils/math_utils';\nimport { imageDataFormat } from './common';\n// tslint:enable\n/* Setting and getting backend from deeplearn.js. */\n// Default deeplearn.js backend is WebGL (GPU).\nlet backend = 'webgl';\nexport function setBackend(requestedBackend) {\n    tfc.setBackend(requestedBackend);\n    backend = requestedBackend;\n}\nexport function getBackend() {\n    return backend;\n}\n/**\n * Indicates whether the backend is operating symbolically.\n *\n * This function will be used to determine how to interpret user code. If\n * it returns true, calls to the backend construct a symbolic graph; if\n * it returns false, calls to the backend execute immediately.\n */\nexport function isBackendSymbolic() {\n    return false;\n}\n/**\n * Get the number of elements in a Tensor.\n * @param x The Tensor.\n * @return Number of elements in `x`.\n */\nexport function countParams(x) {\n    const shape = x.shape;\n    if (shape.length > 0) {\n        return shape.reduce((a, b) => a * b);\n    }\n    else {\n        // Scalar.\n        return 1;\n    }\n}\n/**\n * Casts a tensor to a different dtype and returns it.\n * @param x Input tensor.\n * @param dtype String: 'float32'|'int32'|'bool'.\n * @returns Tensor of the specified `dtype`.\n */\nexport function cast(x, dtype) {\n    return x.asType(dtype);\n}\n/**\n * Adds a 1-sized dimension at index \"axis\".\n * @param x Input tensor.\n * @param axis Position where to add the new axis.\n * @returns Result of the dimension expansion.\n */\nexport function expandDims(x, axis = -1) {\n    const outShape = x.shape.slice();\n    if (axis < 0) {\n        axis = outShape.length + axis + 1;\n    }\n    outShape.splice(axis, 0, 1);\n    return x.reshape(outShape);\n}\n/**\n * Repeats a 2D tensor.\n *\n * If `x` has shape `[samples, dim]` and `n` is 2, for example, the output\n * will have shape `[samples, 2, dim]`.\n *\n * @param x Input tensor.\n * @param n Integer, number of times to repeat.\n * @returns The result of the repeat operation.\n * @throws ValueError: If input tensor is not 2D.\n */\nexport function repeat(x, n) {\n    return tidy(() => {\n        if (x.shape.length !== 2) {\n            throw new ValueError(`repeat() expects a rank-2 tensor, but received a ` +\n                `rank-${x.shape.length} tensor.`);\n        }\n        const y = expandDims(x, 1);\n        return tile(y, [1, n, 1]);\n    });\n}\n/**\n * Flatten a Tensor into 1D.\n * @param x Input tensor.\n * @return The result of the flattening `x`.\n */\nexport function flatten(x) {\n    const newShape = [math_utils.arrayProd(x.shape)];\n    return x.reshape(newShape);\n}\n/**\n * Turn a nD tensor into a 2D tensor with same 0th dimension.\n * In other words, it flattens each data samples of a batch.\n *\n * @param x The tensor to flatten. The rank of this tensor is required to be 2\n *   or higher.\n * @return The result of the flattening.\n */\nexport function batchFlatten(x) {\n    if (x.rank <= 1) {\n        throw new ValueError(`batchFlatten requires a minimum rank of 2. Got rank: ${x.rank}.`);\n    }\n    const newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n    return x.reshape(newShape);\n}\n/**\n * Do slicing along the first axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size size of the slice along the first axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongFirstAxis(array, start, size) {\n    return tidy(() => {\n        switch (array.rank) {\n            case 1:\n                return tfc.slice1d(array, start, size);\n            case 2:\n                return tfc.slice2d(array, [start, 0], [size, array.shape[1]]);\n            case 3:\n                return tfc.slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);\n            case 4:\n                return tfc.slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);\n            case 5:\n                return tfc.slice(array, [start, 0, 0, 0, 0], [\n                    size, array.shape[1], array.shape[2], array.shape[3], array.shape[4]\n                ]);\n            case 6:\n                return tfc.slice(array, [start, 0, 0, 0, 0, 0], [\n                    size, array.shape[1], array.shape[2], array.shape[3], array.shape[4],\n                    array.shape[5]\n                ]);\n            default:\n                throw new ValueError(`sliceAlongFirstAxis() received an unsupported tensor rank: ` +\n                    `${array.rank}`);\n        }\n    });\n}\n/**\n * Do slicing along the last axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size size of the slice along the last axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongLastAxis(array, start, size) {\n    return tidy(() => {\n        switch (array.rank) {\n            case 1:\n                return tfc.slice1d(array, start, size);\n            case 2:\n                return tfc.slice2d(array, [0, start], [array.shape[0], size]);\n            case 3:\n                return tfc.slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);\n            case 4:\n                return tfc.slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);\n            default:\n                throw new ValueError(`sliceAlongLastAxis() received an unsupported tensor rank: ` +\n                    `${array.rank}`);\n        }\n    });\n}\n/**\n * Do slicing along the sepcified axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size of the slice along the chosen axis.\n * @param choose an axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongAxis(array, start, size, axis) {\n    return tidy(() => {\n        switch (array.rank) {\n            case 1:\n                return tfc.slice1d(array, start, size);\n            case 2:\n                switch (axis) {\n                    case 1:\n                        return sliceAlongFirstAxis(array, start, size);\n                    case 2:\n                        return sliceAlongLastAxis(array, start, size);\n                    default:\n                        throw new ValueError(`The axis is not within the rank of the tensor ` +\n                            `${axis}`);\n                }\n            case 3:\n                switch (axis) {\n                    case 1:\n                        return sliceAlongFirstAxis(array, start, size);\n                    case 2:\n                        return tfc.slice3d(array, [0, start, 0], [array.shape[0], size, array.shape[2]]);\n                    case 3:\n                        return sliceAlongLastAxis(array, start, size);\n                    default:\n                        throw new ValueError(`The axis is not within the rank of the tensor ` +\n                            `${axis}`);\n                }\n            case 4:\n                switch (axis) {\n                    case 1:\n                        return sliceAlongFirstAxis(array, start, size);\n                    case 2:\n                        return tfc.slice4d(array, [0, start, 0, 0], [array.shape[0], size, array.shape[2], array.shape[3]]);\n                    case 3:\n                        return tfc.slice4d(array, [0, 0, start, 0], [array.shape[0], array.shape[1], size, array.shape[3]]);\n                    case 4:\n                        return sliceAlongLastAxis(array, start, size);\n                    default:\n                        throw new ValueError(`The axis is not within the rank of the tensor ` +\n                            `${axis}`);\n                }\n            default:\n                throw new ValueError(`sliceAlongLastAxis() received an unsupported tensor rank: ` +\n                    `${array.rank}`);\n        }\n    });\n}\n/**\n * Concatenates a list of tensors alongside the specified axis.\n * @param tensors `Array` of tensors to concatenate.\n * @param axis Concatenation axis.\n * @returns The result of the concatenation.\n */\nexport function concatenate(tensors, axis = -1) {\n    let rank;\n    if (axis < 0) {\n        rank = tensors[0].rank;\n        if (rank !== 0) {\n            axis = rank;\n        }\n        else {\n            axis = 0;\n        }\n    }\n    if (axis === tensors[0].rank) {\n        // Porting Note: This is necessary because tfc.concat() requires axis to be\n        //   in the interval [-rank, rank).\n        axis = -1;\n    }\n    // Porting Note: Sparse concat is not supported yet.\n    return tfc.concat(tensors, axis);\n}\n/**\n * Concatenate two arrays along the first dimension.\n * @param a The 1st `tf.Tensor` to concatenate.\n * @param b The 2nd `tf.Tensor` to concatenate.\n * @returns Result of the concatenation.\n * @throws ValueError: If `a` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function concatAlongFirstAxis(a, b) {\n    switch (a.rank) {\n        case 1:\n            return tfc.concat1d([a, b]);\n        case 2:\n            return tfc.concat2d([a, b], 0);\n        case 3:\n            return tfc.concat3d([a, b], 0);\n        case 4:\n            return tfc.concat4d([a, b], 0);\n        default:\n            throw new ValueError(`concatAlongFirstAxis() received an unsupported ` +\n                `tensor rank: ${a.rank}`);\n    }\n}\n/**\n * Creates a tensor by tiling `x` by `n`.\n * @param x A tensor.\n * @param n An Array of integers or a single integer. If an Array, the length\n *   must be the same as the number of dimensions in `x`. If a single integer,\n *   it will be treated as an Array of length 1.\n */\nexport function tile(x, n) {\n    if (!Array.isArray(n)) {\n        n = [n];\n    }\n    if (x.rank !== n.length) {\n        throw new ValueError(`The length of input n (${n.length}) does not match ` +\n            `the number of dimensions in input x (${x.rank})`);\n    }\n    return tfc.tile(x, n);\n}\n/* Creation of random tensors. */\n/**\n * Get a tensor with normal distribution of values.\n *\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @return The normal tensor.\n */\nexport function randomNormal(shape, mean = 0.0, stddev = 1.0, dtype, seed) {\n    return tfc.randomNormal(shape, mean, stddev, dtype, seed);\n}\n/* Linear Algebra */\n/**\n * Multiply two tensors and returns the result as a tensor.\n *\n * For 2D tensors, this is equivalent to matrix multiplication (matMul).\n * For tensors of higher ranks, it follows the Theano behavior,\n * (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`).  From the Theano documentation:\n *\n * For N dimensions it is a sum product over the last axis of x and the\n * second-to-last of y:\n *\n * @param a A tensor of at least rank 2.\n * @param b A tensor of at least rank 2.\n * @param activation (optional) A string identifying the activation\n *   function.\n * @return Result of the dot operation.\n */\nexport function dot(a, b, activation, bias) {\n    if ((a.rank < 2) || (b.rank < 2)) {\n        throw new NotImplementedError(`dot requires both inputs to be rank >= 2` +\n            ` but got x shape = ${a.shape} and y shape = ${b.shape}`);\n    }\n    if (b.rank >= 3) {\n        const xLastDim = a.shape.slice(-1)[0];\n        const ySecondLastDim = b.shape.slice(-2)[0];\n        if (xLastDim !== ySecondLastDim) {\n            throw new NotImplementedError(`If rank y >= 3, then the second last dim` +\n                ` of y must equal the last dim of x but got x shape = ${a.shape} and ` +\n                ` y shape = ${b.shape}`);\n        }\n    }\n    // Handle basic 2D x 2D case.\n    if ((a.rank === 2) && (b.rank === 2)) {\n        const transposeA = false;\n        const transposeB = false;\n        // tfc.fused.matMul only fuses certain activation functions. Unsupported\n        // activation functions are treated as 'linear' activations, which is\n        // equivalent to a no-op.\n        return tfc.fused.matMul({\n            a,\n            b: b,\n            transposeA,\n            transposeB,\n            bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n            activation\n        });\n    }\n    else {\n        // Reshape x into the analogous 2D Tensor.\n        const aFirstDims = a.shape.slice(); // Holds all but the last dim of x.\n        const aLastDim = aFirstDims.pop();\n        a = a.reshape([-1, aLastDim]);\n        // Reshape y into the analogous 2D Tensor, and keep track of the\n        // required dimensions to reproduce the output shape.\n        const bShape = b.shape.slice();\n        const bLastDim = bShape.pop();\n        const ySecondLastDim = bShape.pop();\n        const yOtherDims = [...bShape, bLastDim];\n        // permutation should be like [r-2, 0, 1, 2, ... r-4, r-3, r-1]\n        // where r is the rank of y.\n        const perm = Array.from({ length: b.rank }, (_, i) => {\n            if (i === 0) {\n                return b.rank - 2;\n            }\n            else if (i <= b.rank - 2) {\n                return i - 1;\n            }\n            return i;\n        });\n        b = b.transpose(perm).reshape([ySecondLastDim, -1]);\n        // Multiply x and y as 2D Tensors, and then reshape back to original.\n        const outputShape = [...aFirstDims, ...yOtherDims];\n        const transposeA = false;\n        const transposeB = false;\n        return tfc.fused\n            .matMul({\n            a,\n            b,\n            transposeA,\n            transposeB,\n            bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n            activation\n        })\n            .reshape(outputShape);\n    }\n}\n/**\n * Compute the sign Tensor of an input Tensor.\n *\n * Elements of the input `tf.Tensor` that are === 0 are mapped to 0.\n * Elements of the input `tf.Tensor` that are > 0 are mapped to 1.\n * Elements of the input `tf.Tensor` that are < 0 are mapped to -1.\n *\n * @param x Input `tf.Tensor`.\n * @return The sign `tf.Tensor`.\n */\nexport function sign(x) {\n    // TODO(cais): Move to the core.\n    return tidy(() => {\n        const zerosLikeX = coreZerosLike(x);\n        const onesLikeX = coreOnesLike(x);\n        return where(tfc.equal(x, zerosLikeX), zerosLikeX, where(tfc.greater(x, coreZerosLike(x)), onesLikeX, tfc.mul(-1, onesLikeX)));\n    });\n}\n/**\n * Computes the one-hot representation of an integer tensor.\n * @param indices nD integer tensor of shape\n *   `(batch_size, dim1, dim2, ... dim(n-1))`\n * @param numClasses Integer, number of classes to consider.\n * @returns (n + 1)D one hot representation of the input\n *   with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n */\nexport function oneHot(indices, numClasses) {\n    return tidy(() => {\n        if (indices.rank !== 1) {\n            throw new Error('Only 1D one-hot tensors are supported in the ' +\n                'deeplearn backend, at present.');\n        }\n        indices = indices.toInt();\n        return tfc.oneHot(indices, numClasses).toFloat();\n    });\n}\n/* Elementary math functions. */\n/**\n * Retrieves the elements of indices `indices` in the tensor `reference`.\n * @param reference A tensor.\n * @param indices An integer tensor of indices or an `Array` of integers.\n * @param axis Axis along which to perform the gather operation.\n * @returns The result of the gathering as a tensor.\n */\nexport function gather(reference, indices, axis) {\n    return tidy(() => {\n        if (Array.isArray(indices)) {\n            indices = tensor1d(indices, 'int32');\n        }\n        else {\n            indices = indices.toInt();\n        }\n        return tfc.gather(reference, indices, axis);\n    });\n}\n/**\n * Element-wise square.\n * @param x Input tensor.\n * @return element-wise x^2\n */\nexport function square(x) {\n    return tfc.mul(x, x);\n}\n/**\n * Element-wise exponentiation.\n *\n * Porting Note: In PyKeras, `a` (the exponent) is a Python integer, which\n *   takes advatnage of the backend's (e.g., TensorFlow's) automatic\n * conversion to tensor. Here we allow `a` to be either a number or a tensor.\n *\n * @param x The base tensor.\n * @param a The exponent, tensor or number. If a number, it is rounded to the\n *   nearest integer and converted to a tensor.\n * @returns A tensor of the same shape as `x`.\n */\nexport function pow(x, a) {\n    return tidy(() => {\n        if (typeof (a) === 'number') {\n            a = scalar(Math.round(a), 'int32');\n        }\n        if (a.dtype !== 'int32') {\n            throw new NotImplementedError(`Non-int32 dtype (${a.dtype}) is not supported by pow() yet`);\n        }\n        return tfc.pow(x, a);\n    });\n}\n/**\n * Reshapes bias tensor according to rank of x.\n */\nfunction reshapeBias(xRank, bias, dataFormat) {\n    const biasShape = bias.shape;\n    if (bias.rank !== 1 && bias.rank !== xRank) {\n        throw new ValueError(`Unexpected bias dimensions: ${bias.rank}` +\n            `; expected it to be 1 or ${xRank}`);\n    }\n    if (xRank === 5) {\n        if (dataFormat === 'channelsFirst') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, biasShape[0], 1, 1, 1]);\n            }\n            else {\n                return bias.reshape([1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]);\n            }\n        }\n        else if (dataFormat === 'channelsLast') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, 1, 1, 1, biasShape[0]]);\n            }\n            else {\n                return bias.reshape([1].concat(biasShape));\n            }\n        }\n    }\n    else if (xRank === 4) {\n        if (dataFormat === 'channelsFirst') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, biasShape[0], 1, 1]);\n            }\n            else {\n                return bias.reshape([1, biasShape[2], biasShape[0], biasShape[1]]);\n            }\n        }\n        else if (dataFormat === 'channelsLast') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, 1, 1, biasShape[0]]);\n            }\n            else {\n                return bias.reshape([1].concat(biasShape));\n            }\n        }\n    }\n    else if (xRank === 3) {\n        if (dataFormat === 'channelsFirst') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, biasShape[0], 1]);\n            }\n            else {\n                return bias.reshape([1, biasShape[1], biasShape[0]]);\n            }\n        }\n        else if (dataFormat === 'channelsLast') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, 1, biasShape[0]]);\n            }\n            else {\n                return bias.reshape([1].concat(biasShape));\n            }\n        }\n    }\n    else if (xRank < 3) {\n        return bias;\n    }\n    throw new ValueError(`Unsupported input rank by biasAdd: ${bias.rank}`);\n}\n/* Neural-network operations. */\n/**\n * Add a bias to a tensor.\n *\n * @param x The tensor to add the bias to.\n * @param bias The bias to add to `x`. Must be 1D or the same rank as `x`.\n * @return Result of the bias adding.\n * @throws ValueError: If the rank of `bias` is incorrect.\n */\nexport function biasAdd(x, bias, dataFormat) {\n    return tidy(() => {\n        if (dataFormat == null) {\n            dataFormat = imageDataFormat();\n        }\n        checkDataFormat(dataFormat);\n        return x.add(reshapeBias(x.rank, bias, dataFormat));\n    });\n}\n/**\n * Exponential linear unit (ELU).\n * @param x A tensor or variable to compute the activation function for.\n * @param alpha: A scalar, a scaling factor for the negative section.\n * @return Output of the ELU operation.\n */\nexport function elu(x, alpha = 1) {\n    // TODO(cais): Add support for alpha values other than 1.\n    if (alpha !== 1) {\n        throw new NotImplementedError(`Support for alpha values other than 1 (${alpha}) is not implemented ` +\n            `yet.`);\n    }\n    return tfc.elu(x);\n}\n/**\n * Softsign of a tensor.\n *\n * Defined as x / (abs(x) + 1), element-wise.\n *\n * @param x: Input.\n * @returns Output.\n */\nexport function softsign(x) {\n    return tidy(() => tfc.div(x, tfc.abs(x).add(1)));\n}\n/**\n * Sets entries in `x` to zero at random, while scaling the entire tensor.\n *\n * @param x input tensor.\n * @param level fraction of the entries in the tensor that will be set to 0.\n * @param noiseShape shape of randomly generated keep/drop flags, must be\n *   broadcastable to the shape of `x`. Optional.\n * @param seed random seed to ensure determinism. Optional.\n * @returns Result of the dropout operation.\n */\nexport function dropout(x, level, noiseShape, seed) {\n    return tidy(() => tfc.dropout(x, level, noiseShape, seed));\n}\n/**\n * Element-wise, segment-wise linear approximation of sigmoid.\n *\n * Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n * In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n *\n * @param x Input tensor.\n * @returns Output tensor.\n */\nexport function hardSigmoid(x) {\n    return tidy(() => {\n        const y = tfc.add(.5, tfc.mul(.2, x));\n        return tfc.clipByValue(y, 0, 1);\n    });\n}\n/**\n * Invoke `x` in the training phase, and `alt` otherwise.\n *\n * Porting Note: We do not create placeholder tensors for the `training`\n * boolean flag here, because there is no such thing in the TF.js imperative\n * backend.\n *\n * @param x The function to invoke iff `training` is `true`.\n * @param alt The function to invoke iff `training` is `false`.\n * @param training Boolean flag for whether training phase is active.\n * @returns The return value of `x()` if `training` is `true`, or the return\n *   value of `alt()` if `training` is `false`.\n */\nexport function inTrainPhase(x, alt, training = false) {\n    return training ? x() : alt();\n}\n//# sourceMappingURL=tfjs_backend.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport { deserializeKerasObject } from './utils/generic_utils';\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\nexport class Activation extends serialization.Serializable {\n    getConfig() {\n        return {};\n    }\n}\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\nexport class Elu extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x: Input.\n     * @param alpha: Scaling factor the negative section.\n     * @return Output of the ELU activation.\n     */\n    apply(x, alpha = 1) {\n        return K.elu(x, alpha);\n    }\n}\n/** @nocollapse */\nElu.className = 'elu';\nserialization.registerClass(Elu);\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\nexport class Selu extends Activation {\n    apply(x) {\n        return tfc.selu(x);\n    }\n}\n/** @nocollapse */\nSelu.className = 'selu';\nserialization.registerClass(Selu);\n/**\n *  Rectified linear unit\n */\nexport class Relu extends Activation {\n    apply(x) {\n        return tfc.relu(x);\n    }\n}\n/** @nocollapse */\nRelu.className = 'relu';\nserialization.registerClass(Relu);\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\nexport class Relu6 extends Activation {\n    apply(x) {\n        return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n    }\n}\n/** @nocollapse */\nRelu6.className = 'relu6';\nserialization.registerClass(Relu6);\n//* Linear activation (no-op) */\nexport class Linear extends Activation {\n    apply(x) {\n        return x;\n    }\n}\n/** @nocollapse */\nLinear.className = 'linear';\nserialization.registerClass(Linear);\n/**\n * Sigmoid activation function.\n */\nexport class Sigmoid extends Activation {\n    apply(x) {\n        return tfc.sigmoid(x);\n    }\n}\n/** @nocollapse */\nSigmoid.className = 'sigmoid';\nserialization.registerClass(Sigmoid);\n/**\n * Segment-wise linear approximation of sigmoid.\n */\nexport class HardSigmoid extends Activation {\n    apply(x) {\n        return K.hardSigmoid(x);\n    }\n}\n/** @nocollapse */\nHardSigmoid.className = 'hardSigmoid';\nserialization.registerClass(HardSigmoid);\n/**\n * Softplus activation function.\n */\nexport class Softplus extends Activation {\n    apply(x) {\n        return tfc.softplus(x);\n    }\n}\n/** @nocollapse */\nSoftplus.className = 'softplus';\nserialization.registerClass(Softplus);\n/**\n * Softsign activation function.\n */\nexport class Softsign extends Activation {\n    apply(x) {\n        return K.softsign(x);\n    }\n}\n/** @nocollapse */\nSoftsign.className = 'softsign';\nserialization.registerClass(Softsign);\n/**\n * Hyperbolic tangent function.\n */\nexport class Tanh extends Activation {\n    apply(x) {\n        return tfc.tanh(x);\n    }\n}\n/** @nocollapse */\nTanh.className = 'tanh';\nserialization.registerClass(Tanh);\n/**\n * Softmax activation function\n */\nexport class Softmax extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @param axis Integer, axis along which the softmax normalization is applied.\n     * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n     * an error.\n     *\n     * @returns a Tensor of the same shape as x\n     *\n     * @throws ValueError: In case `dim(x) < 2`.\n     */\n    apply(x, axis = (-1)) {\n        return tfc.softmax(x, axis);\n    }\n}\n/** @nocollapse */\nSoftmax.className = 'softmax';\nserialization.registerClass(Softmax);\n/**\n * Log softmax activation function\n */\nexport class LogSoftmax extends Activation {\n    /**\n     * Calculate the activation function of log softmax:\n     * log( exp(x_i) / sum(exp(x)) )\n     *\n     * @param x Tensor.\n     * @param axis Integer, axis along which the softmax normalization is applied.\n     * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n     * an error.\n     *\n     * @returns a Tensor of the same shape as x\n     *\n     * @throws ValueError: In case `dim(x) < 2`.\n     */\n    apply(x, axis = (-1)) {\n        return tfc.logSoftmax(x, axis);\n    }\n}\n/** @nocollapse */\nLogSoftmax.className = 'logSoftmax';\nserialization.registerClass(LogSoftmax);\n/**\n * Swish activation function\n */\nexport class Swish extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @param alpha Scaling factor for the sigmoid function.\n     * @returns a Tensor of the same shape as x\n     */\n    apply(x, alpha = 1) {\n        return tidy(() => tfc.sigmoid(x.mul(alpha)).mul(x));\n    }\n}\n/** @nocollapse */\nSwish.className = 'swish';\nserialization.registerClass(Swish);\n/**\n * Mish activation function\n */\nexport class Mish extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @returns a Tensor of the same shape as x\n     */\n    apply(x) {\n        return tidy(() => tfc.mul(x, tfc.tanh(tfc.softplus(x))));\n    }\n}\n/** @nocollapse */\nMish.className = 'mish';\nserialization.registerClass(Mish);\nexport function serializeActivation(activation) {\n    return activation.getClassName();\n}\nexport function deserializeActivation(config, customObjects = {}) {\n    return deserializeKerasObject(config, serialization.SerializationMap.getMap().classNameMap, customObjects, 'activation');\n}\nexport function getActivation(identifier) {\n    if (identifier == null) {\n        const config = {};\n        config['className'] = 'linear';\n        config['config'] = {};\n        return deserializeActivation(config);\n    }\n    if (typeof identifier === 'string') {\n        const config = {};\n        config['className'] = identifier;\n        config['config'] = {};\n        return deserializeActivation(config);\n    }\n    else if (identifier instanceof Activation) {\n        return identifier;\n    }\n    else {\n        return deserializeActivation(identifier);\n    }\n}\n//# sourceMappingURL=activations.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Utilities related to persistent state in the backend.\n */\n/**\n * An ID to track `tf.SymbolicTensor`s and derived classes.\n * Required in different places in engine/topology.ts to identify unique\n * tensors.\n */\nlet _nextUniqueTensorId = 0;\nexport function getNextUniqueTensorId() {\n    return _nextUniqueTensorId++;\n}\nconst _uidPrefixes = {};\n/**\n * Provides a unique UID given a string prefix.\n *\n * @param prefix\n */\nexport function getUid(prefix = '') {\n    if (!(prefix in _uidPrefixes)) {\n        _uidPrefixes[prefix] = 0;\n    }\n    _uidPrefixes[prefix] += 1;\n    return prefix + _uidPrefixes[prefix].toString();\n}\n//# sourceMappingURL=state.js.map"],"names":["_epsilon","epsilon","backend","imageDataFormat","cast","x","dtype","asType","expandDims","axis","outShape","shape","slice","length","splice","reshape","repeat","n","tidy","tile","flatten","newShape","batchFlatten","rank","sliceAlongFirstAxis","array","start","size","sliceAlongLastAxis","sliceAlongAxis","concatenate","tensors","concatAlongFirstAxis","a","b","Array","isArray","randomNormal","mean","stddev","seed","dot","activation","bias","transposeA","transposeB","reshapeBias","aFirstDims","aLastDim","pop","bShape","bLastDim","ySecondLastDim","yOtherDims","perm","from","_","i","transpose","outputShape","gather","reference","indices","tensor1d","toInt","square","xRank","dataFormat","biasShape","concat","biasAdd","add","elu","alpha","softsign","dropout","level","noiseShape","hardSigmoid","y","inTrainPhase","alt","training","Activation","serialization","getConfig","Elu","apply","className","Selu","Relu","Relu6","Linear","Sigmoid","HardSigmoid","Softplus","Softsign","Tanh","Softmax","LogSoftmax","Swish","mul","Mish","serializeActivation","getClassName","deserializeActivation","config","customObjects","getMap","classNameMap","getActivation","identifier","_nextUniqueTensorId","getNextUniqueTensorId","_uidPrefixes","getUid","prefix","toString"],"sourceRoot":""}