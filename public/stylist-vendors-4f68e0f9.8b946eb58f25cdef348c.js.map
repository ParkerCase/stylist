{"version":3,"file":"stylist-vendors-4f68e0f9.8b946eb58f25cdef348c.js","mappings":"+OAyBO,MAAMA,UAA0B,EAAAC,EACnC,WAAAC,CAAYC,EAAcC,EAAUC,GAAc,GAC9CC,MAAMH,GACNI,KAAKJ,aAAeA,EACpBI,KAAKH,SAAWA,EAChBG,KAAKF,YAAcA,EACnBE,KAAKC,cAAgB,GACrBD,KAAKE,GAAI,OAAOF,KAAKH,SACzB,CACA,cAAAM,CAAeC,IACWC,MAAMC,QAAQF,GAChCA,EAAkBG,KAAIC,GAAQA,EAAKC,OACnCC,OAAOC,KAAKP,IACFQ,SAAQ,CAACH,EAAMI,KACzB,MAAMC,EAAQ,KAAOC,oBAAoBN,GACzC,GAA6B,MAAzBT,KAAKC,cAAcY,GAAY,CAC/B,MAAMG,GAAY,EAClBhB,KAAKC,cAAcY,GAAK,CACpBI,aAAc,GAAGR,aACjBS,UAAU,SAAK,KAAM,OAAUJ,GAAOI,SAASF,KAEvD,CACA,MAAMG,EAAenB,KAAKC,cAAcY,GAAGK,SACrCE,EAAWf,MAAMC,QAAQF,GAC3BA,EAAkBS,GAAGQ,OACrBjB,EAAkBK,GACN,MAAZW,IAGJ,SAAK,KACD,IAAIE,EACJ,MAAMC,GAAkB,QAAI,OAAIvB,KAAKE,EAAGiB,GAAeC,GAEnDE,EADAtB,KAAKF,aACM,QAAI,OAAIE,KAAKwB,GAAG,OAAIJ,GAAU,OAAIG,EAAiBvB,KAAKE,KAAMY,IAG9D,QAAI,OAAId,KAAKwB,EAAGD,GAAkBT,GAEjDK,EAAaM,OAAOF,GACpBT,EAAMW,OAAOH,EAAS,GACxB,IAENtB,KAAK0B,qBACT,CACA,OAAAC,GACI3B,KAAKE,EAAEyB,UACmB,MAAtB3B,KAAKC,gBACL,QAAQD,KAAKC,cAAcM,KAAIqB,GAAKA,EAAEV,WAE9C,CAMA,WAAAW,CAAYhC,GACRG,KAAKH,SAAWA,CACpB,CACA,gBAAMiC,GAEF,MAAO,OAAO9B,KAAK+B,kBAAkBC,OAAOhC,KAAKC,cAAcM,KAAIqB,IAAK,CAAGnB,KAAMmB,EAAEX,aAAcI,OAAQO,EAAEV,aAC/G,CACA,gBAAMe,CAAWC,GACbA,QAAqBlC,KAAKmC,kBAAkBD,GAE5ClC,KAAKC,cAAgBiC,EAAa3B,KAAIqB,IAAK,CAAGX,aAAcW,EAAEnB,KAAMS,SAAUU,EAAEP,OAAOH,SADrE,UAEtB,CACA,SAAAkB,GACI,MAAO,CACH,aAAgBpC,KAAKJ,aACrB,SAAYI,KAAKH,SACjB,YAAeG,KAAKF,YAE5B,CAEA,iBAAOuC,CAAWC,EAAKC,GACnB,OAAO,IAAID,EAAIC,EAAqB,aAAGA,EAAiB,SAAGA,EAAoB,YACnF,EAGJ9C,EAAkB+C,UAAY,YAC9B,IAAAC,eAAchD,E,uGChDP,MAAMiD,GAAY,E,SAAAC,IAAG,CAAEC,WAlB9B,SAAoBC,EAAGC,GACnB,MAAMC,GAAK,QAAgBF,EAAG,IAAK,aAUnC,GATY,MAARC,IACAA,EAAOC,EAAGC,MAAMzC,KAAI,CAAC0C,EAAGpC,IAAMA,IAAGqC,WAErC,KAAYH,EAAGI,OAASL,EAAKM,QAAQ,IAAM,qCAAqCL,EAAGI,kCAClDL,OACjCA,EAAKlC,SAAQyC,IACT,KAAYA,GAAQ,GAAKA,EAAON,EAAGI,MAAM,IAAM,gDAA+CJ,EAAGI,KAAO,GACpG,YAAYL,KAAO,IAEvBC,EAAGI,MAAQ,EACX,OAAOJ,EAAGO,QAEd,MAAMC,EAAS,CAAEV,EAAGE,GACdS,EAAQ,CAAEV,QAChB,OAAO,KAAOW,UAAU,KAAWF,EAAQC,EAC/C,G,mHCMO,MAAME,GAAQ,E,SAAAf,IAAG,CAAEgB,OAlB1B,SAAgBC,EAAWC,EAAGC,GAC1B,MAAMC,GAAK,QAAgBF,EAAG,IAAK,SAC7BG,GAAK,QAAgBF,EAAG,IAAK,SAC7BG,GAAa,QAAgBL,EAAW,YAAa,QAAS,QAI9DM,GAAiB,SAA2B,QAA2BD,EAAWjB,MAAOe,EAAGf,OAAQgB,EAAGhB,OAIvGO,EAAS,CACXK,WAJ0B,OAAYK,EAAYC,GAKlDC,GAJkB,OAAYJ,EAAIG,GAKlCE,GAJkB,OAAYJ,EAAIE,IAMtC,OAAO,KAAOT,UAAU,KAAQF,EACpC,G,iFCzCO,MAAMc,EACT,KAAAC,CAAMC,EAAMC,GACR,OAAOF,MAAMC,EAAMC,EACvB,CACA,GAAAC,GACI,OAAOC,YAAYD,KACvB,CACA,MAAAE,CAAOC,EAAMC,GACT,GAAiB,UAAbA,GAAqC,SAAbA,EACxB,MAAM,IAAIC,MAAM,kDAAkDD,KAKtE,OAHwB,MAApB7E,KAAK+E,cACL/E,KAAK+E,YAAc,IAAIC,aAEpBhF,KAAK+E,YAAYJ,OAAOC,EACnC,CACA,MAAAK,CAAOC,EAAOL,GACV,OAAO,IAAIM,YAAYN,GAAUI,OAAOC,EAC5C,EAEJ,IAAI,UAAME,IAAI,cAAe,EACzB,UAAMC,YAAY,UAAW,IAAIhB,GAEjC,IACI,KAA0BiB,gBAAgB,KAAoBC,WAAY,IAAI,KAClF,CACA,MAAOC,GACP,CAEA,IACI,KAA0BF,gBAAgB,KAAiBC,WAAY,IAAI,KAC/E,CACA,MAAOC,GACP,CACJ,C,6FCIO,MAAMC,GAAO,E,SAAA9C,IAAG,CAAE+C,MAfzB,SAAe7C,EAAG8C,EAAI,EAAGC,GAAS,GAC9B,MAAM7C,GAAK,QAAgBF,EAAG,IAAK,QACnC,GAAgB,IAAZE,EAAGI,KACH,MAAM,IAAI2B,MAAM,sDAEpB,MAAMe,EAAU9C,EAAGC,MAAMD,EAAGC,MAAMI,OAAS,GAC3C,GAAIuC,EAAIE,EACJ,MAAM,IAAIf,MAAM,uDAAuDe,cACxDF,KAEnB,MAAMpC,EAAS,CAAEV,EAAGE,GACdS,EAAQ,CAAEmC,IAAGC,WACZE,EAAQC,GAAW,KAAOtC,UAAU,KAAMF,EAAQC,GACzD,MAAO,CAAEsC,SAAQC,UACrB,G,oLC9BO,MAAMC,UAAyB,IAClC,WAAArG,CAAYC,EAAcqG,EAAQ,GAAKpG,EAAW,EAAKqG,EAAU,KAAMC,GAAW,GAa9E,GAZApG,QACAC,KAAKJ,aAAeA,EACpBI,KAAKiG,MAAQA,EACbjG,KAAKH,SAAWA,EAChBG,KAAKkG,QAAUA,EACflG,KAAKoG,uBAAyB,GAC9BpG,KAAKqG,mBAAqB,GAC1BrG,KAAKsG,qBAAuB,GAC5BtG,KAAKmG,SAAWA,EACD,MAAXD,IACAlG,KAAKkG,QAAU,KAAOK,QAAQL,WAEd,MAAhBtG,EACA,MAAM,IAAIkF,MAAM,qDAExB,CACA,cAAA3E,CAAeC,IACWC,MAAMC,QAAQF,GAChCA,EAAkBG,KAAIC,GAAQA,EAAKC,OACnCC,OAAOC,KAAKP,IACFQ,SAAQ,CAACH,EAAMI,KACzB,MAAMC,EAAQ,KAAOC,oBAAoBN,GACnCO,GAAY,EACoB,MAAlChB,KAAKoG,uBAAuBvF,KAC5Bb,KAAKoG,uBAAuBvF,GAAK,CAC7BI,aAAc,GAAGR,QACjBS,UAAU,SAAK,KAAM,OAAUJ,GAAOI,SAASF,OAGrB,MAA9BhB,KAAKqG,mBAAmBxF,KACxBb,KAAKqG,mBAAmBxF,GAAK,CACzBI,aAAc,GAAGR,aACjBS,UAAU,SAAK,KAAM,OAAUJ,GAAOI,SAASF,OAGnB,MAAhChB,KAAKsG,qBAAqBzF,IAAcb,KAAKmG,WAC7CnG,KAAKsG,qBAAqBzF,GAAK,CAC3BI,aAAc,GAAGR,OACjBS,UAAU,SAAK,KAAM,OAAUJ,GAAOI,SAASF,OAGvD,MAAMI,EAAWf,MAAMC,QAAQF,GAC3BA,EAAkBS,GAAGQ,OACrBjB,EAAkBK,GACtB,GAAgB,MAAZW,EACA,OAEJ,MAAMoF,EAAwBxG,KAAKoG,uBAAuBvF,GAAGK,SACvDmF,EAAqBrG,KAAKqG,mBAAmBxF,GAAGK,UACtD,SAAK,KACD,MAAMuF,GAA2B,QAAI,OAAID,EAAuBxG,KAAKiG,QAAQ,QAAI,OAAO7E,GAAW,EAAIpB,KAAKiG,QAC5G,GAAIjG,KAAKmG,SAAU,CACf,MAAMO,EAAsB1G,KAAKsG,qBAAqBzF,GAAGK,SAEnDyF,GAAyB,QAAI,OAAID,EAAqB1G,KAAKiG,QAAQ,OAAI7E,EAAU,EAAIpB,KAAKiG,QAC1FW,GAAmB,QAAI,OAAIxF,EAAUpB,KAAKJ,eAAe,QAAK,OAAI6G,GAA0B,QAAI,OAAOE,GAAyB3G,KAAKkG,YACrIW,GAAwB,QAAI,OAAIR,EAAoBrG,KAAKH,UAAW+G,GAC1EJ,EAAsB/E,OAAOgF,GAC7BC,EAAoBjF,OAAOkF,GAC3BN,EAAmB5E,OAAOoF,GAC1B,MAAMvF,GAAW,OAAIR,EAAO+F,GAC5B/F,EAAMW,OAAOH,EACjB,KACK,CAED,MAAMmF,GAA2B,QAAI,OAAID,EAAuBxG,KAAKiG,QAAQ,QAAI,OAAO7E,GAAW,EAAIpB,KAAKiG,QACtGY,GAAwB,QAAI,OAAIR,EAAoBrG,KAAKH,WAAW,QAAI,OAAIuB,EAAUpB,KAAKJ,eAAe,QAAK,OAAI6G,EAA0BzG,KAAKkG,YACxJM,EAAsB/E,OAAOgF,GAC7BJ,EAAmB5E,OAAOoF,GAC1B,MAAMvF,GAAW,OAAIR,EAAO+F,GAC5B/F,EAAMW,OAAOH,EACjB,IACF,IAENtB,KAAK0B,qBACT,CACA,OAAAC,GACuC,MAA/B3B,KAAKoG,yBACL,QAAQpG,KAAKoG,uBAAuB7F,KAAIqB,GAAKA,EAAEV,YAElB,MAA7BlB,KAAKsG,sBAAgCtG,KAAKmG,WAC1C,QAAQnG,KAAKsG,qBAAqB/F,KAAIqB,GAAKA,EAAEV,YAElB,MAA3BlB,KAAKqG,qBACL,QAAQrG,KAAKqG,mBAAmB9F,KAAIqB,GAAKA,EAAEV,WAEnD,CACA,gBAAMY,GAEF,MAAMgF,EAAY,IAAI9G,KAAKoG,0BAA2BpG,KAAKqG,oBAI3D,OAHIrG,KAAKmG,UACLW,EAAUC,QAAQ/G,KAAKsG,sBAEpB,OAAOtG,KAAK+B,kBAAkBC,OAAO8E,EAAUvG,KAAIqB,IAAK,CAAGnB,KAAMmB,EAAEX,aAAcI,OAAQO,EAAEV,aACtG,CACA,gBAAMe,CAAWC,GACbA,QAAqBlC,KAAKmC,kBAAkBD,GAC5C,MAAM8E,EAAgBhH,KAAKmG,SAAWjE,EAAakB,OAAS,EAAIlB,EAAakB,OAAS,EAChFpC,GAAY,EAClBhB,KAAKoG,uBACDlE,EAAa+E,MAAM,EAAGD,GAAezG,KAAIqB,IAAK,CAC1CX,aAAcW,EAAEnB,KAChBS,SAAUU,EAAEP,OAAOH,SAASF,OAEpChB,KAAKqG,mBACDnE,EAAa+E,MAAMD,EAA+B,EAAhBA,GAC7BzG,KAAIqB,IAAK,CACVX,aAAcW,EAAEnB,KAChBS,SAAUU,EAAEP,OAAOH,SAASF,OAEhChB,KAAKmG,WACLnG,KAAKsG,qBACDpE,EAAa+E,MAAsB,EAAhBD,EAAmC,EAAhBA,GACjCzG,KAAIqB,IAAK,CACVX,aAAcW,EAAEnB,KAChBS,SAAUU,EAAEP,OAAOH,SAASF,OAG5C,CACA,SAAAoB,GACI,MAAO,CACH,aAAgBpC,KAAKJ,aACrB,MAASI,KAAKiG,MACd,SAAYjG,KAAKH,SACjB,QAAWG,KAAKkG,QAChB,SAAYlG,KAAKmG,SAEzB,CAEA,iBAAO9D,CAAWC,EAAKC,GACnB,OAAO,IAAID,EAAIC,EAAqB,aAAGA,EAAc,MAAGA,EAAiB,SAAGA,EAAgB,QAAGA,EAAiB,SACpH,EAGJyD,EAAiBxD,UAAY,WAC7B,IAAAC,eAAcuD,E,wIC7IP,MAAMkB,UAAqB,IAC9B,WAAAvH,CAAYC,GACRG,QACAC,KAAKJ,aAAeA,EACpBI,KAAKmH,gBAAgBvH,EACzB,CACA,cAAAO,CAAeC,IACMC,MAAMC,QAAQF,GAC3BA,EAAkBG,KAAIqB,GAAKA,EAAEnB,OAC7BC,OAAOC,KAAKP,IACPQ,SAAQ,CAACH,EAAMI,KACpB,MAAMO,EAAWf,MAAMC,QAAQF,GAC3BA,EAAkBS,GAAGQ,OACrBjB,EAAkBK,GACtB,GAAgB,MAAZW,EACA,OAEJ,MAAMN,EAAQ,KAAOC,oBAAoBN,IACzC,SAAK,KACD,MAAMa,GAAW,QAAI,OAAItB,KAAKwB,EAAGJ,GAAWN,GAC5CA,EAAMW,OAAOH,EAAS,GACxB,IAENtB,KAAK0B,qBACT,CAIA,eAAAyF,CAAgBvH,GACZI,KAAKJ,aAAeA,EACN,MAAVI,KAAKwB,GACLxB,KAAKwB,EAAEG,UAEX3B,KAAKwB,GAAI,SAAK,QAAQ5B,GAC1B,CACA,OAAA+B,GACI3B,KAAKwB,EAAEG,SACX,CACA,gBAAMG,GACF,MAAO,OAAO9B,KAAK+B,iBACvB,CACA,gBAAME,CAAWC,GAEb,GAA4B,KAD5BA,QAAqBlC,KAAKmC,kBAAkBD,IAC3BkB,OACb,MAAM,IAAI0B,MAAM,gDAExB,CACA,SAAA1C,GACI,MAAO,CAAE,aAAgBpC,KAAKJ,aAClC,CAEA,iBAAOyC,CAAWC,EAAKC,GACnB,OAAO,IAAID,EAAIC,EAAqB,aACxC,EAGJ2E,EAAa1E,UAAY,OACzB,IAAAC,eAAcyE,E,6FCjDP,SAASE,EAAMpE,EAAOqE,EAAQ,WACjC,GAAc,cAAVA,EAAuB,CACvB,MAAMC,EAAOF,EAAMpE,EAAO,WACpBuE,EAAOH,EAAMpE,EAAO,WAC1B,OAAO,OAAQsE,EAAMC,EACzB,CACA,MAAMzB,GAAS,SAAoB,QAAc9C,GAAQqE,GACzD,OAAO,KAAOG,WAAW1B,EAAQ9C,EAAOqE,EAC5C,C,kFCOO,MAAMI,EATbC,eAA2B9D,GACvB,MAAMK,GAAa,QAAgBL,EAAW,YAAa,aAAc,QACnE+D,QAAa1D,EAAW2D,OACxBC,GAAM,OAAU5D,EAAWjB,MAAO2E,GAIxC,OAHI/D,IAAcK,GACdA,EAAWtC,UAERkG,CACX,C,6FCRO,MAAMC,GAAY,E,SAAAnF,IAAG,CAAEoF,WAL9B,SAAoBlF,GAChB,MACMU,EAAS,CAAEV,GADN,QAAgBA,EAAG,IAAK,cAEnC,OAAO,KAAOY,UAAU,KAAWF,EACvC,G,wGChBO,MAAMyE,UAAkB,EAAAC,aAe3B,QAAAC,CAASC,EAAGC,GAAa,EAAOC,GAC5B,MAAM,MAAEvH,EAAK,MAAEwH,GAAUtI,KAAKuI,iBAAiBJ,EAAGE,GAClD,GAAe,MAAXA,EAAiB,CACjB,MAAMG,EAAYH,EAAQ9H,KAAIqB,IAAK,CAAGnB,KAAMmB,EAAEnB,KAAMY,OAAQiH,EAAM1G,EAAEnB,UACpET,KAAKG,eAAeqI,EACxB,MAEIxI,KAAKG,eAAemI,GAIxB,OADA,QAAQA,GACJF,EACOtH,GAGPA,EAAMa,UACC,KAEf,CAIA,cAAI8G,GAIA,OAHwB,MAApBzI,KAAK0I,cACL1I,KAAK0I,YAAc,GAEhB1I,KAAK0I,WAChB,CACA,mBAAAhH,GACI1B,KAAK0I,YAAc1I,KAAKyI,WAAa,CACzC,CAcA,gBAAAF,CAAiBJ,EAAGE,GAChB,OAAO,QAAcF,EAAGE,EAC5B,CAIA,OAAA1G,GAC4B,MAApB3B,KAAK0I,cACL,QAAQ1I,KAAK0I,YAErB,CACA,oBAAM3G,GAIF,OAHwB,MAApB/B,KAAK0I,cACL1I,KAAK0I,YAAc,GAEhB,CACHjI,KAAM,OAENY,QAAQ,OAAOrB,KAAK0I,YAAa,SAEzC,CACA,gBAAM5G,GACF,MAAM,IAAIgD,MAAM,0DACpB,CACA,gBAAM7C,CAAWC,GACb,MAAM,IAAI4C,MACN,4DAAG9E,KAAK2I,iBAChB,CAQA,uBAAMxG,CAAkBD,GAEpB,OADAlC,KAAK0I,mBAAqBxG,EAAa,GAAGb,OAAOuG,QAAQ,GAClD1F,EAAa+E,MAAM,EAC9B,EAEJvG,OAAOkI,eAAeZ,EAAWa,OAAOC,YAAa,CACjDhI,MAAQiI,GACwB,MAArBA,EAASb,UAAiD,MAA7Ba,EAASR,kBACd,MAA3BQ,EAAS5I,gB,wGCrEd,MAAM6I,GAAO,E,SAAArG,IAAG,CAAEsG,MARzB,SAAepG,EAAGqG,GACd,MAAMnG,GAAK,QAAgBF,EAAG,IAAK,OAAQ,qBAC3C,KAAYE,EAAGI,OAAS+F,EAAK9F,QAAQ,IAAM,qCAAqCL,EAAGI,kCAClD+F,OACjC,MAAM3F,EAAS,CAAEV,EAAGE,GACdS,EAAQ,CAAE0F,QAChB,OAAO,KAAOzF,UAAU,KAAMF,EAAQC,EAC1C,G,uECpBO,SAAStC,EAASiI,EAAcnI,GAAY,EAAMP,EAAM4G,GAC3D,OAAO,KAAO+B,aAAaD,EAAcnI,EAAWP,EAAM4G,EAC9D,C,wGCOO,MAAMgC,GAAU,E,SAAA1G,IAAG,CAAE2G,SAP5B,SAAkBzG,EAAGQ,EAAO,GACxB,MAAMN,GAAK,QAAgBF,EAAG,IAAK,UAAW,qBAC9C,KAAYQ,IAASN,EAAGC,MAAMI,QAAUC,EAAON,EAAGC,MAAMI,QAAQ,IAAM,UAAUC,iBAAoBN,EAAGC,MAAMI,WAAWL,EAAGC,MAAMI,YACjI,MAAMG,EAAS,CAAEzC,MAAOiC,GAClBS,EAAQ,CAAEH,QAChB,OAAO,KAAOI,UAAU,KAAQF,EAAQC,EAC5C,G,wGCqCO,MAAM+F,GAAS,E,SAAA5G,IAAG,CAAE6G,QAR3B,SAAiB3G,EAAGQ,EAAO,GACvB,MAAMN,GAAK,QAAgBF,EAAG,IAAK,SAAU,sBAC7C,QAAOE,EAAGI,KAAO,GAAG,IAAM,yCAC1B,MAAMI,EAAS,CAAEV,EAAGE,GACdS,EAAQ,CAAEH,SACTyC,EAAQC,GAAW,KAAOtC,UAAU,KAAQF,EAAQC,GAC3D,MAAO,CAAEsC,SAAQC,UACrB,G,0MChDO,MAAM0D,UAAsB,IAC/B,WAAA9J,CAAYC,EAAc8J,EAAOC,EAAOzD,EAAU,MAC9CnG,QACAC,KAAKJ,aAAeA,EACpBI,KAAK0J,MAAQA,EACb1J,KAAK2J,MAAQA,EACb3J,KAAKkG,QAAUA,EACflG,KAAK4J,uBAAyB,GAC9B5J,KAAK6J,wBAA0B,IAC/B,SAAK,KAED7J,KAAK8J,UAAW,OAAOJ,GAAOxI,WAC9BlB,KAAK+J,UAAW,OAAOJ,GAAOzI,UAAU,IAE7B,MAAXgF,IACAlG,KAAKkG,QAAU,KAAOK,QAAQL,UAEtC,CACA,cAAA/F,CAAeC,GACX,MAAM4J,EAAW3J,MAAMC,QAAQF,GAC3BA,EAAkBG,KAAIqB,GAAKA,EAAEnB,OAC7BC,OAAOC,KAAKP,IAChB,SAAK,KACD,MAAM6J,GAAmB,OAAI,EAAGjK,KAAK8J,UAC/BI,GAAmB,OAAI,EAAGlK,KAAK+J,UACrCC,EAASpJ,SAAQ,CAACH,EAAMI,KACpB,MAAMC,EAAQ,KAAOC,oBAAoBN,GACnCO,GAAY,EACoB,MAAlChB,KAAK4J,uBAAuB/I,KAC5Bb,KAAK4J,uBAAuB/I,GAAK,CAC7BI,aAAc,GAAGR,MACjBS,UAAU,SAAK,KAAM,OAAUJ,GAAOI,SAASF,OAGhB,MAAnChB,KAAK6J,wBAAwBhJ,KAC7Bb,KAAK6J,wBAAwBhJ,GAAK,CAC9BI,aAAc,GAAGR,MACjBS,UAAU,SAAK,KAAM,OAAUJ,GAAOI,SAASF,OAGvD,MAAMI,EAAWf,MAAMC,QAAQF,GAC3BA,EAAkBS,GAAGQ,OACrBjB,EAAkBK,GACtB,GAAgB,MAAZW,EACA,OAEJ,MAAM+I,EAAcnK,KAAK4J,uBAAuB/I,GAAGK,SAC7CkJ,EAAepK,KAAK6J,wBAAwBhJ,GAAGK,SAC/CmJ,GAAiB,QAAI,OAAIF,EAAanK,KAAK0J,QAAQ,OAAItI,EAAU,EAAIpB,KAAK0J,QAC1EY,GAAkB,QAAI,OAAIF,EAAcpK,KAAK2J,QAAQ,QAAI,OAAOvI,GAAW,EAAIpB,KAAK2J,QACpFY,GAA2B,OAAIF,EAAgBJ,GAC/CO,GAA4B,OAAIF,EAAiBJ,GACvDC,EAAY1I,OAAO4I,GACnBD,EAAa3I,OAAO6I,GACpB,MAAMhJ,GAAW,QAAI,QAAI,OAAIiJ,GAA0B,QAAI,OAAKC,GAA4BxK,KAAKkG,WAAYlG,KAAKJ,cAAekB,GACjIA,EAAMW,OAAOH,EAAS,IAE1BtB,KAAK8J,SAASrI,QAAO,OAAIzB,KAAK8J,SAAU9J,KAAK0J,QAC7C1J,KAAK+J,SAAStI,QAAO,OAAIzB,KAAK+J,SAAU/J,KAAK2J,OAAO,IAExD3J,KAAK0B,qBACT,CACA,OAAAC,GACI3B,KAAK8J,SAASnI,UACd3B,KAAK+J,SAASpI,UACqB,MAA/B3B,KAAK4J,yBACL,QAAQ5J,KAAK4J,uBAAuBrJ,KAAIqB,GAAKA,EAAEV,YAEf,MAAhClB,KAAK6J,0BACL,QAAQ7J,KAAK6J,wBAAwBtJ,KAAIqB,GAAKA,EAAEV,WAExD,CACA,gBAAMY,GAEF,MAAMgF,EAAY,IAAI9G,KAAK4J,0BAA2B5J,KAAK6J,yBAC3D,MAAO,OAAO7J,KAAK+B,kBAAkBC,OAAO8E,EAAUvG,KAAIqB,IAAK,CAAGnB,KAAMmB,EAAEX,aAAcI,OAAQO,EAAEV,aACtG,CACA,gBAAMe,CAAWC,GACbA,QAAqBlC,KAAKmC,kBAAkBD,IAC5C,SAAK,KACDlC,KAAK8J,SAASrI,QAAO,OAAIzB,KAAK0J,MAAO1J,KAAK0I,YAAc,IACxD1I,KAAK+J,SAAStI,QAAO,OAAIzB,KAAK2J,MAAO3J,KAAK0I,YAAc,GAAG,IAE/D,MAAM1B,EAAgB9E,EAAakB,OAAS,EACtCpC,GAAY,EAClBhB,KAAK4J,uBACD1H,EAAa+E,MAAM,EAAGD,GAAezG,KAAIqB,IAAK,CAC1CX,aAAcW,EAAEnB,KAChBS,SAAUU,EAAEP,OAAOH,SAASF,OAEpChB,KAAK6J,wBACD3H,EAAa+E,MAAMD,EAA+B,EAAhBA,GAC7BzG,KAAIqB,IAAK,CACVX,aAAcW,EAAEnB,KAChBS,SAAUU,EAAEP,OAAOH,SAASF,MAExC,CACA,SAAAoB,GACI,MAAO,CACH,aAAgBpC,KAAKJ,aACrB,MAASI,KAAK0J,MACd,MAAS1J,KAAK2J,MACd,QAAW3J,KAAKkG,QAExB,CAEA,iBAAO7D,CAAWC,EAAKC,GACnB,OAAO,IAAID,EAAIC,EAAqB,aAAGA,EAAc,MAAGA,EAAc,MAAGA,EAAgB,QAC7F,EAGJkH,EAAcjH,UAAY,QAC1B,IAAAC,eAAcgH,E,wICtHP,MAAMgB,EAsCT,UAAOC,CAAI9K,GACP,OAAO,IAAI,EAAAF,EAAaE,EAC5B,CAgBA,eAAOC,CAASD,EAAcC,EAAUC,GAAc,GAClD,OAAO,IAAI,IAAkBF,EAAcC,EAAUC,EACzD,CAqBA,cAAO6K,CAAQ/K,EAAcqG,EAAQ,GAAIpG,EAAW,EAAKqG,EAAU,KAAMC,GAAW,GAChF,OAAO,IAAI,IAAiBvG,EAAcqG,EAAOpG,EAAUqG,EAASC,EACxE,CAaA,WAAOyE,CAAKhL,EAAe,KAAO8J,EAAQ,GAAKC,EAAQ,KAAOzD,EAAU,MACpE,OAAO,IAAI,IAActG,EAAc8J,EAAOC,EAAOzD,EACzD,CAaA,eAAO2E,CAASjL,EAAe,KAAMkL,EAAM,IAAK5E,EAAU,MACtD,OAAO,IAAI,IAAkBtG,EAAckL,EAAK5E,EACpD,CAcA,aAAO6E,CAAOnL,EAAe,KAAO8J,EAAQ,GAAKC,EAAQ,KAAOzD,EAAU,KAAMD,EAAQ,GACpF,OAAO,IAAI,IAAgBrG,EAAc8J,EAAOC,EAAOzD,EAASD,EACpE,CAiBA,cAAO+E,CAAQpL,EAAcqL,EAA0B,IACnD,OAAO,IAAI,IAAiBrL,EAAcqL,EAC9C,E,kDCvJG,MAAMC,EAEI,IAAM,EAAQ,OAE/B,IAAIC,EAYG,MAAMC,EACT,WAAAzL,GAEIK,KAAKqL,KAAO,EAAQ,OAGpBrL,KAAK+E,YAAc,IAAI/E,KAAKqL,KAAKrG,WACrC,CACA,KAAAV,CAAMC,EAAM+G,GACR,OAA0B,OAAtB,UAAMC,OAAOjH,OACN,UAAMiH,OAAOjH,MAAMC,EAAM+G,IAEjB,MAAfH,IACAA,EAAcD,KAEXC,EAAY5G,EAAM+G,GAC7B,CACA,GAAA7G,GACI,MAAM+G,EAAOC,EAAQC,SACrB,OAAiB,IAAVF,EAAK,GAAYA,EAAK,GAAK,GACtC,CACA,MAAA7G,CAAOC,EAAMC,GACT,GAAiB,UAAbA,GAAqC,SAAbA,EACxB,MAAM,IAAIC,MAAM,sDAAsDD,KAE1E,OAAO7E,KAAK+E,YAAYJ,OAAOC,EACnC,CACA,MAAAK,CAAOC,EAAOL,GACV,OAAqB,IAAjBK,EAAM9B,OACC,GAEJ,IAAIpD,KAAKqL,KAAKlG,YAAYN,GAAUI,OAAOC,EACtD,GAEA,UAAME,IAAI,aACV,UAAMC,YAAY,OAAQ,IAAI+F,E,yKC1C3B,MAAMO,UAAyB,IAClC,WAAAhM,CAAYC,EAAcqL,EAA0B,IAChDlL,QACAC,KAAKJ,aAAeA,EACpBI,KAAKiL,wBAA0BA,EAC/BjL,KAAK4L,iBAAmB,EAC5B,CACA,cAAAzL,CAAeC,IACWC,MAAMC,QAAQF,GAChCA,EAAkBG,KAAIC,GAAQA,EAAKC,OACnCC,OAAOC,KAAKP,IACFQ,SAAQ,CAACH,EAAMI,KACzB,MAAMC,EAAQ,KAAOC,oBAAoBN,GACzC,GAAgC,MAA5BT,KAAK4L,iBAAiB/K,GAAY,CAClC,MAAMG,GAAY,EAClBhB,KAAK4L,iBAAiB/K,GAAK,CACvBI,aAAc,GAAGR,gBACjBS,UAAU,SAAK,KAAM,OAAKJ,EAAMkC,MAAOhD,KAAKiL,yBACvC/J,SAASF,KAEtB,CACA,MAAMI,EAAWf,MAAMC,QAAQF,GAC3BA,EAAkBS,GAAGQ,OACrBjB,EAAkBK,GACtB,GAAgB,MAAZW,EACA,OAEJ,MAAMyK,EAAkB7L,KAAK4L,iBAAiB/K,GAAGK,UACjD,SAAK,KACD,MAAM4K,GAAqB,OAAID,GAAiB,OAAOzK,IACvDyK,EAAgBpK,OAAOqK,GACvB,MAAMxK,GAAW,QAAI,QAAI,OAAIF,GAAU,QAAK,OAAI0K,EAAoB,KAAOvF,QAAQL,cAAelG,KAAKJ,cAAekB,GACtHA,EAAMW,OAAOH,EAAS,GACxB,IAENtB,KAAK0B,qBACT,CACA,OAAAC,GACiC,MAAzB3B,KAAK4L,mBACL,QAAQ5L,KAAK4L,iBAAiBrL,KAAIqB,GAAKA,EAAEV,WAEjD,CACA,gBAAMY,GAEF,MAAO,OAAO9B,KAAK+B,kBAAkBC,OAAOhC,KAAK4L,iBAAiBrL,KAAIqB,IAAK,CAAGnB,KAAMmB,EAAEX,aAAcI,OAAQO,EAAEV,aAClH,CACA,gBAAMe,CAAWC,GACbA,QAAqBlC,KAAKmC,kBAAkBD,GAE5ClC,KAAK4L,iBAAmB1J,EAAa3B,KAAIqB,IAAK,CAAGX,aAAcW,EAAEnB,KAAMS,SAAUU,EAAEP,OAAOH,SADxE,UAEtB,CACA,SAAAkB,GACI,MAAO,CACH,aAAgBpC,KAAKJ,aACrB,wBAA2BI,KAAKiL,wBAExC,CAEA,iBAAO5I,CAAWC,EAAKC,GACnB,OAAO,IAAID,EAAIC,EAAqB,aAAGA,EAAgC,wBAC3E,EAGJoJ,EAAiBnJ,UAAY,WAC7B,IAAAC,eAAckJ,E,wGC5CP,MAAMI,GAAqB,E,SAAApJ,IAAG,CAAEqJ,oBARvC,SAA6BnJ,EAAGoJ,EAAYC,GACxC,MAAMnJ,GAAK,QAAgBF,EAAG,IAAK,sBAC7BsJ,GAAc,QAAgBF,EAAY,aAAc,qBAAsB,UACpF,SAAO,QAAMC,IAAc,IAAM,qCACjC,MAAM3I,EAAS,CAAEV,EAAGE,EAAIkJ,WAAYE,GAC9B3I,EAAQ,CAAE0I,eAChB,OAAO,KAAOzI,UAAU,KAAoBF,EAAQC,EACxD,G,2FCIO,MAAM4I,GAAkB,IAAAzJ,IAAG,CAAE0J,iBAXpC,SAA0BrJ,EAAOsJ,EAAO,EAAGC,EAAS,EAAGlF,EAAOmF,GAC1D,GAAa,MAATnF,GAA2B,SAAVA,EACjB,MAAM,IAAIvC,MAAM,qCAEpB,MAAM2H,EAAY,IAAI,KAAYH,EAAMC,EAAQlF,GAAO,EAAsBmF,GACvE3E,GAAM,OAAO7E,EAAOqE,GAC1B,IAAK,IAAIxG,EAAI,EAAGA,EAAIgH,EAAI/B,OAAO1C,OAAQvC,IACnCgH,EAAI/B,OAAOjF,GAAK4L,EAAUC,YAE9B,OAAO7E,EAAI8E,UACf,G,8LCrBO,MAAMC,UAAwB,IACjC,WAAAjN,CAAYC,EAAc8J,EAAOC,EAAOzD,EAAU,KAAMD,EAAQ,GAC5DlG,QACAC,KAAKJ,aAAeA,EACpBI,KAAK0J,MAAQA,EACb1J,KAAK2J,MAAQA,EACb3J,KAAKkG,QAAUA,EACflG,KAAKiG,MAAQA,EACbjG,KAAK4J,uBAAyB,GAC9B5J,KAAK6M,2BAA6B,IAClC,SAAK,KACD7M,KAAK8M,WAAY,OAAO,GAAG5L,WAC3BlB,KAAK8J,UAAW,OAAOJ,GAAOxI,UAAU,IAE7B,MAAXgF,IACAlG,KAAKkG,QAAU,KAAOK,QAAQL,UAEtC,CACA,cAAA/F,CAAeC,GACX,MAAM2M,EAAgB1M,MAAMC,QAAQF,GAChCA,EAAkBG,KAAIC,GAAQA,EAAKC,OACnCC,OAAOC,KAAKP,IAChB,SAAK,KACD,MAAM6J,GAAmB,OAAI,EAAGjK,KAAK8J,UAC/BkD,GAAK,QAAKhN,KAAKJ,cAAc,QAAI,OAAII,KAAK8M,UAAW9M,KAAKiG,OAAQ,IACxE8G,EAAcnM,SAAQ,CAACH,EAAMI,KACzB,MAAMC,EAAQ,KAAOC,oBAAoBN,GACnCO,GAAY,EACoB,MAAlChB,KAAK4J,uBAAuB/I,KAC5Bb,KAAK4J,uBAAuB/I,GAAK,CAC7BI,aAAc,GAAGR,MACjBS,UAAU,OAAUJ,GAAOI,SAASF,KAGF,MAAtChB,KAAK6M,2BAA2BhM,KAChCb,KAAK6M,2BAA2BhM,GAAK,CACjCI,aAAc,GAAGR,MACjBS,UAAU,OAAUJ,GAAOI,SAASF,KAG5C,MAAMI,EAAWf,MAAMC,QAAQF,GAC3BA,EAAkBS,GAAGQ,OACrBjB,EAAkBK,GACtB,GAAgB,MAAZW,EACA,OAEJ,MAAM+I,EAAcnK,KAAK4J,uBAAuB/I,GAAGK,SAC7C+L,EAAkBjN,KAAK6M,2BAA2BhM,GAAGK,SACrDmJ,GAAiB,QAAI,OAAIF,EAAanK,KAAK0J,QAAQ,OAAItI,EAAU,EAAIpB,KAAK0J,QAC1EwD,GAAM,OAAID,EAAiBjN,KAAK2J,OAChCwD,GAAM,OAAI/L,GACVgM,GAAqB,OAAQF,EAAKC,GACxChD,EAAY1I,OAAO4I,GACnB4C,EAAgBxL,OAAO2L,GACvB,MAAM9L,GAAW,QAAI,QAAI,OAAI0L,EAAI/C,IAAmB,OAAII,GAAgB,OAAI+C,EAAoBpN,KAAKkG,WAAYpF,GACjHA,EAAMW,OAAOH,EAAS,IAE1BtB,KAAK8M,UAAUrL,QAAO,OAAIzB,KAAK8M,UAAW,IAC1C9M,KAAK8J,SAASrI,QAAO,OAAIzB,KAAK8J,SAAU9J,KAAK0J,OAAO,IAExD1J,KAAK0B,qBACT,CACA,OAAAC,GACI3B,KAAK8J,SAASnI,UACd3B,KAAK8M,UAAUnL,UACoB,MAA/B3B,KAAK4J,yBACL,QAAQ5J,KAAK4J,uBAAuBrJ,KAAIqB,GAAKA,EAAEV,YAEZ,MAAnClB,KAAK6M,6BACL,QAAQ7M,KAAK6M,2BAA2BtM,KAAIqB,GAAKA,EAAEV,WAE3D,CACA,gBAAMY,GACF,MAAM,IAAIgD,MAAM,kDACpB,CACA,gBAAM7C,CAAWC,GACb,MAAM,IAAI4C,MAAM,kDACpB,CACA,SAAA1C,GACI,MAAO,CACH,aAAgBpC,KAAKJ,aACrB,MAASI,KAAK0J,MACd,MAAS1J,KAAK2J,MACd,QAAW3J,KAAKkG,QAChB,MAASlG,KAAKiG,MAEtB,CAEA,iBAAO5D,CAAWC,EAAKC,GACnB,OAAO,IAAID,EAAIC,EAAqB,aAAGA,EAAc,MAAGA,EAAc,MAAGA,EAAgB,QAAGA,EAAc,MAC9G,EAGJqK,EAAgBpK,UAAY,UAC5B,IAAAC,eAAcmK,E,yKC/FP,MAAMS,UAA0B,IACnC,WAAA1N,CAAYC,EAAckL,EAAK5E,EAAU,MACrCnG,QACAC,KAAKJ,aAAeA,EACpBI,KAAK8K,IAAMA,EACX9K,KAAKkG,QAAUA,EACflG,KAAK4L,iBAAmB,GACxB5L,KAAKsN,mBAAqB,GACX,MAAXpH,IACAlG,KAAKkG,QAAU,KAAOK,QAAQL,UAEtC,CACA,cAAA/F,CAAeC,IACWC,MAAMC,QAAQF,GAChCA,EAAkBG,KAAIC,GAAQA,EAAKC,OACnCC,OAAOC,KAAKP,IACFQ,SAAQ,CAACH,EAAMI,KACzB,MAAMC,EAAQ,KAAOC,oBAAoBN,GACnCO,GAAY,EACc,MAA5BhB,KAAK4L,iBAAiB/K,KACtBb,KAAK4L,iBAAiB/K,GAAK,CACvBI,aAAc,GAAGR,eACjBS,UAAU,SAAK,KAAM,OAAUJ,GAAOI,SAASF,OAGrB,MAA9BhB,KAAKsN,mBAAmBzM,KACxBb,KAAKsN,mBAAmBzM,GAAK,CACzBI,aAAc,GAAGR,cACjBS,UAAU,SAAK,KAAM,OAAUJ,GAAOI,SAASF,OAGvD,MAAMI,EAAWf,MAAMC,QAAQF,GAC3BA,EAAkBS,GAAGQ,OACrBjB,EAAkBK,GACtB,GAAgB,MAAZW,EACA,OAEJ,MAAMyK,EAAkB7L,KAAK4L,iBAAiB/K,GAAGK,SAC3CqM,EAAoBvN,KAAKsN,mBAAmBzM,GAAGK,UACrD,SAAK,KACD,MAAM4K,GAAqB,QAAI,OAAID,EAAiB7L,KAAK8K,MAAM,QAAI,OAAO1J,GAAW,EAAIpB,KAAK8K,MACxF0C,GAAU,QAAI,QAAI,QAAK,OAAID,EAAmBvN,KAAKkG,WAAW,QAAK,OAAI2F,EAAiB7L,KAAKkG,WAAY9E,GACzGqM,GAAuB,QAAI,OAAIF,EAAmBvN,KAAK8K,MAAM,QAAI,OAAO0C,GAAU,EAAIxN,KAAK8K,MACjGe,EAAgBpK,OAAOqK,GACvByB,EAAkB9L,OAAOgM,GACzB,MAAMnM,GAAW,QAAI,OAAIkM,GAAUxN,KAAKJ,cAAekB,GACvDA,EAAMW,OAAOH,EAAS,GACxB,IAENtB,KAAK0B,qBACT,CACA,OAAAC,GACmC,MAA3B3B,KAAKsN,sBACL,QAAQtN,KAAK4L,iBAAiBrL,KAAIqB,GAAKA,EAAEV,aACzC,QAAQlB,KAAKsN,mBAAmB/M,KAAIqB,GAAKA,EAAEV,YAEnD,CACA,gBAAMY,GAEF,MAAMgF,EAAY,IAAI9G,KAAK4L,oBAAqB5L,KAAKsN,oBACrD,MAAO,OAAOtN,KAAK+B,kBAAkBC,OAAO8E,EAAUvG,KAAIqB,IAAK,CAAGnB,KAAMmB,EAAEX,aAAcI,OAAQO,EAAEV,aACtG,CACA,gBAAMe,CAAWC,GAEb,MAAM8E,GADN9E,QAAqBlC,KAAKmC,kBAAkBD,IACTkB,OAAS,EACtCpC,GAAY,EAClBhB,KAAK4L,iBACD1J,EAAa+E,MAAM,EAAGD,GAAezG,KAAIqB,IAAK,CAC1CX,aAAcW,EAAEnB,KAChBS,SAAUU,EAAEP,OAAOH,SAASF,OAEpChB,KAAKsN,mBACDpL,EAAa+E,MAAMD,EAA+B,EAAhBA,GAC7BzG,KAAIqB,IAAK,CACVX,aAAcW,EAAEnB,KAChBS,SAAUU,EAAEP,OAAOH,SAASF,MAExC,CACA,SAAAoB,GACI,MAAO,CACH,aAAgBpC,KAAKJ,aACrB,IAAOI,KAAK8K,IACZ,QAAW9K,KAAKkG,QAExB,CAEA,iBAAO7D,CAAWC,EAAKC,GACnB,OAAO,IAAID,EAAIC,EAAqB,aAAGA,EAAY,IAAGA,EAAgB,QAC1E,EAGJ8K,EAAkB7K,UAAY,YAC9B,IAAAC,eAAc4K,E","sources":["webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/optimizers/momentum_optimizer.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/transpose.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/where.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/platforms/platform_browser.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/topk.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/optimizers/rmsprop_optimizer.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/optimizers/sgd_optimizer.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/zeros.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/where_async.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/zeros_like.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/tile.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/variable.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/unstack.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/unique.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/optimizers/adam_optimizer.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/platforms/platform_node.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/optimizers/adagrad_optimizer.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/unsorted_segment_sum.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/truncated_normal.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/optimizers/adamax_optimizer.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/optimizers/adadelta_optimizer.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { SGDOptimizer } from './sgd_optimizer';\n/** @doclink Optimizer */\nexport class MomentumOptimizer extends SGDOptimizer {\n    constructor(learningRate, momentum, useNesterov = false) {\n        super(learningRate);\n        this.learningRate = learningRate;\n        this.momentum = momentum;\n        this.useNesterov = useNesterov;\n        this.accumulations = [];\n        this.m = scalar(this.momentum);\n    }\n    applyGradients(variableGradients) {\n        const variableNames = Array.isArray(variableGradients) ?\n            variableGradients.map(item => item.name) :\n            Object.keys(variableGradients);\n        variableNames.forEach((name, i) => {\n            const value = ENGINE.registeredVariables[name];\n            if (this.accumulations[i] == null) {\n                const trainable = false;\n                this.accumulations[i] = {\n                    originalName: `${name}/momentum`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            const accumulation = this.accumulations[i].variable;\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            tidy(() => {\n                let newValue;\n                const newAccumulation = add(mul(this.m, accumulation), gradient);\n                if (this.useNesterov) {\n                    newValue = add(mul(this.c, add(gradient, mul(newAccumulation, this.m))), value);\n                }\n                else {\n                    newValue = add(mul(this.c, newAccumulation), value);\n                }\n                accumulation.assign(newAccumulation);\n                value.assign(newValue);\n            });\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        this.m.dispose();\n        if (this.accumulations != null) {\n            dispose(this.accumulations.map(v => v.variable));\n        }\n    }\n    /**\n     * Sets the momentum of the optimizer.\n     *\n     * @param momentum\n     */\n    setMomentum(momentum) {\n        this.momentum = momentum;\n    }\n    async getWeights() {\n        // Order matters for Python compatibility.\n        return [await this.saveIterations()].concat(this.accumulations.map(v => ({ name: v.originalName, tensor: v.variable })));\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        const trainable = false;\n        this.accumulations = weightValues.map(v => ({ originalName: v.name, variable: v.tensor.variable(trainable) }));\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'momentum': this.momentum,\n            'useNesterov': this.useNesterov\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['momentum'], config['useNesterov']);\n    }\n}\n/** @nocollapse */\nMomentumOptimizer.className = 'Momentum'; // Name matters for Python compatibility.\nregisterClass(MomentumOptimizer);\n//# sourceMappingURL=momentum_optimizer.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Transpose } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\n/**\n * Transposes the `tf.Tensor`. Permutes the dimensions according to `perm`.\n *\n * The returned `tf.Tensor`'s dimension `i` will correspond to the input\n * dimension `perm[i]`. If `perm` is not given, it is set to `[n-1...0]`,\n * where `n` is the rank of the input `tf.Tensor`. Hence by default, this\n * operation performs a regular matrix transpose on 2-D input `tf.Tensor`s.\n *\n * ```js\n * const a = tf.tensor2d([1, 2, 3, 4, 5, 6], [2, 3]);\n *\n * a.transpose().print();  // or tf.transpose(a)\n * ```\n *\n * @param x The tensor to transpose.\n * @param perm The permutation of the dimensions of a.\n *\n * @doc {heading: 'Operations', subheading: 'Matrices'}\n */\nfunction transpose_(x, perm) {\n    const $x = convertToTensor(x, 'x', 'transpose');\n    if (perm == null) {\n        perm = $x.shape.map((s, i) => i).reverse();\n    }\n    util.assert($x.rank === perm.length, () => `Error in transpose: rank of input ${$x.rank} ` +\n        `must match length of perm ${perm}.`);\n    perm.forEach(axis => {\n        util.assert(axis >= 0 && axis < $x.rank, () => `All entries in 'perm' must be between 0 and ${$x.rank - 1}` +\n            ` but got ${perm}`);\n    });\n    if ($x.rank <= 1) {\n        return $x.clone();\n    }\n    const inputs = { x: $x };\n    const attrs = { perm };\n    return ENGINE.runKernel(Transpose, inputs, attrs);\n}\nexport const transpose = op({ transpose_ });\n//# sourceMappingURL=transpose.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Select } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { broadcastTo } from './broadcast_to';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the elements, either `a` or `b` depending on the `condition`.\n *\n * If the condition is true, select from `a`, otherwise select from `b`.\n *\n * ```js\n * const cond = tf.tensor1d([false, false, true], 'bool');\n * const a = tf.tensor1d([1 , 2, 3]);\n * const b = tf.tensor1d([-1, -2, -3]);\n *\n * a.where(cond, b).print();\n * ```\n *\n * @param condition The input condition. Must be of dtype bool.\n * @param a If `condition` is rank 1, `a` may have a higher rank but\n *     its first dimension must match the size of `condition`.\n * @param b A tensor with the same dtype as `a` and with shape that is\n *     compatible with `a`.\n * @return A tensor with same dtype as `a` and `b`, and shape that is\n *     broadcastable from `a` and `b`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction where_(condition, a, b) {\n    const $a = convertToTensor(a, 'a', 'where');\n    const $b = convertToTensor(b, 'b', 'where');\n    const $condition = convertToTensor(condition, 'condition', 'where', 'bool');\n    // TODO: move this logic to forward function when the broadcastTo op is\n    // implemented in WASM.\n    // Find the broadcastable shape for $condition, $a, and $b.\n    const broadcastShape = assertAndGetBroadcastShape(assertAndGetBroadcastShape($condition.shape, $a.shape), $b.shape);\n    const $broadcastedCondition = broadcastTo($condition, broadcastShape);\n    const $broadcastedA = broadcastTo($a, broadcastShape);\n    const $broadcastedB = broadcastTo($b, broadcastShape);\n    const inputs = {\n        condition: $broadcastedCondition,\n        t: $broadcastedA,\n        e: $broadcastedB\n    };\n    return ENGINE.runKernel(Select, inputs);\n}\nexport const where = op({ where_ });\n//# sourceMappingURL=where.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport '../flags';\nimport { env } from '../environment';\nimport { BrowserIndexedDB, BrowserIndexedDBManager } from '../io/indexed_db';\nimport { BrowserLocalStorage, BrowserLocalStorageManager } from '../io/local_storage';\nimport { ModelStoreManagerRegistry } from '../io/model_management';\nexport class PlatformBrowser {\n    fetch(path, init) {\n        return fetch(path, init);\n    }\n    now() {\n        return performance.now();\n    }\n    encode(text, encoding) {\n        if (encoding !== 'utf-8' && encoding !== 'utf8') {\n            throw new Error(`Browser's encoder only supports utf-8, but got ${encoding}`);\n        }\n        if (this.textEncoder == null) {\n            this.textEncoder = new TextEncoder();\n        }\n        return this.textEncoder.encode(text);\n    }\n    decode(bytes, encoding) {\n        return new TextDecoder(encoding).decode(bytes);\n    }\n}\nif (env().get('IS_BROWSER')) {\n    env().setPlatform('browser', new PlatformBrowser());\n    // Register LocalStorage IOHandler\n    try {\n        ModelStoreManagerRegistry.registerManager(BrowserLocalStorage.URL_SCHEME, new BrowserLocalStorageManager());\n    }\n    catch (err) {\n    }\n    // Register IndexedDB IOHandler\n    try {\n        ModelStoreManagerRegistry.registerManager(BrowserIndexedDB.URL_SCHEME, new BrowserIndexedDBManager());\n    }\n    catch (err) {\n    }\n}\n//# sourceMappingURL=platform_browser.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { TopK } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Finds the values and indices of the `k` largest entries along the last\n * dimension.\n *\n * If the input is a vector (rank=1), finds the k largest entries in the vector\n * and outputs their values and indices as vectors. Thus values[j] is the j-th\n * largest entry in input, and its index is indices[j].\n * For higher rank inputs, computes the top k entries along the last dimension.\n *\n * If two elements are equal, the lower-index element appears first.\n *\n * ```js\n * const a = tf.tensor2d([[1, 5], [4, 3]]);\n * const {values, indices} = tf.topk(a);\n * values.print();\n * indices.print();\n * ```\n * @param x 1-D or higher `tf.Tensor` with last dimension being at least `k`.\n * @param k Number of top elements to look for along the last dimension.\n * @param sorted If true, the resulting `k` elements will be sorted by the\n *     values in descending order.\n *\n * @doc {heading: 'Operations', subheading: 'Evaluation'}\n */\nfunction topk_(x, k = 1, sorted = true) {\n    const $x = convertToTensor(x, 'x', 'topk');\n    if ($x.rank === 0) {\n        throw new Error('topk() expects the input to be of rank 1 or higher');\n    }\n    const lastDim = $x.shape[$x.shape.length - 1];\n    if (k > lastDim) {\n        throw new Error(`'k' passed to topk() must be <= the last dimension (${lastDim}) ` +\n            `but got ${k}`);\n    }\n    const inputs = { x: $x };\n    const attrs = { k, sorted };\n    const [values, indices] = ENGINE.runKernel(TopK, inputs, attrs);\n    return { values, indices };\n}\nexport const topk = op({ topk_ });\n//# sourceMappingURL=topk.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class RMSPropOptimizer extends Optimizer {\n    constructor(learningRate, decay = 0.9, momentum = 0.0, epsilon = null, centered = false) {\n        super();\n        this.learningRate = learningRate;\n        this.decay = decay;\n        this.momentum = momentum;\n        this.epsilon = epsilon;\n        this.accumulatedMeanSquares = [];\n        this.accumulatedMoments = [];\n        this.accumulatedMeanGrads = [];\n        this.centered = centered;\n        if (epsilon == null) {\n            this.epsilon = ENGINE.backend.epsilon();\n        }\n        if (learningRate == null) {\n            throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n        }\n    }\n    applyGradients(variableGradients) {\n        const variableNames = Array.isArray(variableGradients) ?\n            variableGradients.map(item => item.name) :\n            Object.keys(variableGradients);\n        variableNames.forEach((name, i) => {\n            const value = ENGINE.registeredVariables[name];\n            const trainable = false;\n            if (this.accumulatedMeanSquares[i] == null) {\n                this.accumulatedMeanSquares[i] = {\n                    originalName: `${name}/rms`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            if (this.accumulatedMoments[i] == null) {\n                this.accumulatedMoments[i] = {\n                    originalName: `${name}/momentum`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            if (this.accumulatedMeanGrads[i] == null && this.centered) {\n                this.accumulatedMeanGrads[i] = {\n                    originalName: `${name}/mg`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n            const accumulatedMoments = this.accumulatedMoments[i].variable;\n            tidy(() => {\n                const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n                if (this.centered) {\n                    const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;\n                    // Centered gradient\n                    const newAccumulatedMeanGrad = add(mul(accumulatedMeanGrad, this.decay), mul(gradient, 1 - this.decay));\n                    const gradContribution = div(mul(gradient, this.learningRate), sqrt(sub(newAccumulatedMeanSquare, add(square(newAccumulatedMeanGrad), this.epsilon))));\n                    const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), gradContribution);\n                    accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n                    accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n                    accumulatedMoments.assign(newAccumulatedMoments);\n                    const newValue = sub(value, newAccumulatedMoments);\n                    value.assign(newValue);\n                }\n                else {\n                    // Plain gradient\n                    const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n                    const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), div(mul(gradient, this.learningRate), sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n                    accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n                    accumulatedMoments.assign(newAccumulatedMoments);\n                    const newValue = sub(value, newAccumulatedMoments);\n                    value.assign(newValue);\n                }\n            });\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        if (this.accumulatedMeanSquares != null) {\n            dispose(this.accumulatedMeanSquares.map(v => v.variable));\n        }\n        if (this.accumulatedMeanGrads != null && this.centered) {\n            dispose(this.accumulatedMeanGrads.map(v => v.variable));\n        }\n        if (this.accumulatedMoments != null) {\n            dispose(this.accumulatedMoments.map(v => v.variable));\n        }\n    }\n    async getWeights() {\n        // Order matters for Python compatibility.\n        const variables = [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n        if (this.centered) {\n            variables.push(...this.accumulatedMeanGrads);\n        }\n        return [await this.saveIterations()].concat(variables.map(v => ({ name: v.originalName, tensor: v.variable })));\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        const variableCount = this.centered ? weightValues.length / 3 : weightValues.length / 2;\n        const trainable = false;\n        this.accumulatedMeanSquares =\n            weightValues.slice(0, variableCount).map(v => ({\n                originalName: v.name,\n                variable: v.tensor.variable(trainable)\n            }));\n        this.accumulatedMoments =\n            weightValues.slice(variableCount, variableCount * 2)\n                .map(v => ({\n                originalName: v.name,\n                variable: v.tensor.variable(trainable)\n            }));\n        if (this.centered) {\n            this.accumulatedMeanGrads =\n                weightValues.slice(variableCount * 2, variableCount * 3)\n                    .map(v => ({\n                    originalName: v.name,\n                    variable: v.tensor.variable(trainable)\n                }));\n        }\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'decay': this.decay,\n            'momentum': this.momentum,\n            'epsilon': this.epsilon,\n            'centered': this.centered\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['decay'], config['momentum'], config['epsilon'], config['centered']);\n    }\n}\n/** @nocollapse */\nRMSPropOptimizer.className = 'RMSProp'; // Note: Name matters for Python compatibility.\nregisterClass(RMSPropOptimizer);\n//# sourceMappingURL=rmsprop_optimizer.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { keep, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class SGDOptimizer extends Optimizer {\n    constructor(learningRate) {\n        super();\n        this.learningRate = learningRate;\n        this.setLearningRate(learningRate);\n    }\n    applyGradients(variableGradients) {\n        const varNames = Array.isArray(variableGradients) ?\n            variableGradients.map(v => v.name) :\n            Object.keys(variableGradients);\n        varNames.forEach((name, i) => {\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            const value = ENGINE.registeredVariables[name];\n            tidy(() => {\n                const newValue = add(mul(this.c, gradient), value);\n                value.assign(newValue);\n            });\n        });\n        this.incrementIterations();\n    }\n    /**\n     * Sets the learning rate of the optimizer.\n     */\n    setLearningRate(learningRate) {\n        this.learningRate = learningRate;\n        if (this.c != null) {\n            this.c.dispose();\n        }\n        this.c = keep(scalar(-learningRate));\n    }\n    dispose() {\n        this.c.dispose();\n    }\n    async getWeights() {\n        return [await this.saveIterations()];\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        if (weightValues.length !== 0) {\n            throw new Error('SGD optimizer does not have settable weights.');\n        }\n    }\n    getConfig() {\n        return { 'learningRate': this.learningRate };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate']);\n    }\n}\n/** @nocollapse */\nSGDOptimizer.className = 'SGD'; // Note: Name matters for Python compatibility.\nregisterClass(SGDOptimizer);\n//# sourceMappingURL=sgd_optimizer.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { makeZerosTypedArray, sizeFromShape } from '../util';\nimport { complex } from './complex';\n/**\n * Creates a `tf.Tensor` with all elements set to 0.\n *\n * ```js\n * tf.zeros([2, 2]).print();\n * ```\n *\n * @param shape An array of integers defining the output tensor shape.\n * @param dtype The type of an element in the resulting tensor. Can\n *     be 'float32', 'int32' or 'bool'. Defaults to 'float'.\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nexport function zeros(shape, dtype = 'float32') {\n    if (dtype === 'complex64') {\n        const real = zeros(shape, 'float32');\n        const imag = zeros(shape, 'float32');\n        return complex(real, imag);\n    }\n    const values = makeZerosTypedArray(sizeFromShape(shape), dtype);\n    return ENGINE.makeTensor(values, shape, dtype);\n}\n//# sourceMappingURL=zeros.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { whereImpl } from '../backends/where_impl';\nimport { convertToTensor } from '../tensor_util_env';\n/**\n * Returns the coordinates of true elements of condition.\n *\n * The coordinates are returned in a 2-D tensor where the first dimension (rows)\n * represents the number of true elements, and the second dimension (columns)\n * represents the coordinates of the true elements. Keep in mind, the shape of\n * the output tensor can vary depending on how many true values there are in\n * input. Indices are output in row-major order. The resulting tensor has the\n * shape `[numTrueElems, condition.rank]`.\n *\n * This is analogous to calling the python `tf.where(cond)` without an x or y.\n *\n * ```js\n * const cond = tf.tensor1d([false, false, true], 'bool');\n * const result = await tf.whereAsync(cond);\n * result.print();\n * ```\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nasync function whereAsync_(condition) {\n    const $condition = convertToTensor(condition, 'condition', 'whereAsync', 'bool');\n    const vals = await $condition.data();\n    const res = whereImpl($condition.shape, vals);\n    if (condition !== $condition) {\n        $condition.dispose();\n    }\n    return res;\n}\nexport const whereAsync = whereAsync_;\n//# sourceMappingURL=where_async.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { ZerosLike } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Creates a `tf.Tensor` with all elements set to 0 with the same shape as the\n * given tensor.\n *\n * ```js\n * const x = tf.tensor([1, 2]);\n * tf.zerosLike(x).print();\n * ```\n *\n * @param x The tensor of required shape.\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nfunction zerosLike_(x) {\n    const $x = convertToTensor(x, 'x', 'zerosLike');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(ZerosLike, inputs);\n}\nexport const zerosLike = op({ zerosLike_ });\n//# sourceMappingURL=zeros_like.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\nexport class Optimizer extends Serializable {\n    /**\n     * Executes `f()` and minimizes the scalar output of `f()` by computing\n     * gradients of y with respect to the list of trainable variables provided by\n     * `varList`. If no list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to minimize.\n     * @param returnCost Whether to return the scalar cost value produced by\n     * executing `f()`.\n     * @param varList An optional list of variables to update. If specified, only\n     * the trainable variables in varList will be updated by minimize. Defaults to\n     * all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n    minimize(f, returnCost = false, varList) {\n        const { value, grads } = this.computeGradients(f, varList);\n        if (varList != null) {\n            const gradArray = varList.map(v => ({ name: v.name, tensor: grads[v.name] }));\n            this.applyGradients(gradArray);\n        }\n        else {\n            this.applyGradients(grads);\n        }\n        // Dispose gradients.\n        dispose(grads);\n        if (returnCost) {\n            return value;\n        }\n        else {\n            value.dispose();\n            return null;\n        }\n    }\n    /**\n     * The number of iterations that this optimizer instance has been invoked for.\n     */\n    get iterations() {\n        if (this.iterations_ == null) {\n            this.iterations_ = 0;\n        }\n        return this.iterations_;\n    }\n    incrementIterations() {\n        this.iterations_ = this.iterations + 1;\n    }\n    /**\n     * Executes f() and computes the gradient of the scalar output of f() with\n     * respect to the list of trainable variables provided by `varList`. If no\n     * list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to use for computing\n     * gradients with respect to variables.\n     * @param varList An optional list of variables to compute gradients with\n     * respect to. If specified, only the trainable variables in varList will have\n     * gradients computed with respect to. Defaults to all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n    computeGradients(f, varList) {\n        return variableGrads(f, varList);\n    }\n    /**\n     * Dispose the variables (if any) owned by this optimizer instance.\n     */\n    dispose() {\n        if (this.iterations_ != null) {\n            dispose(this.iterations_);\n        }\n    }\n    async saveIterations() {\n        if (this.iterations_ == null) {\n            this.iterations_ = 0;\n        }\n        return {\n            name: 'iter',\n            // TODO(cais): Use 'int64' type when available.\n            tensor: scalar(this.iterations_, 'int32')\n        };\n    }\n    async getWeights() {\n        throw new Error('getWeights() is not implemented for this optimizer yet.');\n    }\n    async setWeights(weightValues) {\n        throw new Error(`setWeights() is not implemented for this optimizer class ` +\n            `${this.getClassName()}`);\n    }\n    /**\n     * Extract the first element of the weight values and set it\n     * as the iterations counter variable of this instance of optimizer.\n     *\n     * @param weightValues\n     * @returns Weight values with the first element consumed and excluded.\n     */\n    async extractIterations(weightValues) {\n        this.iterations_ = (await weightValues[0].tensor.data())[0];\n        return weightValues.slice(1);\n    }\n}\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n    value: (instance) => {\n        return instance.minimize != null && instance.computeGradients != null &&\n            instance.applyGradients != null;\n    }\n});\n//# sourceMappingURL=optimizer.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Tile } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\n/**\n * Construct a tensor by repeating it the number of times given by reps.\n *\n * This operation creates a new tensor by replicating `input` `reps`\n * times. The output tensor's i'th dimension has `input.shape[i] *\n * reps[i]` elements, and the values of `input` are replicated\n * `reps[i]` times along the i'th dimension. For example, tiling\n * `[a, b, c, d]` by `[2]` produces `[a, b, c, d, a, b, c, d]`.\n *\n * ```js\n * const a = tf.tensor1d([1, 2]);\n *\n * a.tile([2]).print();    // or a.tile([2])\n * ```\n *\n * ```js\n * const a = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * a.tile([1, 2]).print();  // or a.tile([1, 2])\n * ```\n * @param x The tensor to tile.\n * @param reps Determines the number of replications per dimension.\n *\n * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}\n */\nfunction tile_(x, reps) {\n    const $x = convertToTensor(x, 'x', 'tile', 'string_or_numeric');\n    util.assert($x.rank === reps.length, () => `Error in transpose: rank of input ${$x.rank} ` +\n        `must match length of reps ${reps}.`);\n    const inputs = { x: $x };\n    const attrs = { reps };\n    return ENGINE.runKernel(Tile, inputs, attrs);\n}\nexport const tile = op({ tile_ });\n//# sourceMappingURL=tile.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\n/**\n * Creates a new variable with the provided initial value.\n * ```js\n * const x = tf.variable(tf.tensor([1, 2, 3]));\n * x.assign(tf.tensor([4, 5, 6]));\n *\n * x.print();\n * ```\n *\n * @param initialValue Initial value for the tensor.\n * @param trainable If true, optimizers are allowed to update it.\n * @param name Name of the variable. Defaults to a unique id.\n * @param dtype If set, initialValue will be converted to the given type.\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nexport function variable(initialValue, trainable = true, name, dtype) {\n    return ENGINE.makeVariable(initialValue, trainable, name, dtype);\n}\n//# sourceMappingURL=variable.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Unpack } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\n/**\n * Unstacks a `tf.Tensor` of rank-`R` into a list of rank-`(R-1)` `tf.Tensor`s.\n *\n * ```js\n * const a = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * tf.unstack(a).forEach(tensor => tensor.print());\n * ```\n *\n * @param x A tensor object.\n * @param axis The axis to unstack along. Defaults to 0 (the first dim).\n *\n * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}\n */\nfunction unstack_(x, axis = 0) {\n    const $x = convertToTensor(x, 'x', 'unstack', 'string_or_numeric');\n    util.assert(axis >= -$x.shape.length && axis < $x.shape.length, () => `Axis = ${axis} is not in [-${$x.shape.length}, ${$x.shape.length})`);\n    const inputs = { value: $x };\n    const attrs = { axis };\n    return ENGINE.runKernel(Unpack, inputs, attrs);\n}\nexport const unstack = op({ unstack_ });\n//# sourceMappingURL=unstack.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Unique } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assert } from '../util';\nimport { op } from './operation';\n/**\n * Finds unique elements along an axis of a tensor.\n *\n * It returns a tensor `values` containing all of the unique elements along the\n * `axis` of the given tensor `x` in the same order that they occur along the\n * `axis` in `x`; `x` does not need to be sorted. It also returns a tensor\n * `indices` the same size as the number of the elements in `x` along the `axis`\n * dimension. It contains the index in the unique output `values`.\n *\n * ```js\n * // A 1-D tensor\n * const a = tf.tensor1d([1, 1, 2, 4, 4, 4, 7, 8, 8]);\n * const {values, indices} = tf.unique(a);\n * values.print();   // [1, 2, 4, 7, 8,]\n * indices.print();  // [0, 0, 1, 2, 2, 2, 3, 4, 4]\n * ```\n *\n * ```js\n * // A 2-D tensor with axis=0\n * //\n * // 'a' is: [[1, 0, 0],\n * //          [1, 0, 0],\n * //          [2, 0, 0]]\n * const a = tf.tensor2d([[1, 0, 0], [1, 0, 0], [2, 0, 0]]);\n * const {values, indices} = tf.unique(a, 0)\n * values.print();   // [[1, 0, 0],\n *                   //  [2, 0, 0]]\n * indices.print();  // [0, 0, 1]\n * ```\n *\n * ```js\n * // A 2-D tensor with axis=1\n * //\n * // 'a' is: [[1, 0, 0],\n * //          [1, 0, 0],\n * //          [2, 0, 0]]\n * const a = tf.tensor2d([[1, 0, 0], [1, 0, 0], [2, 0, 0]]);\n * const {values, indices} = tf.unique(a, 1)\n * values.print();   // [[1, 0],\n *                   //  [1, 0],\n *                   //  [2, 0]]\n * indices.print();  // [0, 1, 1]\n * ```\n * @param x A tensor (int32, string, bool).\n * @param axis The axis of the tensor to find the unique elements.\n * @returns [uniqueElements, indices] (see above for details)\n *\n * @doc {heading: 'Operations', subheading: 'Evaluation'}\n */\nfunction unique_(x, axis = 0) {\n    const $x = convertToTensor(x, 'x', 'unique', 'string_or_numeric');\n    assert($x.rank > 0, () => 'The input tensor must be at least 1D');\n    const inputs = { x: $x };\n    const attrs = { axis };\n    const [values, indices] = ENGINE.runKernel(Unique, inputs, attrs);\n    return { values, indices };\n}\nexport const unique = op({ unique_ });\n//# sourceMappingURL=unique.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { pow } from '../ops/pow';\nimport { scalar } from '../ops/scalar';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\nexport class AdamOptimizer extends Optimizer {\n    constructor(learningRate, beta1, beta2, epsilon = null) {\n        super();\n        this.learningRate = learningRate;\n        this.beta1 = beta1;\n        this.beta2 = beta2;\n        this.epsilon = epsilon;\n        this.accumulatedFirstMoment = [];\n        this.accumulatedSecondMoment = [];\n        tidy(() => {\n            // accB* will be updated by batch.\n            this.accBeta1 = scalar(beta1).variable();\n            this.accBeta2 = scalar(beta2).variable();\n        });\n        if (epsilon == null) {\n            this.epsilon = ENGINE.backend.epsilon();\n        }\n    }\n    applyGradients(variableGradients) {\n        const varNames = Array.isArray(variableGradients) ?\n            variableGradients.map(v => v.name) :\n            Object.keys(variableGradients);\n        tidy(() => {\n            const oneMinusAccBeta1 = sub(1, this.accBeta1);\n            const oneMinusAccBeta2 = sub(1, this.accBeta2);\n            varNames.forEach((name, i) => {\n                const value = ENGINE.registeredVariables[name];\n                const trainable = false;\n                if (this.accumulatedFirstMoment[i] == null) {\n                    this.accumulatedFirstMoment[i] = {\n                        originalName: `${name}/m`,\n                        variable: tidy(() => zerosLike(value).variable(trainable))\n                    };\n                }\n                if (this.accumulatedSecondMoment[i] == null) {\n                    this.accumulatedSecondMoment[i] = {\n                        originalName: `${name}/v`,\n                        variable: tidy(() => zerosLike(value).variable(trainable))\n                    };\n                }\n                const gradient = Array.isArray(variableGradients) ?\n                    variableGradients[i].tensor :\n                    variableGradients[name];\n                if (gradient == null) {\n                    return;\n                }\n                const firstMoment = this.accumulatedFirstMoment[i].variable;\n                const secondMoment = this.accumulatedSecondMoment[i].variable;\n                const newFirstMoment = add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));\n                const newSecondMoment = add(mul(secondMoment, this.beta2), mul(square(gradient), 1 - this.beta2));\n                const biasCorrectedFirstMoment = div(newFirstMoment, oneMinusAccBeta1);\n                const biasCorrectedSecondMoment = div(newSecondMoment, oneMinusAccBeta2);\n                firstMoment.assign(newFirstMoment);\n                secondMoment.assign(newSecondMoment);\n                const newValue = add(mul(div(biasCorrectedFirstMoment, add(sqrt(biasCorrectedSecondMoment), this.epsilon)), -this.learningRate), value);\n                value.assign(newValue);\n            });\n            this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n            this.accBeta2.assign(mul(this.accBeta2, this.beta2));\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        this.accBeta1.dispose();\n        this.accBeta2.dispose();\n        if (this.accumulatedFirstMoment != null) {\n            dispose(this.accumulatedFirstMoment.map(v => v.variable));\n        }\n        if (this.accumulatedSecondMoment != null) {\n            dispose(this.accumulatedSecondMoment.map(v => v.variable));\n        }\n    }\n    async getWeights() {\n        // Order matters for Python compatibility.\n        const variables = [...this.accumulatedFirstMoment, ...this.accumulatedSecondMoment];\n        return [await this.saveIterations()].concat(variables.map(v => ({ name: v.originalName, tensor: v.variable })));\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        tidy(() => {\n            this.accBeta1.assign(pow(this.beta1, this.iterations_ + 1));\n            this.accBeta2.assign(pow(this.beta2, this.iterations_ + 1));\n        });\n        const variableCount = weightValues.length / 2;\n        const trainable = false;\n        this.accumulatedFirstMoment =\n            weightValues.slice(0, variableCount).map(v => ({\n                originalName: v.name,\n                variable: v.tensor.variable(trainable)\n            }));\n        this.accumulatedSecondMoment =\n            weightValues.slice(variableCount, variableCount * 2)\n                .map(v => ({\n                originalName: v.name,\n                variable: v.tensor.variable(trainable)\n            }));\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'beta1': this.beta1,\n            'beta2': this.beta2,\n            'epsilon': this.epsilon,\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['beta1'], config['beta2'], config['epsilon']);\n    }\n}\n/** @nocollapse */\nAdamOptimizer.className = 'Adam'; // Note: Name matters for Python compatibility.\nregisterClass(AdamOptimizer);\n//# sourceMappingURL=adam_optimizer.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { AdadeltaOptimizer } from './adadelta_optimizer';\nimport { AdagradOptimizer } from './adagrad_optimizer';\nimport { AdamOptimizer } from './adam_optimizer';\nimport { AdamaxOptimizer } from './adamax_optimizer';\nimport { MomentumOptimizer } from './momentum_optimizer';\nimport { RMSPropOptimizer } from './rmsprop_optimizer';\nimport { SGDOptimizer } from './sgd_optimizer';\nexport class OptimizerConstructors {\n    /**\n     * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\n     *\n     * ```js\n     * // Fit a quadratic function by learning the coefficients a, b, c.\n     * const xs = tf.tensor1d([0, 1, 2, 3]);\n     * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\n     *\n     * const a = tf.scalar(Math.random()).variable();\n     * const b = tf.scalar(Math.random()).variable();\n     * const c = tf.scalar(Math.random()).variable();\n     *\n     * // y = a * x^2 + b * x + c.\n     * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\n     * const loss = (pred, label) => pred.sub(label).square().mean();\n     *\n     * const learningRate = 0.01;\n     * const optimizer = tf.train.sgd(learningRate);\n     *\n     * // Train the model.\n     * for (let i = 0; i < 10; i++) {\n     *   optimizer.minimize(() => loss(f(xs), ys));\n     * }\n     *\n     * // Make predictions.\n     * console.log(\n     *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\n     * const preds = f(xs).dataSync();\n     * preds.forEach((pred, i) => {\n     *   console.log(`x: ${i}, pred: ${pred}`);\n     * });\n     * ```\n     *\n     * @param learningRate The learning rate to use for the SGD algorithm.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static sgd(learningRate) {\n        return new SGDOptimizer(learningRate);\n    }\n    /**\n     * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\n     * descent.\n     *\n     * See\n     * [http://proceedings.mlr.press/v28/sutskever13.pdf](\n     * http://proceedings.mlr.press/v28/sutskever13.pdf)\n     *\n     * @param learningRate The learning rate to use for the Momentum gradient\n     * descent algorithm.\n     * @param momentum The momentum to use for the momentum gradient descent\n     * algorithm.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static momentum(learningRate, momentum, useNesterov = false) {\n        return new MomentumOptimizer(learningRate, momentum, useNesterov);\n    }\n    /**\n     * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\n     * descent. This implementation uses plain momentum and is not centered\n     * version of RMSProp.\n     *\n     * See\n     * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\n     * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     *\n     * @param learningRate The learning rate to use for the RMSProp gradient\n     * descent algorithm.\n     * @param decay The discounting factor for the history/coming gradient.\n     * @param momentum The momentum to use for the RMSProp gradient descent\n     * algorithm.\n     * @param epsilon Small value to avoid zero denominator.\n     * @param centered If true, gradients are normalized by the estimated\n     * variance of the gradient.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static rmsprop(learningRate, decay = .9, momentum = 0.0, epsilon = null, centered = false) {\n        return new RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n    }\n    /**\n     * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\n     * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n     *\n     * @param learningRate The learning rate to use for the Adam gradient\n     * descent algorithm.\n     * @param beta1 The exponential decay rate for the 1st moment estimates.\n     * @param beta2 The exponential decay rate for the 2nd moment estimates.\n     * @param epsilon A small constant for numerical stability.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adam(learningRate = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = null) {\n        return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\n    }\n    /**\n     * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\n     * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\n     *\n     * @param learningRate The learning rate to use for the Adadelta gradient\n     * descent algorithm.\n     * @param rho The learning rate decay over each update.\n     * @param epsilon A constant epsilon used to better condition the grad\n     * update.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adadelta(learningRate = .001, rho = .95, epsilon = null) {\n        return new AdadeltaOptimizer(learningRate, rho, epsilon);\n    }\n    /**\n     * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\n     * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n     *\n     * @param learningRate The learning rate to use for the Adamax gradient\n     * descent algorithm.\n     * @param beta1 The exponential decay rate for the 1st moment estimates.\n     * @param beta2 The exponential decay rate for the 2nd moment estimates.\n     * @param epsilon A small constant for numerical stability.\n     * @param decay The learning rate decay over each update.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adamax(learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon = null, decay = 0.0) {\n        return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n    }\n    /**\n     * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\n     * See\n     * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\n     * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     * or\n     * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\n     * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n     *\n     * @param learningRate The learning rate to use for the Adagrad gradient\n     * descent algorithm.\n     * @param initialAccumulatorValue Starting value for the accumulators, must be\n     * positive.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adagrad(learningRate, initialAccumulatorValue = 0.1) {\n        return new AdagradOptimizer(learningRate, initialAccumulatorValue);\n    }\n}\n//# sourceMappingURL=optimizer_constructors.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env } from '../environment';\n// We are wrapping this within an object so it can be stubbed by Jasmine.\nexport const getNodeFetch = {\n    // tslint:disable-next-line:no-require-imports\n    importFetch: () => require('node-fetch')\n};\nlet systemFetch;\n// These getters and setters are for testing so we don't export a mutable\n// variable.\nexport function resetSystemFetch() {\n    systemFetch = null;\n}\nexport function setSystemFetch(fetchFn) {\n    systemFetch = fetchFn;\n}\nexport function getSystemFetch() {\n    return systemFetch;\n}\nexport class PlatformNode {\n    constructor() {\n        // tslint:disable-next-line:no-require-imports\n        this.util = require('util');\n        // According to the spec, the built-in encoder can do only UTF-8 encoding.\n        // https://developer.mozilla.org/en-US/docs/Web/API/TextEncoder/TextEncoder\n        this.textEncoder = new this.util.TextEncoder();\n    }\n    fetch(path, requestInits) {\n        if (env().global.fetch != null) {\n            return env().global.fetch(path, requestInits);\n        }\n        if (systemFetch == null) {\n            systemFetch = getNodeFetch.importFetch();\n        }\n        return systemFetch(path, requestInits);\n    }\n    now() {\n        const time = process.hrtime();\n        return time[0] * 1000 + time[1] / 1000000;\n    }\n    encode(text, encoding) {\n        if (encoding !== 'utf-8' && encoding !== 'utf8') {\n            throw new Error(`Node built-in encoder only supports utf-8, but got ${encoding}`);\n        }\n        return this.textEncoder.encode(text);\n    }\n    decode(bytes, encoding) {\n        if (bytes.length === 0) {\n            return '';\n        }\n        return new this.util.TextDecoder(encoding).decode(bytes);\n    }\n}\nif (env().get('IS_NODE')) {\n    env().setPlatform('node', new PlatformNode());\n}\n//# sourceMappingURL=platform_node.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { fill } from '../ops/fill';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class AdagradOptimizer extends Optimizer {\n    constructor(learningRate, initialAccumulatorValue = 0.1) {\n        super();\n        this.learningRate = learningRate;\n        this.initialAccumulatorValue = initialAccumulatorValue;\n        this.accumulatedGrads = [];\n    }\n    applyGradients(variableGradients) {\n        const variableNames = Array.isArray(variableGradients) ?\n            variableGradients.map(item => item.name) :\n            Object.keys(variableGradients);\n        variableNames.forEach((name, i) => {\n            const value = ENGINE.registeredVariables[name];\n            if (this.accumulatedGrads[i] == null) {\n                const trainable = false;\n                this.accumulatedGrads[i] = {\n                    originalName: `${name}/accumulator`,\n                    variable: tidy(() => fill(value.shape, this.initialAccumulatorValue)\n                        .variable(trainable))\n                };\n            }\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            const accumulatedGrad = this.accumulatedGrads[i].variable;\n            tidy(() => {\n                const newAccumulatedGrad = add(accumulatedGrad, square(gradient));\n                accumulatedGrad.assign(newAccumulatedGrad);\n                const newValue = add(mul(div(gradient, sqrt(add(newAccumulatedGrad, ENGINE.backend.epsilon()))), -this.learningRate), value);\n                value.assign(newValue);\n            });\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        if (this.accumulatedGrads != null) {\n            dispose(this.accumulatedGrads.map(v => v.variable));\n        }\n    }\n    async getWeights() {\n        // Order matters for Python compatibility.\n        return [await this.saveIterations()].concat(this.accumulatedGrads.map(v => ({ name: v.originalName, tensor: v.variable })));\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        const trainable = false;\n        this.accumulatedGrads = weightValues.map(v => ({ originalName: v.name, variable: v.tensor.variable(trainable) }));\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'initialAccumulatorValue': this.initialAccumulatorValue,\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['initialAccumulatorValue']);\n    }\n}\n/** @nocollapse */\nAdagradOptimizer.className = 'Adagrad'; // Note: Name matters for Python compatibility.\nregisterClass(AdagradOptimizer);\n//# sourceMappingURL=adagrad_optimizer.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { UnsortedSegmentSum } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assert, isInt } from '../util';\nimport { op } from './operation';\n/**\n * Computes the sum along segments of a `tf.Tensor`.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3, 4]);\n * const segmentIds = tf.tensor1d([1, 2, 0, 1], 'int32');\n * const numSegments = 3;\n *\n * x.unsortedSegmentSum(segmentIds, numSegments).print()\n * //or tf.unsortedSegmentSum(x, segmentIds, numSegments)\n * ```\n * @param x The `tf.Tensor` that will be summed along its segments.\n * @param segmentIds A `tf.Tensor1D` whose rank is equal to the rank of `x`'s\n * dimension along the `axis`.  Maps each element of `x` to a segment.\n * @param numSegments The number of distinct `segmentIds`.\n *\n * @doc {heading: 'Operations', subheading: 'Segment'}\n */\nfunction unsortedSegmentSum_(x, segmentIds, numSegments) {\n    const $x = convertToTensor(x, 'x', 'unsortedSegmentSum');\n    const $segmentIds = convertToTensor(segmentIds, 'segmentIds', 'unsortedSegmentSum', 'int32');\n    assert(isInt(numSegments), () => 'numSegments must be of dtype int');\n    const inputs = { x: $x, segmentIds: $segmentIds };\n    const attrs = { numSegments };\n    return ENGINE.runKernel(UnsortedSegmentSum, inputs, attrs);\n}\nexport const unsortedSegmentSum = op({ unsortedSegmentSum_ });\n//# sourceMappingURL=unsorted_segment_sum.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { buffer } from './buffer';\nimport { op } from './operation';\nimport { MPRandGauss } from './rand_util';\n/**\n * Creates a `tf.Tensor` with values sampled from a truncated normal\n * distribution.\n *\n * ```js\n * tf.truncatedNormal([2, 2]).print();\n * ```\n *\n * The generated values follow a normal distribution with specified mean and\n * standard deviation, except that values whose magnitude is more than 2\n * standard deviations from the mean are dropped and re-picked.\n *\n * @param shape An array of integers defining the output tensor shape.\n * @param mean The mean of the normal distribution.\n * @param stdDev The standard deviation of the normal distribution.\n * @param dtype The data type of the output tensor.\n * @param seed The seed for the random number generator.\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nfunction truncatedNormal_(shape, mean = 0, stdDev = 1, dtype, seed) {\n    if (dtype != null && dtype === 'bool') {\n        throw new Error(`Unsupported data type $ { dtype }`);\n    }\n    const randGauss = new MPRandGauss(mean, stdDev, dtype, true /* truncated */, seed);\n    const res = buffer(shape, dtype);\n    for (let i = 0; i < res.values.length; i++) {\n        res.values[i] = randGauss.nextValue();\n    }\n    return res.toTensor();\n}\nexport const truncatedNormal = op({ truncatedNormal_ });\n//# sourceMappingURL=truncated_normal.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { abs } from '../ops/abs';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { maximum } from '../ops/maximum';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\nexport class AdamaxOptimizer extends Optimizer {\n    constructor(learningRate, beta1, beta2, epsilon = null, decay = 0.0) {\n        super();\n        this.learningRate = learningRate;\n        this.beta1 = beta1;\n        this.beta2 = beta2;\n        this.epsilon = epsilon;\n        this.decay = decay;\n        this.accumulatedFirstMoment = [];\n        this.accumulatedWeightedInfNorm = [];\n        tidy(() => {\n            this.iteration = scalar(0).variable();\n            this.accBeta1 = scalar(beta1).variable();\n        });\n        if (epsilon == null) {\n            this.epsilon = ENGINE.backend.epsilon();\n        }\n    }\n    applyGradients(variableGradients) {\n        const variableNames = Array.isArray(variableGradients) ?\n            variableGradients.map(item => item.name) :\n            Object.keys(variableGradients);\n        tidy(() => {\n            const oneMinusAccBeta1 = sub(1, this.accBeta1);\n            const lr = div(-this.learningRate, add(mul(this.iteration, this.decay), 1));\n            variableNames.forEach((name, i) => {\n                const value = ENGINE.registeredVariables[name];\n                const trainable = false;\n                if (this.accumulatedFirstMoment[i] == null) {\n                    this.accumulatedFirstMoment[i] = {\n                        originalName: `${name}/m`,\n                        variable: zerosLike(value).variable(trainable)\n                    };\n                }\n                if (this.accumulatedWeightedInfNorm[i] == null) {\n                    this.accumulatedWeightedInfNorm[i] = {\n                        originalName: `${name}/v`,\n                        variable: zerosLike(value).variable(trainable)\n                    };\n                }\n                const gradient = Array.isArray(variableGradients) ?\n                    variableGradients[i].tensor :\n                    variableGradients[name];\n                if (gradient == null) {\n                    return;\n                }\n                const firstMoment = this.accumulatedFirstMoment[i].variable;\n                const weightedInfNorm = this.accumulatedWeightedInfNorm[i].variable;\n                const newFirstMoment = add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));\n                const ut0 = mul(weightedInfNorm, this.beta2);\n                const ut1 = abs(gradient);\n                const newWeightedInfNorm = maximum(ut0, ut1);\n                firstMoment.assign(newFirstMoment);\n                weightedInfNorm.assign(newWeightedInfNorm);\n                const newValue = add(mul(div(lr, oneMinusAccBeta1), div(newFirstMoment, add(newWeightedInfNorm, this.epsilon))), value);\n                value.assign(newValue);\n            });\n            this.iteration.assign(add(this.iteration, 1));\n            this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        this.accBeta1.dispose();\n        this.iteration.dispose();\n        if (this.accumulatedFirstMoment != null) {\n            dispose(this.accumulatedFirstMoment.map(v => v.variable));\n        }\n        if (this.accumulatedWeightedInfNorm != null) {\n            dispose(this.accumulatedWeightedInfNorm.map(v => v.variable));\n        }\n    }\n    async getWeights() {\n        throw new Error('getWeights() is not implemented for Adamax yet.');\n    }\n    async setWeights(weightValues) {\n        throw new Error('setWeights() is not implemented for Adamax yet.');\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'beta1': this.beta1,\n            'beta2': this.beta2,\n            'epsilon': this.epsilon,\n            'decay': this.decay\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['beta1'], config['beta2'], config['epsilon'], config['decay']);\n    }\n}\n/** @nocollapse */\nAdamaxOptimizer.className = 'Adamax'; // Note: Name matters for Python compatbility.\nregisterClass(AdamaxOptimizer);\n//# sourceMappingURL=adamax_optimizer.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/ops';\nimport { square } from '../ops/square';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class AdadeltaOptimizer extends Optimizer {\n    constructor(learningRate, rho, epsilon = null) {\n        super();\n        this.learningRate = learningRate;\n        this.rho = rho;\n        this.epsilon = epsilon;\n        this.accumulatedGrads = [];\n        this.accumulatedUpdates = [];\n        if (epsilon == null) {\n            this.epsilon = ENGINE.backend.epsilon();\n        }\n    }\n    applyGradients(variableGradients) {\n        const variableNames = Array.isArray(variableGradients) ?\n            variableGradients.map(item => item.name) :\n            Object.keys(variableGradients);\n        variableNames.forEach((name, i) => {\n            const value = ENGINE.registeredVariables[name];\n            const trainable = false;\n            if (this.accumulatedGrads[i] == null) {\n                this.accumulatedGrads[i] = {\n                    originalName: `${name}/accum_grad`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            if (this.accumulatedUpdates[i] == null) {\n                this.accumulatedUpdates[i] = {\n                    originalName: `${name}/accum_var`,\n                    variable: tidy(() => zerosLike(value).variable(trainable))\n                };\n            }\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            const accumulatedGrad = this.accumulatedGrads[i].variable;\n            const accumulatedUpdate = this.accumulatedUpdates[i].variable;\n            tidy(() => {\n                const newAccumulatedGrad = add(mul(accumulatedGrad, this.rho), mul(square(gradient), 1 - this.rho));\n                const updates = mul(div(sqrt(add(accumulatedUpdate, this.epsilon)), sqrt(add(accumulatedGrad, this.epsilon))), gradient);\n                const newAccumulatedUpdate = add(mul(accumulatedUpdate, this.rho), mul(square(updates), 1 - this.rho));\n                accumulatedGrad.assign(newAccumulatedGrad);\n                accumulatedUpdate.assign(newAccumulatedUpdate);\n                const newValue = add(mul(updates, -this.learningRate), value);\n                value.assign(newValue);\n            });\n        });\n        this.incrementIterations();\n    }\n    dispose() {\n        if (this.accumulatedUpdates != null) {\n            dispose(this.accumulatedGrads.map(v => v.variable));\n            dispose(this.accumulatedUpdates.map(v => v.variable));\n        }\n    }\n    async getWeights() {\n        // Order matters for Python compatibility.\n        const variables = [...this.accumulatedGrads, ...this.accumulatedUpdates];\n        return [await this.saveIterations()].concat(variables.map(v => ({ name: v.originalName, tensor: v.variable })));\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        const variableCount = weightValues.length / 2;\n        const trainable = false;\n        this.accumulatedGrads =\n            weightValues.slice(0, variableCount).map(v => ({\n                originalName: v.name,\n                variable: v.tensor.variable(trainable)\n            }));\n        this.accumulatedUpdates =\n            weightValues.slice(variableCount, variableCount * 2)\n                .map(v => ({\n                originalName: v.name,\n                variable: v.tensor.variable(trainable)\n            }));\n    }\n    getConfig() {\n        return {\n            'learningRate': this.learningRate,\n            'rho': this.rho,\n            'epsilon': this.epsilon\n        };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate'], config['rho'], config['epsilon']);\n    }\n}\n/** @nocollapse */\nAdadeltaOptimizer.className = 'Adadelta'; // Name matters for Python compatibility.\nregisterClass(AdadeltaOptimizer);\n//# sourceMappingURL=adadelta_optimizer.js.map"],"names":["MomentumOptimizer","S","constructor","learningRate","momentum","useNesterov","super","this","accumulations","m","applyGradients","variableGradients","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","originalName","variable","accumulation","gradient","tensor","newValue","newAccumulation","c","assign","incrementIterations","dispose","v","setMomentum","getWeights","saveIterations","concat","setWeights","weightValues","extractIterations","getConfig","fromConfig","cls","config","className","registerClass","transpose","op","transpose_","x","perm","$x","shape","s","reverse","rank","length","axis","clone","inputs","attrs","runKernel","where","where_","condition","a","b","$a","$b","$condition","broadcastShape","t","e","PlatformBrowser","fetch","path","init","now","performance","encode","text","encoding","Error","textEncoder","TextEncoder","decode","bytes","TextDecoder","get","setPlatform","registerManager","URL_SCHEME","err","topk","topk_","k","sorted","lastDim","values","indices","RMSPropOptimizer","decay","epsilon","centered","accumulatedMeanSquares","accumulatedMoments","accumulatedMeanGrads","backend","accumulatedMeanSquare","newAccumulatedMeanSquare","accumulatedMeanGrad","newAccumulatedMeanGrad","gradContribution","newAccumulatedMoments","variables","push","variableCount","slice","SGDOptimizer","setLearningRate","zeros","dtype","real","imag","makeTensor","whereAsync","async","vals","data","res","zerosLike","zerosLike_","Optimizer","Serializable","minimize","f","returnCost","varList","grads","computeGradients","gradArray","iterations","iterations_","getClassName","defineProperty","Symbol","hasInstance","instance","tile","tile_","reps","initialValue","makeVariable","unstack","unstack_","unique","unique_","AdamOptimizer","beta1","beta2","accumulatedFirstMoment","accumulatedSecondMoment","accBeta1","accBeta2","varNames","oneMinusAccBeta1","oneMinusAccBeta2","firstMoment","secondMoment","newFirstMoment","newSecondMoment","biasCorrectedFirstMoment","biasCorrectedSecondMoment","OptimizerConstructors","sgd","rmsprop","adam","adadelta","rho","adamax","adagrad","initialAccumulatorValue","getNodeFetch","systemFetch","PlatformNode","util","requestInits","global","time","process","hrtime","AdagradOptimizer","accumulatedGrads","accumulatedGrad","newAccumulatedGrad","unsortedSegmentSum","unsortedSegmentSum_","segmentIds","numSegments","$segmentIds","truncatedNormal","truncatedNormal_","mean","stdDev","seed","randGauss","nextValue","toTensor","AdamaxOptimizer","accumulatedWeightedInfNorm","iteration","variableNames","lr","weightedInfNorm","ut0","ut1","newWeightedInfNorm","AdadeltaOptimizer","accumulatedUpdates","accumulatedUpdate","updates","newAccumulatedUpdate"],"sourceRoot":""}