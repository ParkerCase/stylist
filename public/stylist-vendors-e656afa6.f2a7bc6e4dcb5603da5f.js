"use strict";(self.webpackChunkStylistWidget=self.webpackChunkStylistWidget||[]).push([[1733],{2956:function(e,t,n){n.r(t),n.d(t,{conv2d:function(){return k},depthwiseConv2d:function(){return $},matMul:function(){return E}});var r=n(67897),a=n(31830),s=n(15441),o=n(30565),i=n(28189),u=n(45119),h=n(37523),l=n(62198),p=n(28794),d=n(25248),c=n(80252),f=n(47195),v=n(68646),g=n(70929),m=n(62302);const k=(0,g.op)({fusedConv2d_:function({x:e,filter:t,strides:n,pad:g,dataFormat:k="NHWC",dilations:b=[1,1],dimRoundingMode:w,bias:A,activation:$="linear",preluActivationWeights:T,leakyreluAlpha:E}){if($=$||"linear",!1===(0,v.zE)(r.T2.state.gradientDepth,$)){u.vA("NHWC"===k,(()=>`Error in fused conv2d: got dataFormat of ${k} but only NHWC is currently supported for the case of gradient depth is 0 and the activation is not linear.`));let r=(0,p.X)(e,t,n,g,k,b,w);return null!=A&&(r=(0,h.W)(r,A)),(0,v.f2)(r,$,T,E)}const C=(0,i.YT)(e,"x","conv2d","float32"),y=(0,i.YT)(t,"filter","conv2d","float32");let D=C,N=!1;3===C.rank&&(N=!0,D=(0,m.t)(C,[1,C.shape[0],C.shape[1],C.shape[2]])),u.vA(4===D.rank,(()=>`Error in fused conv2d: input must be rank 4, but got rank ${D.rank}.`)),u.vA(4===y.rank,(()=>`Error in fused conv2d: filter must be rank 4, but got rank ${y.rank}.`)),f.s_("fused conv2d",g,w);const W="NHWC"===k?D.shape[3]:D.shape[1];u.vA(y.shape[2]===W,(()=>`Error in conv2d: depth of input (${W}) must match input depth for filter ${y.shape[2]}.`)),u.vA(f.G0(n,b),(()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${b}'`));const M=f.uf(D.shape,y.shape,n,b,g,w);let x,Y;if(null!=A&&(x=(0,i.YT)(A,"bias","fused conv2d"),[x]=(0,o.makeTypesMatch)(x,C),"NHWC"===k?l.assertAndGetBroadcastShape(M.outShape,x.shape):(u.vA(x.shape.length<=1,(()=>`Error in fused conv2d: only supports scalar or 1-D Tensor bias for NCHW format but got the bias of rank-${x.shape.length}.`)),u.vA(0===x.shape.length||x.shape[0]===M.outChannels||1===x.shape[0],(()=>`Error in fused conv2d: bias shape (${x.shape}) is not compatible with the number of output channels (${M.outChannels})`)))),null!=T){const e=T.shape;if(u.vA(e.length<=1||3===e.length,(()=>`Error in fused conv2d: only supports scalar, 1-D Tensor or 3-D Tensor PReLU activation weights but got a tensor of rank-${e.length}.`)),1===e.length)u.vA(1===e[0]||e[0]===M.outChannels,(()=>`Error in fused conv2d: PReLU activation weights (${e}) is not compatible with the number of output channels (${M.outChannels}).`));else if(3===e.length)try{l.assertAndGetBroadcastShape(e,M.outShape)}catch(F){const t=`Error in fused conv2d: PReLU activation weights (${e}) is not compatible with the output shape of the conv2d (${M.outShape}).`;throw Error(t)}Y=(0,i.YT)(T,"prelu weights","fused conv2d")}const _=(e,t)=>{u.vA("NHWC"===k,(()=>`Error in gradient of fused conv2D: got dataFormat of ${k} but only NHWC is currently supported.`));const[r,a,s,o]=t,i=(0,v.XB)(e,s,$);u.vA(f.Dh(b),(()=>`Error in gradient of fused conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${b}'`));const h=[(0,c.v)(a.shape,i,r,n,g),(0,d.H)(a,i,r.shape,n,g)];if(null!=o){const e=(0,v.Do)(o,i);h.push(e)}return h},G={x:D,filter:y,bias:x,preluActivationWeights:Y},B={strides:n,pad:g,dataFormat:k,dilations:b,dimRoundingMode:w,activation:$,leakyreluAlpha:E};if(null==A){const e=(0,a._X)(((e,t,n)=>{let a=r.T2.runKernel(s.aAr,G,B);return n([t,e,a]),N&&(a=(0,m.t)(a,[a.shape[1],a.shape[2],a.shape[3]])),{value:a,gradFunc:_}}));return e(D,y)}{const e=(0,a._X)(((e,t,n,a)=>{let o=r.T2.runKernel(s.aAr,G,B);return a([t,e,o,n]),N&&(o=(0,m.t)(o,[o.shape[1],o.shape[2],o.shape[3]])),{value:o,gradFunc:_}}));return e(D,y,x)}}});var b=n(10676),w=n(41890),A=n(83134);const $=(0,g.op)({fusedDepthwiseConv2d_:function({x:e,filter:t,strides:n,pad:p,dataFormat:d="NHWC",dilations:c=[1,1],dimRoundingMode:g,bias:k,activation:$="linear",preluActivationWeights:T,leakyreluAlpha:E}){if(!1===(0,v.zE)(r.T2.state.gradientDepth,$)){let r=(0,b.G)(e,t,n,p,d,c,g);return null!=k&&(r=(0,h.W)(r,k)),(0,v.f2)(r,$,T,E)}const C=(0,i.YT)(e,"x","depthwiseConv2d","float32"),y=(0,i.YT)(t,"filter","depthwiseConv2d","float32");let D=C,N=!1;3===C.rank&&(N=!0,D=(0,m.t)(C,[1,C.shape[0],C.shape[1],C.shape[2]])),u.vA(4===D.rank,(()=>`Error in fused depthwiseConv2d: input must be rank 4, but got rank ${D.rank}.`)),u.vA(4===y.rank,(()=>`Error in fused depthwiseConv2d: filter must be rank 4, but got rank ${y.rank}.`)),u.vA(D.shape[3]===y.shape[2],(()=>`Error in fused depthwiseConv2d: number of input channels (${D.shape[3]}) must match the inChannels dimension in filter ${y.shape[2]}.`)),null==c&&(c=[1,1]),u.vA(f.G0(n,c),(()=>`Error in fused depthwiseConv2d: Either strides or dilations must be 1. Got strides ${n} and dilations '${c}'`)),f.s_("fused depthwiseConv2d",p,g);const W=f.uf(D.shape,y.shape,n,c,p,g,!0);let M,x;null!=k&&(M=(0,i.YT)(k,"bias","fused conv2d"),[M]=(0,o.makeTypesMatch)(M,C),l.assertAndGetBroadcastShape(W.outShape,M.shape)),null!=T&&(x=(0,i.YT)(T,"prelu weights","fused depthwiseConv2d"));const Y=(e,t)=>{u.vA(f.Dh(c),(()=>`Error in gradient of fused depthwiseConv2d: dilation rates greater than 1 are not yet supported. Got dilations '${c}'`));const[r,a,s,o]=t,i=(0,v.XB)(e,s,$),h=(0,A.l)(a.shape,i,r,n,p,c,g),l=(0,w.x)(a,i,r.shape,n,p,c,g);if(null!=o){return[h,l,(0,v.Do)(M,i)]}return[h,l]},_={x:D,filter:y,bias:M,preluActivationWeights:x},G={strides:n,pad:p,dataFormat:d,dilations:c,dimRoundingMode:g,activation:$,leakyreluAlpha:E};if(null==k){const e=(0,a._X)(((e,t,n)=>{let a=r.T2.runKernel(s.T7M,_,G);return n([t,e,a]),N&&(a=(0,m.t)(a,[a.shape[1],a.shape[2],a.shape[3]])),{value:a,gradFunc:Y}}));return e(D,y)}{const e=(0,a._X)(((e,t,n,a)=>{let o=r.T2.runKernel(s.T7M,_,G);return a([t,e,o,n]),N&&(o=(0,m.t)(o,[o.shape[1],o.shape[2],o.shape[3]])),{value:o,gradFunc:Y}}));return e(D,y,M)}}});var T=n(65703);const E=(0,g.op)({fusedMatMul_:function({a:e,b:t,transposeA:n=!1,transposeB:p=!1,bias:d,activation:c="linear",preluActivationWeights:f,leakyreluAlpha:g=.2}){if(!1===(0,v.zE)(r.T2.state.gradientDepth,c)){let r=(0,T.N)(e,t,n,p);return null!=d&&(r=(0,h.W)(r,d)),(0,v.f2)(r,c,f,g)}let k=(0,i.YT)(e,"a","fused matMul"),b=(0,i.YT)(t,"b","fused matMul");[k,b]=(0,o.makeTypesMatch)(k,b);const w=n?k.shape[k.rank-2]:k.shape[k.rank-1],A=p?b.shape[b.rank-1]:b.shape[b.rank-2],$=n?k.shape[k.rank-1]:k.shape[k.rank-2],E=p?b.shape[b.rank-2]:b.shape[b.rank-1],C=k.shape.slice(0,-2),y=b.shape.slice(0,-2),D=u.Ze(C),N=u.Ze(y);u.vA(w===A,(()=>`Error in fused matMul: inner shapes (${w}) and (${A}) of Tensors with shapes ${k.shape} and ${b.shape} and transposeA=${n} and transposeB=${p} must match.`));const W=l.assertAndGetBroadcastShape(k.shape.slice(0,-2),b.shape.slice(0,-2)).concat([$,E]),M=n?(0,m.t)(k,[D,w,$]):(0,m.t)(k,[D,$,w]),x=p?(0,m.t)(b,[N,E,A]):(0,m.t)(b,[N,A,E]);let Y,_;null!=d&&(Y=(0,i.YT)(d,"bias","fused matMul"),[Y]=(0,o.makeTypesMatch)(Y,k),l.assertAndGetBroadcastShape(W,Y.shape)),null!=f&&(_=(0,i.YT)(f,"prelu weights","fused matMul"));const G=(e,t)=>{const[r,a,s,o]=t,i=(0,v.XB)((0,m.t)(e,s.shape),s,c);let u,h;if(n||p?!n&&p?(u=(0,T.N)(i,a,!1,!1),h=(0,T.N)(i,r,!0,!1)):n&&!p?(u=(0,T.N)(a,i,!1,!0),h=(0,T.N)(r,i,!1,!1)):(u=(0,T.N)(a,i,!0,!0),h=(0,T.N)(i,r,!0,!0)):(u=(0,T.N)(i,a,!1,!0),h=(0,T.N)(r,i,!0,!1)),null!=d){return[u,h,(0,v.Do)(o,i)]}return[u,h]},B={a:M,b:x,bias:Y,preluActivationWeights:_},F={transposeA:n,transposeB:p,activation:c,leakyreluAlpha:g};if(null==d){const e=(0,a._X)(((e,t,n)=>{const a=r.T2.runKernel(s.Dr,B,F);return n([e,t,a]),{value:(0,m.t)(a,W),gradFunc:G}}));return e(M,x)}{const e=(0,a._X)(((e,t,n,a)=>{const o=r.T2.runKernel(s.Dr,B,F);return a([e,t,o,n]),{value:(0,m.t)(o,W),gradFunc:G}}));return e(M,x,Y)}}})},48229:function(e,t,n){n.d(t,{k:function(){return o}});var r=n(67897),a=n(15441),s=n(28189);const o=(0,n(70929).op)({gather_:function(e,t,n=0,o=0){const i={x:(0,s.YT)(e,"x","gather"),indices:(0,s.YT)(t,"indices","gather","int32")},u={axis:n,batchDims:o};return r.T2.runKernel(a.mxL,i,u)}})},55598:function(e,t,n){n.d(t,{S:function(){return o}});var r=n(67897),a=n(15441),s=n(28189);const o=(0,n(70929).op)({gatherND_:function(e,t){const n=(0,s.YT)(t,"indices","gatherND","int32"),o={params:(0,s.YT)(e,"x","gatherND","string_or_numeric"),indices:n};return r.T2.runKernel(a.O4G,o)}})},68646:function(e,t,n){n.d(t,{Do:function(){return v},XB:function(){return f},f2:function(){return g},zE:function(){return m}});var r=n(62198),a=n(83416),s=n(66919),o=n(9258),i=n(64394),u=n(90112),h=n(83732),l=n(62302),p=n(28968),d=n(10700),c=n(83791);function f(e,t,n){if(null==n||"linear"===n)return e;if("relu"===n)return(0,o.l)(e,(0,d.P)(t));throw new Error(`Cannot compute gradient for fused activation ${n}.`)}function v(e,t){let n=t;const a=r.getReductionAxes(e.shape,t.shape);return a.length>0&&(n=(0,c.c)(n,a)),(0,l.t)(n,e.shape)}function g(e,t,n,r){if("linear"===t)return e;if("relu"===t)return(0,u.V)(e);if("elu"===t)return(0,a.P)(e);if("relu6"===t)return(0,h.j)(e);if("prelu"===t)return(0,i.N)(e,n);if("leakyrelu"===t)return(0,s.H)(e,r);if("sigmoid"===t)return(0,p.r)(e);throw new Error(`Unknown fused activation ${t}.`)}const m=(e,t)=>!(e>0)||"linear"===t},83869:function(e,t,n){n.r(t),n.d(t,{prepareAndValidate:function(){return a}});var r=n(45119);function a(e,t){const n=e.shape.length,a=t.shape.length;if(n<1)throw new Error(`tf.gatherND() expects the input to be rank 1 or higher, but the rank was ${n}.`);if(a<1)throw new Error(`tf.gatherND() expects the indices to be rank 1 or higher, but the rank was ${a}.`);if("int32"!==t.dtype)throw new Error(`tf.gatherND() expects the indices to be int32 type, but the dtype was ${t.dtype}.`);if(t.shape[a-1]>n)throw new Error(`index innermost dimension length must be <= tensor rank; saw: ${t.shape[a-1]} vs. ${n}`);if(0===(0,r.Ze)(e.shape))throw new Error(`Requested more than 0 entries, but input is empty. Input shape: ${e.shape}.`);const s=t.shape,o=s[s.length-1];let i=1;for(let r=0;r<s.length-1;++r)i*=s[r];const u=e.shape,h=s.slice();h.pop();let l=1;for(let r=o;r<n;++r)l*=u[r],h.push(u[r]);const p=[...(0,r.Ur)(e.shape).map((e=>e/l)),1].slice(0,o);return[h,i,l,p]}}}]);
//# sourceMappingURL=stylist-vendors-e656afa6.f2a7bc6e4dcb5603da5f.js.map