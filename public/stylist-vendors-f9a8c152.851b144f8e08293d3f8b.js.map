{"version":3,"file":"stylist-vendors-f9a8c152.851b144f8e08293d3f8b.js","mappings":"wOAwCO,MAAMA,EAMT,WAAAC,CAAYC,GAIR,GAHAC,KAAKC,SAAW,CAAC,EACjBD,KAAKE,QAAU,CAAC,EAChBF,KAAKG,QAAU,CAAC,EACZJ,aAAiBF,EACjB,IAAK,MAAMO,KAAML,EAAME,SACnBD,KAAKC,SAASG,GAAML,EAAME,SAASG,GAC/BA,KAAML,EAAMG,UACZF,KAAKE,QAAQE,GAAML,EAAMG,QAAQE,QAIxC,CACD,GAAa,MAATL,EACA,OAEJ,IAAK,MAAMM,KAAQN,EACfC,KAAKM,IAAID,EAAKE,IAAKF,EAAKG,MAEhC,CACJ,CAWA,GAAAF,CAAIC,EAAKC,EAAOC,GACZ,GAA6B,MAAzBT,KAAKC,SAASM,EAAIH,IAQlB,MAAM,IAAI,KAAW,uBAAuBG,EAAIG,YAAYH,EAAIH,MAEpE,OATIJ,KAAKC,SAASM,EAAIH,IA3D9B,SAAiCG,EAAKI,GAElC,GAAiB,MAAbJ,EAAIK,OAAiBL,EAAIK,QAAUD,EAAIC,MAEvC,OAAOD,EAEX,IAEI,OAAO,IAAAE,MAAKF,EAAKJ,EAAIK,MACzB,CACA,MAAOE,GAEH,MAAM,IAAI,KAAW,0BAA0BH,EAAIC,mDAChCL,EAAIG,UAAUH,EAAIK,UACzC,CACJ,CA4CoCG,CAAwBR,EAAKC,GACrDR,KAAKG,QAAQI,EAAIG,MAAQH,EAAIH,GACjB,MAARK,IACAT,KAAKE,QAAQK,EAAIH,IAAMK,GAMxBT,IACX,CAMA,OAAAgB,CAAQX,GACJL,KAAKM,IAAID,EAAKE,IAAKF,EAAKG,MAC5B,CAKA,MAAAS,CAAOV,GACH,OAAgC,MAAzBP,KAAKC,SAASM,EAAIH,GAC7B,CAIA,KAAAc,GACI,OAAOC,OAAOC,KAAKpB,KAAKG,QAC5B,CAQA,QAAAkB,CAASd,GACL,GAAIA,aAAe,KAAgB,CAC/B,GAA6B,MAAzBP,KAAKC,SAASM,EAAIH,IAClB,MAAM,IAAI,KAAW,oBAAoBG,EAAIG,QAG7C,OAAOV,KAAKC,SAASM,EAAIH,GAEjC,CACK,CACD,MAAMA,EAAKJ,KAAKG,QAAQI,GACxB,GAAU,MAANH,EACA,MAAM,IAAI,KAAW,yCAAyCG,KAElE,OAAOP,KAAKC,SAASG,EACzB,CACJ,CAQA,OAAAkB,CAAQf,GACJ,GAAIA,aAAe,KAAgB,CAC/B,GAA6B,MAAzBP,KAAKC,SAASM,EAAIH,IAClB,MAAM,IAAI,KAAW,oBAAoBG,EAAIG,QAG7C,OAAOV,KAAKE,QAAQK,EAAIH,GAEhC,CACK,CACD,MAAMA,EAAKJ,KAAKG,QAAQI,GACxB,GAAU,MAANH,EACA,MAAM,IAAI,KAAW,yCAAyCG,KAElE,OAAOP,KAAKE,QAAQE,EACxB,CACJ,CAEA,YAAAmB,GACwB,MAAhBvB,KAAKE,UACL,IAAAsB,SAAQxB,KAAKE,QAErB,EAIJ,MAAMuB,EAAe,CAAC,EAEhBC,EAAwB,CAAC,EAsBxB,SAASC,EAAQC,EAASC,EAAUC,EAAQC,GAC/C,MAAMC,EAAqB,MAAVF,GAAyBA,EAAiB,SACrDG,EAAeC,MAAMC,QAAQP,GAC7BQ,EAAaH,EAAeL,EAAU,CAACA,GACvCS,EAAcD,EAAWE,KAAIC,GAAKA,EAAE7B,OACpC8B,EAAe,GACfC,EAAYZ,EAASX,QAC3B,IAAK,MAAMwB,KAAcL,GACkB,IAAnCI,EAAUE,QAAQD,GAClBF,EAAaI,KAAKf,EAASR,SAASqB,IAGpCF,EAAaI,KAAK,MAGb,MAATb,IAEAA,EAAMc,eAAiBC,IACvBf,EAAMgB,cAAgBD,KAG1B,MAAME,EAAkBX,EAAYY,KAAK,KAAO,IAAMpB,EAASX,QAAQ+B,KAAK,KAC5E,IAAIC,EACAC,EACJ,GAAqC,MAAjC1B,EAAauB,GAA0B,CAGvC,MAAMI,EA+Fd,SAA8CxB,EAASC,GACnD,EAAAwB,KAAA,OAAuB,MAAXzB,GAAmBA,EAAQ0B,OAAS,GAAG,IAAM,0CACzD,IAAIC,EAAc,GACdC,EAAoB,CAAC,EACzB,GAAuB,IAAnB5B,EAAQ0B,OAAc,CAEtB,MAAMF,EAAMK,EAAgD7B,EAAQ,GAAIC,GACxE0B,EAAcH,EAAIF,OAClBM,EAAoBJ,EAAIM,YAC5B,KACK,CACD,MAAMC,EAAU,IAAIC,IACpB,IAAK,MAAMC,KAASjC,EAAS,CACzB,MAAM,OAAEsB,EAAM,aAAEQ,GAAiBD,EAAgDI,EAAOhC,GAExF,IAAK,MAAMiC,KAAkBZ,EACpBS,EAAQI,IAAID,EAAepD,QAC5B6C,EAAYX,KAAKkB,GACjBH,EAAQrD,IAAIwD,EAAepD,OAInC,IAAK,MAAMA,KAAQgD,EACgB,MAA3BF,EAAkB9C,KAClB8C,EAAkB9C,GAAQ,IAAIkD,KAElCF,EAAahD,GAAMsD,SAAQC,GAAaT,EAAkB9C,GAAMJ,IAAI2D,IAE5E,CACJ,CACA,MAAO,CACHf,OAAQK,EACRJ,gBAAiBe,EAAoBV,GAE7C,CAjIoBW,CAAqC/B,EAAYP,GAC7DqB,EAASE,EAAIF,OACbC,EAAkBC,EAAID,gBAEtB1B,EAAauB,GAAmBE,EAChCxB,EAAsBsB,GAAmBG,CAC7C,CACAD,EAASzB,EAAauB,GACtBG,EAAkB,CAAC,EACdnB,GACDb,OAAOiD,OAAOjB,EAAiBzB,EAAsBsB,IAEzD,MAAMqB,EAAmB,IAAIxE,EAASgC,GAEtC,IAAK,IAAIyC,EAAI,EAAGA,EAAIpB,EAAOI,SAAUgB,EAAG,CACpC,GAAa,MAATvC,EAAe,CAEf,MAAMwC,GAAa,IAAAC,UAASD,WACxBA,EAAaxC,EAAMc,gBACnBd,EAAMc,cAAgB0B,GAEtBA,EAAaxC,EAAMgB,gBACnBhB,EAAMgB,cAAgBwB,EAE9B,CACA,MAAME,EAAWvB,EAAOoB,GAClBI,EAAWD,EAASE,YAC1B,GAAID,aAAoB,IACpB,SAEJ,MAAME,EAAc,GACdC,EAAa,GACbC,EAAmB,GACzB,IAAIC,GAAa,EACjB,IAAK,MAAMC,KAASP,EAASQ,OAAQ,CACjC,MAAMzE,EAAQ6D,EAAiBhD,SAAS2D,GAClCvE,EAAO4D,EAAiB/C,QAAQ0D,GACtCJ,EAAYhC,KAAKpC,GACjBqE,EAAWjC,KAAKnC,GACJ,MAARA,IACAsE,GAAa,GAEZ/C,IACDmB,EAAgB6B,EAAMtE,QACc,IAAhCyC,EAAgB6B,EAAMtE,OAAgBmB,EAASZ,OAAO+D,KACjB,IAArC3C,EAAYM,QAAQqC,EAAMtE,OAAiBF,EAAM0E,aAClB,IAA/BF,EAAML,YAAYQ,UAClBL,EAAiBlC,KAAKpC,GAGlC,CACIuE,KACAjD,EAASA,GAAU,CAAC,GACP,KAAI+C,EAAW,IAEhC,MAAMO,GAAgB,QAAOV,EAASW,MAAMT,EAAa9C,IACzD,IAAIwD,EAAa,KACbZ,EAASa,kBACTD,EAAaZ,EAASc,YAAYZ,EAAaC,IAEnD,MAAMY,EAAeC,EAAejB,GAC9BkB,EAAwBzD,MAAMC,QAAQsD,GAAgBA,EAAe,CAACA,GAC5E,IAAK,IAAInB,EAAI,EAAGA,EAAIqB,EAAsBrC,SAAUgB,EAAG,CAC9CD,EAAiBpD,OAAO0E,EAAsBrB,KAC/CD,EAAiB/D,IAAIqF,EAAsBrB,GAAIc,EAAcd,GAAIpC,MAAMC,QAAQmD,GAAcA,EAAW,GAAKA,GAEjH,MAAMM,EAAQvD,EAAYM,QAAQgD,EAAsBrB,GAAG5D,OAC5C,IAAXkF,IACApD,EAAaoD,GAASR,EAAcd,GAE5C,CACKtC,IAED,IAAAR,SAAQsD,EAEhB,CAOA,OADAT,EAAiB9C,eACVU,EAAeO,EAAeA,EAAa,EACtD,CA+CA,SAAS0B,EAAoBR,GACzB,MAAMP,EAAkB,CAAC,EACzB,IAAK,MAAMzC,KAAQgD,EACfP,EAAgBzC,GAAQgD,EAAahD,GAAMmF,KAE/C,OAAO1C,CACX,CAYO,SAASM,EAAgDI,EAAOhC,GACnE,MAAM8B,EAAU,IAAIC,IACdV,EAAS,GACTQ,EAAe,CAAC,EAItB,IAAK,MAAMnD,KAAOsB,EAASX,QACvByC,EAAQrD,IAAIC,GAEhB,MAAMuF,EAAQ,GACRC,EAAQ,GAGd,IADAD,EAAMlD,KAAKiB,GACJiC,EAAMxC,OAAS,GAAG,CACrB,MAAM0C,EAAMF,EAAMA,EAAMxC,OAAS,GACjC,GAAIK,EAAQI,IAAIiC,EAAItF,MAAO,CACvBoF,EAAMG,MACN,QACJ,CACA,MAAMC,EAAcH,EAAMA,EAAMzC,OAAS,KAAOwC,EAAMxC,OAAS,EAC/D,GAA0B,IAAtB0C,EAAIf,OAAO3B,QAAgB4C,EAE3BJ,EAAMG,MACN/C,EAAON,KAAKoD,GACZrC,EAAQrD,IAAI0F,EAAItF,MACZwF,GACAH,EAAME,UAGT,CAGDF,EAAMnD,KAAKkD,EAAMxC,OAAS,GAC1B,IAAK,MAAM0B,KAASgB,EAAIf,OAGY,MAA5BvB,EAAasB,EAAMtE,QACnBgD,EAAasB,EAAMtE,MAAQ,IAAIkD,KAEnCF,EAAasB,EAAMtE,MAAMJ,IAAI0F,EAAItF,MAC7BiD,EAAQI,IAAIiB,EAAMtE,OAGtBoF,EAAMlD,KAAKoC,EAEnB,CACJ,CACA,MAAO,CAAE9B,SAAQQ,eACrB,CAOA,SAASgC,EAAe7B,GACpB,IAAI4B,EACJ,GAA8C,IAA1C5B,EAAMc,YAAYwB,aAAa7C,OAC/BmC,EAAe5B,EAAMc,YAAYyB,WAEhC,CACD,IAAIC,EAAY,KAChB,IAAK,IAAI/B,EAAI,EAAGA,EAAIT,EAAMc,YAAYwB,aAAa7C,SAAUgB,EACzD,IAAK,MAAMgC,KAAgBzC,EAAMc,YAAYwB,aAAa7B,GACrDc,cACD,GAAIkB,EAAalG,KAAOyD,EAAMzD,GAAI,CAC9BiG,EAAY/B,EACZ,KACJ,CAGRmB,EAAe5B,EAAMc,YAAY4B,YAAYF,EACjD,CACA,OAAOZ,CACX,C,iQChbA,SAASe,EAAgCC,EAASpE,EAAaqE,GAC3D,MAAMC,EAAatE,EAAYiB,OAC/B,GAAe,MAAXmD,GAAoBvE,MAAMC,QAAQsE,IAA+B,IAAnBA,EAAQnD,OACtD,OAAOjB,EAAYC,KAAI5B,GAAQ,OAEnC,GAAmB,IAAfiG,EACA,OAAIzE,MAAMC,QAAQsE,IAA+B,IAAnBA,EAAQnD,OAC3BmD,EAEiB,kBAAZA,GAAwBpE,EAAY,KAAMoE,EAC/C,CAACA,EAAQpE,EAAY,KAGrB,CAACoE,GAGhB,GAAIvE,MAAMC,QAAQsE,GAAU,CACxB,GAAIA,EAAQnD,SAAWqD,EACnB,MAAM,IAAIC,MAAM,YAAYF,oBAA6BD,EAAQnD,wCAC5BqD,4EAGzC,OAAOF,CACX,CACK,GAAuB,kBAAZA,GAAwBtF,OAAOC,KAAKqF,GAASnD,OAAS,GAE9D,kBADGmD,EAAQtF,OAAOC,KAAKqF,GAAS,IACtB,CACd,MAAML,EAAS,GASf,OARA/D,EAAY2B,SAAQtB,IACZA,KAAc+D,EACdL,EAAOxD,KAAK6D,EAAQ/D,IAGpB0D,EAAOxD,KAAK,KAChB,IAEGwD,CACX,CAEI,MAAM,IAAIQ,MAAM,2BAA2BD,kBACjCD,kCACHC,gCAAyCtE,oBAChCqE,qBAA8BG,KAAKC,UAAUL,KAErE,CAcO,SAASM,EAAwBC,EAAa3E,GACjD,OAAOmE,EAAgCQ,EAAa3E,EAAa,cACrE,CAsBO4E,eAAeC,EAAmBC,EAAGC,EAAcJ,EAAaK,GACnE,GAAoB,MAAhBD,GAA4C,MAApBC,EAGxB,MAAM,IAAIT,MAAM,+CAEpB,GAAmB,MAAfI,EAAqB,CAErB,MAAMM,GAAW,IAAAC,OAAK,KAClB,GAAuB,IAAnBJ,EAAEK,MAAMlE,OAER,OAAO6D,EAAEM,QAER,GAAuB,IAAnBN,EAAEK,MAAMlE,OAAc,CAC3B,GAAI6D,EAAEK,MAAM,GAAK,EAAG,CAEhB,MAAME,EAAO,EACb,OAAOP,EAAEQ,OAAOD,EACpB,CACK,GAAmB,IAAfP,EAAEK,MAAM,GAEb,OAAOL,EAAES,QAAQ,CAACT,EAAEK,MAAM,KAG1B,MAAM,IAAIZ,MAAM,+CAA+CO,EAAEK,MAAM,yEAI/E,CAEI,MAAM,IAAIZ,MAAM,yCAAyCO,EAAEU,6EAE/D,IAEEC,EAAgB5F,MAAM6F,WAAWT,EAASU,SAChD,IAAAxG,SAAQ8F,GACR,MAAMW,EAAoB,GAW1B,OAVAH,EAAc9D,SAAQkE,IAClB,GAA+B,MAA3BlB,EAAYkB,GACZ,MAAM,IAAItB,MACN,wEAAasB,+CAIjBD,EAAkBrF,KAAKoE,EAAYkB,GACvC,KAEG,IAAAC,UAASF,EAAmB,UACvC,CAEI,OAAO,IAEf,CAQO,SAASG,EAAoBC,EAAQC,GACxC,OAAO,IAAAC,KAAIF,EAAQC,EACvB,CCvHA,SAASE,EAITC,EAAOC,GACH,IAAIC,EACAC,EACJ,MAAMC,EAAiBH,EACvBC,EAAKE,EAAmB,GACxBD,EAAKC,EAAmB,GACxB,cAAsB,MAANF,GAAoB,MAANC,GAAY,IAItC,mPAAGF,MACP,MAAMI,EAAcC,EAA0B,QAASN,EAAMO,WAAYL,GACnEM,EAAcF,EAA0B,SAAUN,EAAMpG,YAAauG,GACrEM,EAAYJ,EAAY,GAAGtB,MAAM,GACvC,cAAgBsB,EAAYxF,SAAWmF,EAAMxD,OAAO3B,QAAQ,IAAM,mBAAmBmF,EAAMxD,OAAO3B,2CAClFwF,EAAYxF,yCACrBuD,KAAKC,UAAU2B,EAAMO,iBAC5B,cAAgBC,EAAY3F,SAAWmF,EAAMU,QAAQ7F,QAAQ,IAAM,mBAAmBmF,EAAMU,QAAQ7F,4CACpF2F,EAAY3F,2CACrBuD,KAAKC,UAAU2B,EAAMpG,kBAC5B,IAAK,IAAI+G,EAAS,EAAGA,EAASN,EAAYxF,OAAQ8F,IAC9C,cAAgBN,EAAYM,GAAQ5B,MAAM,KAAO0B,GAAW,IACxD,8BAAGT,EAAMO,WAAWI,UAAeN,EAAYM,GAAQ5B,MAAM,iBAChD0B,oBAA4BT,EAAMO,WAAW,QAElE,IAAK,IAAIK,EAAS,EAAGA,EAASJ,EAAY3F,OAAQ+F,IAC9C,cAAgBJ,EAAYI,GAAQ7B,MAAM,KAAO0B,GAAW,IACxD,+BAAGT,EAAMpG,YAAYgH,UAAeJ,EAAYI,GAAQ7B,MAAM,iBACjD0B,oBAA4BT,EAAMO,WAAW,QAElE,MAAO,CAAEL,GAAIG,EAAaF,GAAIK,EAClC,CACA,SAASF,EAA0BO,EAAepI,EAAOqI,GACrD,GAAIA,aAAkB,SAClB,MAAO,CAACA,GAEP,GAAIrH,MAAMC,QAAQoH,GAEnB,OADA,cAAgBA,EAAOjG,SAAWpC,EAAMoC,QAAQ,IAAM,wBAAwBiG,EAAOjG,gCAAgCpC,EAAMoC,uBAAuBgG,UAAsBpI,OACjKqI,EAEN,CACD,MAAMC,EAAS,GAEf,IAAK,MAAM9I,KAAQQ,EAAO,CACtB,GAAoB,MAAhBqI,EAAO7I,GACP,MAAM,IAAI,KACN,gEAAG4I,UAAsB5I,OAEjC8I,EAAO5G,KAAK2G,EAAO7I,GACvB,CACA,OAAO8I,CACX,CACJ,CAOOvC,eAAewC,EAItBhB,EAAOiB,EAASC,GACZ,MAAMC,EAA6C,MAAxBD,EAAKE,gBAchC,GAbA,cAAmC,MAAnBpB,EAAMqB,WAAmB,IAAM,mGAE/C,cAAwB,MAARH,GAAc,IAAM,kGAEpC,cAA+B,MAAfA,EAAKI,QAAkBJ,EAAKI,OAAS,GAAKC,OAAOC,UAAUN,EAAKI,SAAS,IACrF,iFAAoBJ,EAAKI,WAC7B,eAAiBH,GACZD,EAAKE,gBAAkB,GAAKG,OAAOC,UAAUN,EAAKE,kBAAmB,IACtE,uGAA0CF,EAAKE,oBACnD,cAE2B,MAA3BF,EAAsB,iBAAW,IAAM,sFAEnClB,EAAMyB,WACN,MAAM,IAAItD,MAAM,gEAEpB6B,EAAMyB,YAAa,EACnB,IACI,MAAMC,EAAsC,MAAvBR,EAAKS,eAC1B,IAAIC,EACAC,EACJ,GAAIH,EACA,GAAII,EAAgBZ,EAAKS,gBACrB,cAA0C,MAA1BT,EAAKa,mBAChBb,EAAKa,kBAAoB,GACtBR,OAAOC,UAAUN,EAAKa,oBAAqB,IAG/C,iJAAWb,EAAKa,0BAEnB,CACD,MAAMJ,EA3CtB,SAAyCpC,GACrC,GAAoB,IAAhBA,EAAK1E,OACL,MAAM,IAAI,KAAoB,0DAElC,MAAO,CAAEqF,GAAIX,EAAK,GAAIY,GAAIZ,EAAK,GACnC,CAsCuCyC,CAAgCd,EAAKS,gBAC5DC,EAAQD,EAAezB,GACvB2B,EAAQF,EAAexB,EAC3B,CAEJ,MAAM8B,EAAgBjC,EAAMkC,oBACtBC,EAAYnC,EAAMoC,yBACxB,IAAIC,EAEAA,EADAX,EAEIS,EAAUG,QAAQC,OAAOJ,EAAUtI,KAAI2I,GAAK,OAASA,KAGvCL,EAAUG,QAEhC,MAAMG,GAAY,QAAqBvB,EAAKuB,UAAWvB,EAAKwB,YACtDC,EAA0B,MAAhBzB,EAAKyB,QAAkB,EAAIzB,EAAKyB,SAC1C,aAAEC,EAAY,QAAEC,IAAY,QAAmBJ,EAAWE,EAASzB,EAAKI,OAAQ,KAAM,KA2GpG,SAA0BL,EAASC,GAE/B,IAAI4B,EAAgB,KACQ,MAAxB5B,EAAKE,gBACL0B,EAAgB5B,EAAKE,gBAEhBG,OAAOwB,SAAS9B,EAAQ7D,QAC7B0F,EAAgB7B,EAAQ7D,MAE5B,OAAO0F,CACX,CArH0GE,CAAiB/B,EAASC,GAAO,KACnIQ,EAAcW,GACdO,EAAaK,SAASjD,GACtBA,EAAM6C,QAAUA,QACVD,EAAaM,eACnBlD,EAAMmD,eAAgB,EACtB,IAAIC,EAA6B,MAArBlC,EAAKmC,aAAuB,EAAInC,EAAKmC,aAC7CC,QAAqBrC,EAAQsC,WACjC,KAAOH,EAAQlC,EAAKI,QAAQ,CACxB,MAAMkC,EAAY,CAAC,QACbZ,EAAaa,aAAaL,GAChC,IAAIM,EAAY,EACZC,EAAa,EAIjB,IAHKxC,IACDmC,QAAqBrC,EAAQsC,aAE1BpC,GAAqBuC,EAAYxC,EAAKE,iBAAwB,CACjE,MAAMnB,QAAoBqD,EAAaM,OAGvC,GAAIzC,GAAsBlB,EAAY4D,KAWlC,MAEJ,GAAyB,MAArB5D,EAAYlI,MAAe,CAC3B,MAAM,GAAEmI,EAAE,GAAEC,GAAOJ,EAA8BC,EAAOC,EAAYlI,OAC9D+L,EAAY,CAAC,EACnBA,EAAiB,MAAIH,EACrBG,EAAgB,KAAI5D,EAAG,GAAGnB,MAAM,SAC1B6D,EAAamB,aAAaJ,EAAYG,GAC5C,MAAMjE,EAAgB,GACtB,GAAwB,MAApBqB,EAAK3C,YAAqB,CAC1B,MAAMyF,EAAuB1F,EAAwB4C,EAAK3C,YAAayB,EAAMpG,aAC7E,IAAK,IAAIiC,EAAI,EAAGA,EAAImI,EAAqBnJ,SAAUgB,EAC/CgE,EAAc1F,WAAWsE,EAAmB0B,EAAGtE,GAAI,KAAMmI,EAAqBnI,IAEtF,CAEA,MAAMoI,EAAM/D,EAAGqC,OAAOpC,GAAIoC,OAAO1C,GAC3BqE,EAAOjC,EAAcgC,GAC3B,UAAYA,GACZ,IAAK,IAAIpI,EAAI,EAAGA,EAAIsG,EAAUtH,SAAUgB,EAAG,CACvC,MAAMsI,EAAQhC,EAAUtG,GAClBlB,EAAMuJ,EAAKrI,GACjBiI,EAAUK,GAASxJ,EACnB,OAASA,EACb,OACMiI,EAAawB,WAAWT,EAAYG,IAC1C,OAAqBA,GACrBH,IACAD,GACJ,CACA,GAAIvC,EAAqBuC,GAAaxC,EAAKE,gBACvCnB,EAAY4D,KAAM,CAElB,GAAInC,EAAc,CACd,IAAI2C,EAEAA,EADAvC,EAAgBZ,EAAKS,iBACX,cAAa3B,EAAMsE,gBAAgBpD,EAAKS,eAAgB,CAAE4C,QAASrD,EAAKa,sBAGxE,QAAO/B,EAAMwE,SAAS5C,EAAOC,EAAO,CAC1CpB,UAAuC,MAA5BS,EAAKuD,oBA3MV,GA6MFvD,EAAKuD,oBACT9B,QAAS,KAGjB,IAAK,IAAI9G,EAAI,EAAGA,EAAImE,EAAM0E,aAAa7J,SAAUgB,EAC7C2H,EAAU,OAAOxD,EAAM0E,aAAa7I,MAAQwI,EAAQxI,EAE5D,CAMA,KACJ,CACA,GAAImE,EAAMmD,cACN,KAER,CAGA,SAFMP,EAAa+B,WAAWvB,EAAOI,GACrCJ,IACIpD,EAAMmD,cACN,KAER,CAGA,aAFMP,EAAagC,mBACb5E,EAAM6C,QAAQgC,WACb7E,EAAM6C,OACjB,CACA,QACI7C,EAAMyB,YAAa,CACvB,CACJ,CAeA,SAASK,EAAgBb,GACrB,MAAoC,oBAArBA,EAAQsC,QAC3B,CC/PO,SAASuB,EAAerE,GAC3B,cAAgBA,EAAY,GAAKc,OAAOC,UAAUf,IAAY,IAAM,2DAA2DA,KACnI,CAcO,SAASsE,EAAYC,EAAQC,EAAOC,GACvC,OAAc,MAAVF,EACO,CAAC,MAEHvL,MAAMC,QAAQsL,GACZA,EAAOnL,KAAIsL,IAAS,QAAoBA,EAAOF,EAAOC,EAAOD,MAG7D,QAAoBD,EAAQC,EAAOC,EAAOD,EAEzD,CAcO,SAASG,EAAqBJ,EAAQK,GACzC,OAAO,QAAS,IACE,MAAVL,EACO,KAEFvL,MAAMC,QAAQsL,GACZA,EAAOnL,KAAIsL,GAASC,EAAqBD,EAAOE,MAKhD,QAAOL,EAA0B,UAAlBK,EAAQlN,MAAoBkN,EAAUA,EAAQC,UAGhF,CASO,SAASC,EAAYnI,EAAMqD,GAC9B,MAAM9C,EAAS,GACf,IAAI6H,EAAa,EACbC,EAAW,KACf,KAAOD,EAAapI,GAChBqI,EAAWD,EAAa/E,EACpBgF,GAAYrI,IACZqI,EAAWrI,GAEfO,EAAOxD,KAAK,CAACqL,EAAYC,IACzBD,EAAaC,EAEjB,OAAO9H,CACX,CA8IOa,eAAekH,EAGtB1F,EAAO2F,EAAGjH,EAAGwC,EAAO,CAAC,GACjB,GAAIlB,EAAMyB,WACN,MAAM,IAAItD,MAAM,gEAGpB,IAAI3B,EACAoJ,EACAC,EACAC,EACAC,EACAC,EACAnG,EAPJG,EAAMyB,YAAa,EAQnB,IACI,MAAMhB,EAA8B,MAAlBS,EAAKT,UAAoB,GAAKS,EAAKT,UACrDqE,EAAerE,GAGf,MAAMwF,GAAiB,EACjBC,QAAyBlG,EAAMmG,oBAAoBR,EAAGjH,EAAGwC,EAAKvC,aAAcuC,EAAK3C,YAAa0H,EAAgBxF,GACpHjE,EAAS0J,EAAiB,GAC1BN,EAAUM,EAAiB,GAC3BrG,EAAgBqG,EAAiB,GAEjC,IACIE,EADA1E,GAAe,EAEnB,GAA2B,MAAvBR,EAAKS,gBAA0BT,EAAKS,eAAe9G,OAAS,EAAG,CAE/D,GADA6G,GAAe,EACoB,IAA/BR,EAAKS,eAAe9G,OAKnB,MAAmC,IAA/BqG,EAAKS,eAAe9G,OACnB,IAAI,KAAoB,iEAGxB,IAAI,KAEN,0GAAGqG,EAAKS,8BATZkE,EAAY3E,EAAKS,eAAe,GAChCmE,EAAY5E,EAAKS,eAAe,GAUpC,MAAMsE,GAAiB,EACjBI,QAAwBrG,EAAMmG,oBAAoBN,EAAWC,EAAW,KAAoC,KAAmCG,EAAgBxF,GACrKsF,EAAOM,EAAgB,GACvBL,EAAOK,EAAgB,GACvBD,EAASL,EAAKxD,OAAOyD,EAEzB,MACK,GAA4B,MAAxB9E,EAAKoF,iBAA2BpF,EAAKoF,gBAAkB,GAC5DpF,EAAKoF,gBAAkB,EAAG,CAC1B5E,GAAe,EAEf,MAAM6E,EAAUC,KAAKC,MAAMjK,EAAO,GAAGuC,MAAM,IAAM,EAAImC,EAAKoF,kBACpDI,EAAoBlK,EAAO,GAAGuC,MAAM,GAC1CgH,EAAOhB,EAAYvI,EAAQ+J,EAASG,GACpClK,EAASuI,EAAYvI,EAAQ,EAAG+J,GAChCP,EAAOjB,EAAYa,EAASW,EAASG,GACrCd,EAAUb,EAAYa,EAAS,EAAGW,GAGlCH,EAASL,EAAKxD,OAAOyD,EAEzB,MACiC,MAAxB9E,EAAKyF,kBACVjF,GAAe,GAGnB,MAAMuC,EAAMzH,EAAO+F,OAAOqD,GAASrD,OAAO1C,GAC1CG,EAAM4G,mCAYN,MAAM3E,EAAgBjC,EAAMkC,oBACtBC,EAAYnC,EAAMoC,yBACxB,IAAIyE,EACAxE,EACAX,GACA1B,EAAM8G,mBACND,EAAc7G,EAAM+G,aACpB1E,EACIF,EAAUG,QAAQC,OAAOJ,EAAUtI,KAAI2I,GAAK,OAASA,OAGzDqE,EAAc,KACdT,EAAS,GACT/D,EAAkBF,EAAUG,SAEhC,MAAMG,GAAY,QAAqBvB,EAAKuB,UAAWvB,EAAKwB,YACtD/H,QApNd6D,eAGAwB,EAAOgH,EAAG/C,EAAK9B,EAAW1B,EAAWa,EAAQqB,EAASF,EAAWwE,EAAMb,EAAQc,EAAS7E,EAAiBgB,EAAcP,EAAe6D,GACjH,MAAblG,IACAA,EAAY,IAEF,MAAVa,IACAA,EAAS,GAEE,MAAX4F,IACAA,GAAU,GAEM,MAAhB7D,IACAA,EAAe,GAGnB,IAAI3B,GAAe,EAKnB,GAJY,MAARuF,GAA0B,MAAVb,IAChB1E,GAAe,GAGI,MAAnBiF,IACAjF,GAAe,EACM,MAAjBoB,GACA,MAAM,IAAI,KAAW,oGAI7B,MAAMqE,EAAkBnH,EAAMoH,gBAAgBnD,EAAKxD,EAAWqC,EAAe,mBAC7E,IAAIuE,EACmB,MAAnBF,IACAE,GAAa,QAAM,EAAGF,IAEX,MAAXxE,IACAA,EAAU,GAEd,MAAM,aAAEC,EAAY,QAAEC,IAAY,QAAmBJ,EAAWE,EAASrB,EAAQ+B,EAAc8D,EAAiBrE,EAAerC,EAAWiB,EAAcW,GACxJO,EAAaK,SAASjD,GACtBA,EAAM6C,QAAUA,QACVD,EAAaM,eACnBlD,EAAMmD,eAAgB,EAGtB,IAAK,IAAIC,EAAQC,EAAcD,EAAQ9B,IAAU8B,EAAO,OAC9CR,EAAaa,aAAaL,GAChC,MAAMI,EAAY,CAAC,EACnB,GAAqB,MAAjBV,EACA,MAAM,IAAI,KAAoB,8CAE7B,CACD,GAAgB,UAAZoE,EACA,MAAM,IAAI,KAAoB,0CAEzBA,GACL,EAAAtM,KAAA,QAAayM,GAIjB,MAAMC,GAAoB,IAAA5H,UAAS2H,GAC7B9C,EAAUgB,EAAY4B,EAAiB1G,GAC7C,IAAK,IAAIkD,EAAa,EAAGA,EAAaY,EAAQ1J,SAAU8I,EAAY,CAChE,MAAMG,EAAY,CAAC,EAmCnB,SAlCMlB,EAAamB,aAAaJ,EAAYG,GAC5C,QAAS,KACL,MAAM0B,EAAajB,EAAQZ,GAAY,GACjC8B,EAAWlB,EAAQZ,GAAY,GAC/B4D,GAAW,QAAoBD,EAAmB9B,EAAYC,EAAWD,GAC/E1B,EAAiB,MAAIH,EACrBG,EAAgB,KAAI2B,EAAWD,EAG/B,MAAMgC,EAAWpC,EAAqBnB,EAAKsD,GACrCrD,EAAO8C,EAAEQ,GACf,IAAK,IAAI3L,EAAI,EAAGA,EAAIsG,EAAUtH,SAAUgB,EAAG,CACvC,MAAMsI,EAAQhC,EAAUtG,GAClBlB,EAAMuJ,EAAKrI,GACjBiI,EAAUK,GAASxJ,EACnB,OAASA,EAEb,CACA,GAAIgJ,IAAeY,EAAQ1J,OAAS,GAC5B6G,EAAc,CACd,MAAM2C,EAAUrE,EAAMyH,SAASR,EAAMb,EAAQ3F,GAE7C,IAAK,IAAI5E,EAAI,EAAGA,EAAIsG,EAAUtH,SAAUgB,EAAG,CACvC,MAAMsI,EAAQhC,EAAUtG,GAClBlB,EAAM0J,EAAQxI,GACpB,OAASlB,GAET6I,EAAU,OAASW,GAASxJ,CAChC,CACJ,CACJ,UAEEiI,EAAawB,WAAWT,EAAYG,IAC1C,OAAqBA,GACjB9D,EAAMmD,cACN,KAGR,CACAmE,EAAkBvO,SACtB,CAGA,SADM6J,EAAa+B,WAAWvB,EAAOI,GACjCxD,EAAMmD,cACN,KAER,CAGA,aAFMP,EAAagC,mBACb5E,EAAM6C,QAAQgC,WACb7E,EAAM6C,OACjB,CAmG0B6E,CAAQ1H,EAAOiC,EAAegC,EAAK9B,EAAW1B,EAAWS,EAAKI,OAAQJ,EAAKyB,QAASF,EAAWoE,EAAaT,EAAQlF,EAAKgG,QAAS7E,EAAiBnB,EAAKmC,aAAc,KAAM,MAC9L,OAAO1I,CACX,CACA,QACIqF,EAAMyB,YAAa,EAEnBkG,EAAkBnL,EAAQmJ,GAC1BgC,EAAkB/B,EAASlH,GAC3BiJ,EAAkB5B,EAAMF,GACxB8B,EAAkB3B,EAAMF,GACH,MAAjBjG,GACA,UAAYA,EAEpB,CAEJ,CAOO,SAAS+H,EAA2BC,GACvC,MAAM3D,EAAO,GACT2D,aAAmB,EAAAC,SACnBD,EAAU,CAACA,IAGf,IAAK,IAAIhM,EAAI,EAAGA,EAAIgM,EAAQhN,SAAUgB,EAAG,CACrC,MAAMkM,EAASF,EAAQhM,GACvB,GAAoB,IAAhBkM,EAAO3I,KACP8E,EAAK/J,MAAK,QAAW4N,EAAQ,QAE5B,IAAoB,IAAhBA,EAAO3I,KACZ,MAAM,IAAIjB,MAAM,yEAIhB+F,EAAK/J,KAAK4N,EACd,CACJ,CACA,OAAO7D,CACX,CAaO,SAASyD,EAAkBE,EAASG,GACvC,GAAe,MAAXH,EACA,OAEJ,MAAMI,EAAe,GACrB,GAAID,aAAsB,EAAAF,OACtBG,EAAa9N,KAAK6N,EAAWrQ,SAE5B,GAAI8B,MAAMC,QAAQsO,GACnBA,EAAWzM,SAAQzB,GAAKmO,EAAa9N,KAAKL,EAAEnC,WAE3C,GAAkB,MAAdqQ,EAEL,IAAK,MAAM/P,KAAQ+P,EAAY,CAC3B,MAAME,EAAYF,EAAW/P,GAC7BgQ,EAAa9N,KAAK+N,EAAUvQ,GAChC,CAEJ,MAAM0E,EAAmB,GACzB,GAAIwL,aAAmB,EAAAC,QACuB,IAAtCG,EAAa/N,QAAQ2N,EAAQlQ,KAC7B0E,EAAiBlC,KAAK0N,QAGzB,GAAIpO,MAAMC,QAAQmO,GACnBA,EAAQtM,SAAQzB,KACwB,IAAhCmO,EAAa/N,QAAQJ,EAAEnC,KACvB0E,EAAiBlC,KAAKL,EAC1B,SAGH,GAAe,MAAX+N,EAEL,IAAK,MAAM5P,KAAQ4P,EAAS,CACxB,MAAME,EAASF,EAAQ5P,IACkB,IAArCgQ,EAAa/N,QAAQ6N,EAAOpQ,KAC5B0E,EAAiBlC,KAAK4N,EAE9B,CAEJ1L,EAAiBd,SAAQzB,IAChBA,EAAE2C,YACH3C,EAAEf,SACN,GAER,CC5YO,SAASoP,EAAYxC,GACxB,OAAOlM,MAAMC,QAAQiM,EACzB,CAIO,SAASyC,EAAWzC,GACvB,OAbG,SAAsBA,GACzB,OAAOA,aAAa,EAAAmC,MACxB,CAWYO,CAAa1C,KAAOwC,EAAYxC,EAC5C,CAYO,SAAS2C,EAAqB/I,EAAM9G,EAAO8P,EAAQtC,GAAiB,EAAMuC,EAAkB,IAC/F,GAAa,MAAT/P,GAAkC,IAAjBA,EAAMoC,OAAc,CAGrC,GAAY,MAAR0E,EAAc,CACd,IAAIkJ,GAAoB,EACxB,GAAIN,EAAY5I,IAASA,EAAK1E,OAAS,EACnC4N,GAAoB,OAEnB,GAAIL,EAAW7I,IAChB,IAAK,MAAMzH,KAAOyH,EACd,GAAIA,EAAKmJ,eAAe5Q,GAAM,CAC1B2Q,GAAoB,EACpB,KACJ,OAKJA,GAAoB,EAExB,GAAIA,EACA,MAAM,IAAI,KAAW,6BAA6BD,+BACnCjJ,IAEvB,CACA,MAAO,EACX,CACA,GAAY,MAARA,EACA,OAAO9G,EAAMoB,KAAI5B,GAAQ,OAE7B,IAAI+M,EACJ,GAAIoD,EAAW7I,GAAO,CAElByF,EAAS,GACT,IAAK,MAAM/M,KAAQQ,EAAO,CACtB,GAAkB,MAAd8G,EAAKtH,GACL,MAAM,IAAI,KAAW,yBAAyBA,kCACvCQ,KAEXuM,EAAO7K,KAAKoF,EAAKtH,GACrB,CACJ,MACK,GAAIkQ,EAAY5I,GAAO,CAExB,GAAIA,EAAK1E,SAAWpC,EAAMoC,OACtB,MAAM,IAAI,KAAW,6BAA6B2N,kHAEX/P,EAAMoC,sEACO0E,KAExDyF,EAASzF,CACb,KACK,CAED,GAAI9G,EAAMoC,OAAS,EACf,MAAM,IAAI,KAAW,aAAa2N,aAA2B/P,EAAMoC,4EACL0E,EAAKR,SAEvEiG,EAAS,CAACzF,EACd,CAGA,GAFAyF,EAAS4C,EAA2B5C,GAEtB,MAAVuD,EACA,IAAK,IAAI1M,EAAI,EAAGA,EAAIpD,EAAMoC,SAAUgB,EAAG,CACnC,GAAiB,MAAb0M,EAAO1M,GACP,SAEJ,MAAMsJ,EAAQH,EAAOnJ,GACrB,GAAIsJ,EAAMpG,MAAMlE,SAAW0N,EAAO1M,GAAGhB,OACjC,MAAM,IAAI,KAAW,uBAAuB2N,eAA6B/P,EAAMoD,cAChE0M,EAAO1M,GAAGhB,iDACZsK,EAAMpG,SAEvB,IAAK,IAAI4J,EAAI,EAAGA,EAAIJ,EAAO1M,GAAGhB,SAAU8N,EAAG,CACvC,GAAU,IAANA,IAAY1C,EAEZ,SAEJ,MAAM2C,EAAMzD,EAAMpG,MAAM4J,GAClBE,EAASN,EAAO1M,GAAG8M,GACzB,GAAc,MAAVE,GAAkBA,GAAU,GAAKD,IAAQC,EACzC,MAAM,IAAI,KAAW,uBAAuBL,eAA6B/P,EAAMoD,qBACzD0M,EAAO1M,kCACrBsJ,EAAMpG,UAEtB,CACJ,CAEJ,OAAOiG,CACX,CAsGA,SAAS8D,EAAevJ,EAAM9G,EAAO8P,EAAQtC,GAAiB,EAAMuC,EAAkB,IAClF,IAAIxD,EACJ,GAAIvL,MAAMC,QAAQ6F,GAAO,CACrB,GAAIA,EAAK1E,SAAWpC,EAAMoC,OACtB,MAAM,IAAI,KAAW,6BAA6B2N,sHAEP/P,EAAMoC,qCACzB0E,EAAK1E,sBAEjCmK,EAASzF,CACb,KACK,CACD,GAAI9G,EAAMoC,OAAS,EACf,MAAM,IAAI,KAAW,qBAAqBpC,EAAMoC,UAAU2N,oEAEnDpK,KAAKC,UAAUkB,EAAKR,WAE/BiG,EAAS,CAACzF,EACd,CACA,GAAc,MAAVgJ,EACA,IAAK,IAAI1M,EAAI,EAAGA,EAAIpD,EAAMoC,SAAUgB,EAAG,CACnC,GAAiB,MAAb0M,EAAO1M,GACP,SAEJ,MAAMsJ,EAAQH,EAAOnJ,GACrB,GAAIsJ,EAAMpG,MAAMlE,SAAW0N,EAAO1M,GAAGhB,OACjC,MAAM,IAAI,KAAW,uBAAuB2N,eAA6B/P,EAAMoD,cAChE0M,EAAO1M,GAAGhB,iDACZuD,KAAKC,UAAU8G,EAAMpG,UAEtC,IAAK,IAAI4J,EAAI,EAAGA,EAAIJ,EAAO1M,GAAGhB,SAAU8N,EAAG,CACvC,GAAU,IAANA,IAAY1C,EACZ,SAEJ,MAAM2C,EAAMzD,EAAMpG,MAAM4J,GAClBE,EAASN,EAAO1M,GAAG8M,GACzB,GAAc,MAAVE,GACIA,IAAWD,EACX,MAAM,IAAI,KAAW,uBAAuBJ,eACrC/P,EAAMoD,oBAAoBuC,KAAKC,UAAUkK,EAAO1M,gCAC3BuC,KAAKC,UAAU8G,EAAMpG,UAG7D,CACJ,CAER,CA2DO,MAAMgK,UAAoB,IAC7B,WAAA1R,CAAY6J,GACR8H,MAAM9H,GACN3J,KAAKkK,YAAa,CACtB,CAoCA,OAAAwH,CAAQC,EAAYC,EAAWC,EAAUC,QAAQC,KAC7C,IAAK/R,KAAKgS,MACN,MAAM,IAAI,KAAW,iLAIzB,OAAahS,KAAM2R,EAAYC,EAAWC,EAC9C,CAWA,OAAAI,CAAQtI,GAKJ,GAJiB,MAAbA,EAAKuI,OACLvI,EAAKuI,KAAO,IAEhBlS,KAAKkS,KAAOvI,EAAKuI,KACa,kBAAnBvI,EAAKG,UACZ9J,KAAKmS,WAAaC,EAAA,EAAwBzI,EAAKG,WAC/C9J,KAAKqS,kBAAmB,MAEvB,CACD,KAAM1I,EAAKG,qBAAqB,EAAAwI,WAC5B,MAAM,IAAI,KAAW,+DAEzBtS,KAAKmS,WAAaxI,EAAKG,UACvB9J,KAAKqS,kBAAmB,CAC5B,CAIA,IAAIE,EAAgB,GACpB,GAAKrQ,MAAMC,QAAQwH,EAAKuI,OAA8B,kBAAdvI,EAAKuI,MACpB,oBAAdvI,EAAKuI,KAiBX,GAAIhQ,MAAMC,QAAQwH,EAAKuI,MAAO,CAC/B,GAAIvI,EAAKuI,KAAK5O,SAAWtD,KAAKmJ,QAAQ7F,OAClC,MAAM,IAAI,KACN,2FAA+BtD,KAAKmJ,QAAQ7F,yCACrBqG,EAAKuI,SAEpC,MAAMM,EAAY7I,EAAKuI,KACvBK,EAAgBC,EAAUlQ,KAAImQ,GAAKpK,EAAA,GAAWoK,IAClD,KACK,CACD,MAAMC,EAAerK,EAAA,GAAWsB,EAAKuI,MACrClS,KAAKmJ,QAAQnF,SAAQ2O,IACjBJ,EAAc3P,KAAK8P,EAAa,GAExC,KA/BqC,CACjC/I,EAAKuI,KAAOvI,EAAKuI,KACjB,IAAK,MAAMxR,KAAQiJ,EAAKuI,KACpB,IAAwC,IAApClS,KAAKqC,YAAYM,QAAQjC,GACzB,MAAM,IAAI,KAAW,sCAAsCA,yCAClBV,KAAKqC,eAGtD,IAAK,MAAM3B,KAAQV,KAAKqC,YAChBsH,EAAKuI,KAAKxR,GAKd6R,EAAc3P,KAAKyF,EAAA,GAAWsB,EAAKuI,KAAKxR,IAEhD,CAgBAV,KAAKuS,cAAgBA,EACrBvS,KAAK4S,gBAAkB,GACvB5S,KAAK6S,iBAAmB,GACxB7S,KAAK8S,YAAc,GACnB,IAAK,IAAIxO,EAAI,EAAGA,EAAItE,KAAKmJ,QAAQ7F,SAAUgB,EAAG,CAE1C,MAAMkD,EAAQxH,KAAK+S,qBAAqBzO,GAClC5D,EAAOV,KAAKqC,YAAYiC,GAC9BtE,KAAK4S,gBAAgBhQ,KAAKlC,GAC1BV,KAAK6S,iBAAiBjQ,KAAK4E,GAC3BxH,KAAK8S,YAAYlQ,KAAK5C,KAAKuS,cAAcjO,GAC7C,CAGA,MAAM0O,EAAoB,GAE1BhT,KAAKiT,QAAUtJ,EAAKsJ,QAEpBjT,KAAKmN,aAAe,CAAC,QACrBnN,KAAKkT,eAAiB,IAKtB,QAAU,QAAQ,KACd,IAAK,IAAI5O,EAAI,EAAGA,EAAItE,KAAKmJ,QAAQ7F,SAAUgB,EAAG,CAC1C,IAAsC,IAAlC0O,EAAkBrQ,QAAQ2B,GAC1B,SAIJ,MAAM6O,EAAenT,KAAKuS,cAAcjO,GACpCtE,KAAKmJ,QAAQ7F,OAAS,IACtBtD,KAAKkT,eAAetQ,KAAK,CAACuQ,EAAc7O,IACxCtE,KAAKmN,aAAavK,KAAK5C,KAAKqC,YAAYiC,GAAK,SAErD,KAIJ,MAAM8O,EApMP,SAAwBH,EAAS5Q,GACpC,GAAe,MAAX4Q,GAAmB/Q,MAAMC,QAAQ8Q,IAA+B,IAAnBA,EAAQ3P,OACrD,OAAOjB,EAAYC,KAAI5B,GAAQ,KAEnC,IAAI2S,EACJ,GAAuB,kBAAZJ,GAA2C,oBAAZA,EACtCI,EAAiB,CAACJ,OAEjB,KAAI/Q,MAAMC,QAAQ8Q,IAA+B,kBAAZA,EAItC,MAAM,IAAIK,UACN,kGAAsCL,KAJ1CI,EAAiBJ,CAKrB,CACA,GAAI/Q,MAAMC,QAAQkR,GAEd,OAAOhR,EAAYC,KAAI5B,GAAQ2S,IAE9B,CAED,MAAMD,EAAgB,GACtB,IAAK,MAAM1S,KAAQ2B,EAAa,CAC5B,IAAIkR,EAAgBF,EAAelC,eAAezQ,GAAQ2S,EAAe3S,GAAQ,GAC5EwB,MAAMC,QAAQoR,KACfA,EAAgB,CAACA,IAErBH,EAAcxQ,KAAK2Q,EACvB,CACA,OAAOH,CACX,CACJ,CAqK8BI,CAAe7J,EAAKsJ,QAASjT,KAAKqC,aAKlDoR,EAAe,CAACC,EAAaC,EAAYC,KACvC5T,KAAKqC,YAAYiB,OAAS,IAC1BqQ,EAAa3T,KAAKqC,YAAYqR,GAAe,IAAMC,GAEvD3T,KAAKmN,aAAavK,KAAK+Q,GACvB3T,KAAKkT,eAAetQ,KAAK,CAACgR,EAAcF,GAAa,GAEzD,QAAU,UAAU,KAChB,IAAK,IAAIpP,EAAI,EAAGA,EAAItE,KAAKmJ,QAAQ7F,SAAUgB,EAAG,CAC1C,IAAsC,IAAlC0O,EAAkBrQ,QAAQ2B,GAC1B,SAKkB,CAAC2O,IAEnB,IAAIU,EACAE,EACAC,EAEJ,IAAK,MAAMC,KAAUd,EAAS,CAC1B,GAAsB,kBAAXc,IAEF,IADL,CAAC,WAAY,MAAO,eAAgB,MAAMpR,QAAQoR,GAC1C,CACR,MAAMC,EAAchU,KAAK+S,qBAAqBzO,GA+B9C,IAAI2P,EA9BwC,IAAxCD,EAAYA,EAAY1Q,OAAS,IACjCtD,KAAKuS,cAAcjO,KAAO+D,EAAA,IAEmB,IAAzC,CAAC,WAAY,OAAO1F,QAAQoR,GAC5BF,EAAQ,MAEyC,IAA5C,CAAC,eAAgB,MAAMlR,QAAQoR,KACpCF,EAAQ,MAGP7T,KAAKuS,cAAcjO,KACxB+D,EAAA,IAG6C,IAAzC,CAAC,WAAY,OAAO1F,QAAQoR,GAC5BF,EAAQ,MAEyC,IAA5C,CAAC,eAAgB,MAAMlR,QAAQoR,KACpCF,EAAQ,OAKiC,IAAzC,CAAC,WAAY,OAAOlR,QAAQoR,GAC5BF,EAAQ,MAEyC,IAA5C,CAAC,eAAgB,MAAMlR,QAAQoR,KACpCF,EAAQ,OAI6B,IAAzC,CAAC,WAAY,OAAOlR,QAAQoR,GAC5BE,EAAS,OAEwC,IAA5C,CAAC,eAAgB,MAAMtR,QAAQoR,KACpCE,EAAS,MAGbH,EAAmBD,EACnBF,EAjDiB,GAiDeM,CACpC,KACK,CACD,MAAMC,EAAW,KAAYH,GAE7BD,EAAmBI,EACnBP,EAvDiB,GAwDM,KAA4BI,EACvD,CAEA,IAAII,GACJ,QAAUR,GAAY,KAClBQ,EAAeL,CAAgB,IAEnCL,EAAanP,EAAGqP,EAAYQ,EAChC,GAEJC,CAtEsBhB,EAAc9O,GAwExC,KAIJtE,KAAKqU,0BAA4BrU,KAAKsU,gBAC1C,CAUA,gCAAAjF,GAC0C,MAAlCrP,KAAKqU,4BAGLrU,KAAKsU,iBAAiBhR,OACtBtD,KAAKqU,0BAA0B/Q,OAKvC,CAgCA,QAAA2J,CAASmB,EAAGjH,EAAGwC,EAAO,CAAC,GACnB,MAAMT,EAA8B,MAAlBS,EAAKT,UAAoB,GAAKS,EAAKT,UACrDqE,EAAerE,GAGf,MACMyF,EAAmB3O,KAAKuU,sBAAsBnG,EAAGjH,GADhC,EACmD+B,GAC1E,IAGI,MAAMwD,EAAMiC,EAAiB,GAAG3D,OAAO2D,EAAiB,IACxD3O,KAAKuP,mBACL,MAAME,EAAIzP,KAAKwP,aACTgF,EAAWxU,KAAKkQ,SAAST,EAAG/C,EAAKxD,EAAWS,EAAKyB,QAASzB,EAAK8K,OACrE,OAAO,QAAiBD,EAC5B,CACA,QACIpE,EAAkBzB,EAAiB,GAAIP,GACvCgC,EAAkBzB,EAAiB,GAAIxH,EAC3C,CACJ,CAuBA,qBAAM4F,CAAgBrD,EAASC,GAE3B,OADA3J,KAAKuP,mBFjaNtI,eAIPwB,EAAOiB,EAASC,GAEZ,MAAM+K,EAA6B,OADnC/K,EAAOA,GAAQ,CAAC,GACQqD,QAClByC,EAAIhH,EAAM+G,aAChB,IAAI7C,EAAO,GACX,GAAIhD,EAAKyB,QAAU,EACf,MAAM,IAAI,KAAoB,wCAElC,eAAiBsJ,GAAe/K,EAAKqD,QAAU,GAAKhD,OAAOC,UAAUN,EAAKqD,UAAW,IACjF,wEAAYnG,KAAKC,UAAU6C,EAAKqD,aACpC,MAAMjB,EAhB2B,oBAgBSrC,EAhBlB2C,KAiBpB3C,QACMA,EAAQsC,WAElB,IAAI2I,EAAc,EACdC,EAAQ,EACZ,MAAOF,GAAaE,EAAQjL,EAAKqD,SAAgB,CAC7C,MAAMtE,QAAoBqD,EAAaM,OA8BvC,GA7BAM,EAAO,QAAS,KACZ,GAAIjE,EAAYlI,MAAO,CAGnB,MAAM,GAAEmI,EAAE,GAAEC,GAAOJ,EAA8BC,EAAOC,EAAYlI,OAC9DqU,EAAUlM,EAAGqC,OAAOpC,GACpBkM,EAAY,QAAS,IAAMrF,EAAEoF,KAEnC,GADA,UAAYA,GACE,IAAVD,EACA,IAAK,IAAItQ,EAAI,EAAGA,EAAIwQ,EAAUxR,SAAUgB,EACpCqI,EAAK/J,MAAK,IAAAmS,QAAO,IAGzB,MAAM7L,EAAY2L,EAAQ,GAAGrN,MAAM,GACnC,IAAK,IAAIlD,EAAI,EAAGA,EAAIwQ,EAAUxR,SAAUgB,EAAG,CACvC,MAAM0Q,EAAWF,EAAUxQ,GACrB2Q,EAAYtI,EAAKrI,GACvBqI,EAAKrI,GACD,QAAS,IAAM,MAAQqI,EAAKrI,GAAI,MAAQ4E,EAAW8L,MACnDJ,EAAQ,GACR,UAAYK,EAEpB,CACA,UAAYH,GACZH,GAAezL,IACb0L,CACN,CACA,OAAOjI,CAAI,IAEXjE,EAAY4D,KASZ,KAER,CACA,IAAK,IAAIhI,EAAI,EAAGA,EAAIqI,EAAKrJ,SAAUgB,EAAG,CAClC,MAAM2Q,EAAYtI,EAAKrI,GACvBqI,EAAKrI,GAAK,MAAQqI,EAAKrI,GAAIqQ,GAC3B,UAAYM,EAChB,CACA,OAAO,QAAiBtI,EAC5B,CE6VeI,CAAgB/M,KAAM0J,EAASC,EAC1C,CAWA,eAAAkG,CAAgBnD,EAAKxD,EAAWuL,EAAOS,EAAY,SAC/C,IAAIC,EACJ,GAAa,MAATV,GAEA,GADAU,EAAa,KACI,MAAbjM,EACA,MAAM,IAAI,KAAW,MAAMgM,iEACJhM,SAG1B,IAAW,MAAPwD,EASL,MAAM,IAAI,KACN,yDAAGwI,yBARHC,EADAjT,MAAMC,QAAQuK,GACDA,EAAI,GAAGlF,MAAM,GAGbkF,EAAIlF,MAAM,EAM/B,CACA,OAAO2N,CACX,CAQA,OAAAxT,CAAQsD,EAAQkE,GACZ,GAAIjH,MAAMC,QAAQgH,IAA+B,IAAnBA,EAAQ7F,OAClC,MAAM,IAAI,KAAW,sDAEzB,MAAM8R,EAAiBlT,MAAMC,QAAQgH,GAC/B9G,EAAe+S,EAAiBjM,EAAU,CAACA,GAC3CxD,EAAwB3F,KAAKqV,wBAAwBhT,GAErDR,EAAW,IAAI,KAIrB,GAHIoD,aAAkB,EAAAsL,SAClBtL,EAAS,CAACA,IAEV/C,MAAMC,QAAQ8C,GAAS,CACvB,GAAIA,EAAO3B,SAAWtD,KAAKiF,OAAO3B,OAC9B,MAAM,IAAI,KAAW,kCAAkC2B,EAAO3B,8DAEtDtD,KAAKiF,OAAO3B,YAExB,IAAK,IAAIgB,EAAI,EAAGA,EAAItE,KAAKiF,OAAO3B,SAAUgB,EACtCzC,EAASvB,IAAIN,KAAKiF,OAAOX,GAAIW,EAAOX,GAE5C,MAEI,IAAK,MAAMU,KAAShF,KAAKiF,OAAQ,CAC7B,MAAMqQ,EAAcrQ,EAAOD,EAAMtE,MACjC,GAAmB,MAAf4U,EACA,MAAM,IAAI,KAAW,8CAA8CtQ,EAAMtE,QAE7EmB,EAASvB,IAAI0E,EAAOsQ,EACxB,CAGJ,MAAMC,GAAiB,QAAQ5P,EAAuB9D,GACtD,OAAOuT,EAAiBG,EAAiBA,EAAe,EAC5D,CAIA,uBAAAF,CAAwBG,GACpB,MAAM7P,GAAwB,QAAa,KAAM6P,EAAoBlS,QACrE,IAAImS,EAAmBD,EAAoBlS,OAC3C,IAAK,MAAMoS,KAAS1V,KAAK2V,OAAQ,CAC7B,MAAMlQ,EAAevD,MAAMC,QAAQuT,EAAMtP,QAAUsP,EAAMtP,OAAS,CAACsP,EAAMtP,QACnEwP,EAAmBnQ,EAAanD,KAAI8D,GAAUA,EAAO1F,OAC3D,IAAK,IAAI4D,EAAI,EAAGA,EAAIkR,EAAoBlS,SAAUgB,EAAG,CACjD,MAAMsB,EAAQgQ,EAAiBjT,QAAQ6S,EAAoBlR,IAK3D,IAJe,IAAXsB,IACAD,EAAsBrB,GAAKmB,EAAaG,GACxC6P,KAEqB,IAArBA,EACA,KAER,CACA,GAAyB,IAArBA,EACA,KAER,CACA,GAAIA,EAAmB,EAAG,CACtB,MAAMI,EAAiB,GAMvB,MALAlQ,EAAsB3B,SAAQ,CAACwM,EAAQlM,KACrB,MAAVkM,GACAqF,EAAejT,KAAK4S,EAAoBlR,GAC5C,IAEE,IAAI,KACN,mDAAGuC,KAAKC,UAAU+O,KAC1B,CACA,OAAOlQ,CACX,CAcA,WAAAmQ,CAAYpJ,EAAKxD,EAAY,GAAIkC,GAAU,GACvC,OAAO,QAAS,KACZ,MAAM+J,EAAanV,KAAK6P,gBAAgBnD,GACxC,GAAItB,EACA,MAAM,IAAI,KAAoB,iDAMlC,MAAM4B,EAAUgB,EAAYmH,EAAYjM,GAClC6M,EAAc/V,KAAKmJ,QAAQ7G,KAAI8D,GAAU,KAE/C,IAAK,IAAIgG,EAAa,EAAGA,EAAaY,EAAQ1J,SAAU8I,EAAY,CAC9C,QAAS,KACvB,MAAM6B,EAAajB,EAAQZ,GAAY,GACjC8B,EAAWlB,EAAQZ,GAAY,GAG/B6D,EAAWzC,EAAYd,EAAKuB,EAAYC,GAExCnO,EAAQ,GACd,GAAImC,MAAMC,QAAQ8N,GACd,IAAK,IAAI3L,EAAI,EAAGA,EAAI2L,EAAS3M,SAAUgB,EACnCvE,EAAM6C,KAAK,CAAErC,IAAKP,KAAKiF,OAAOX,GAAI9D,MAAOyP,EAAS3L,UAItDvE,EAAM6C,KAAK,CAAErC,IAAKP,KAAKiF,OAAO,GAAIzE,MAAOyP,IAE7C,MAAMpO,EAAW,IAAI,KAAS9B,GAC9B,OAAO,QAAQC,KAAKmJ,QAAStH,EAAS,IAEhCmC,SAAQ,CAACgR,EAAU1Q,IAAMyR,EAAYzR,GAAG1B,KAAKoS,IAC3D,CACA,OAAO,QAAiBe,EAAYzT,KAAI0K,GAAW,SAAWA,EAAS,KAAI,GAEnF,CA4BA,OAAAgJ,CAAQ5H,EAAGzE,EAAO,CAAC,GACf,MAAMsM,EAAkB5F,EAA2BjC,GACnDmD,EAAe0E,EAAiBjW,KAAKgJ,WAAYhJ,KAAKkW,iBAAiB,GACvE,IAKI,MAAMhN,EAA8B,MAAlBS,EAAKT,UAAoB,GAAKS,EAAKT,UAErD,OADAqE,EAAerE,GACRlJ,KAAK8V,YAAYG,EAAiB/M,EAC7C,CACA,QACIkH,EAAkB6F,EAAiB7H,EACvC,CACJ,CAgBA,cAAA+H,CAAe/H,GACXmD,EAAenD,EAAGpO,KAAKgJ,WAAYhJ,KAAKkW,iBAAiB,GAGzD,MAAMhN,GAAahH,MAAMC,QAAQiM,GAAKA,EAAE,GAAKA,GAAG5G,MAAM,GACtD,OAAOxH,KAAK8V,YAAY1H,EAAGlF,EAC/B,CACA,qBAAAqL,CAAsBnG,EAAGjH,EAAGuH,GAAiB,EAAMxF,GAE/C,GAAuB,MAAnBlJ,KAAKmS,WACL,MAAM,IAAI,KAAa,gGAG3B,MAAMiE,EAAe,GACrB,IAAK,IAAI9R,EAAI,EAAGA,EAAItE,KAAK6S,iBAAiBvP,SAAUgB,EAAG,CACnD,MAAM0P,EAAchU,KAAK6S,iBAAiBvO,GAC3BtE,KAAK8S,YAAYxO,KACjB+D,EAAA,GACX+N,EAAaxT,KAAKoR,EAAYjJ,MAAM,EAAGiJ,EAAY1Q,OAAS,GAAG0H,OAAO,CAAC,KAIvEoL,EAAaxT,KAAKoR,EAE1B,CAOA,GAzxBD,SAA2B/O,EAAQoJ,GACtC,MAAMgI,GAAO,QAAOpR,EAAO3C,KAAI0C,GAASA,EAAMwC,MAAM,MACpD6O,EAAKC,OACL,MAAMC,GAAO,QAAOlI,EAAQ/L,KAAIkU,GAAUA,EAAOhP,MAAM,MAGvD,GAFA+O,EAAKD,OAEDD,EAAK/S,OAAS,EACd,MAAM,IAAI,KAEN,mFAAGuD,KAAKC,UAAU7B,EAAO3C,KAAI0C,GAASA,EAAMwC,YAEpD,GAAI+O,EAAKjT,OAAS,EACd,MAAM,IAAI,KAEN,oFAAGuD,KAAKC,UAAUuH,EAAQ/L,KAAIkU,GAAUA,EAAOhP,YAEvD,GAAI6O,EAAK/S,OAAS,GAAKiT,EAAKjT,OAAS,IAAM,EAAAD,KAAA,YAAiBgT,EAAME,GAC9D,MAAM,IAAI,KACN,iFAAkBF,EAAK,0BAA0BE,EAAK,uBAGlE,CAiwBQE,CAHArI,EAAI2C,EAAqB3C,EAAGpO,KAAK0W,eAAgB1W,KAAKkW,iBAAiB,EAAO,SAC9E/O,EAAI4J,EAAqB5J,EAAGnH,KAAK4S,gBAAiBwD,GAAc,EAAO,WArvB/E,SAAyC/H,EAASsI,EAASP,GAEvD,MAAMQ,EAAY,CACdvO,EAAA,GAAyBA,EAAA,GACzBA,EAAA,IAEJ,IAAK,IAAI/D,EAAI,EAAGA,EAAI+J,EAAQ/K,SAAUgB,EAAG,CACrC,MAAM6C,EAAIkH,EAAQ/J,GACZ4N,EAAOyE,EAAQrS,GACfkD,EAAQ4O,EAAa9R,GAC3B,GAAY,MAAR4N,EAAJ,CAGA,GAAIA,IAAS7J,EAAA,IAC2B,IAAhClB,EAAEK,MAAML,EAAEK,MAAMlE,OAAS,GACzB,MAAM,IAAI,KAAW,2CAA2C6D,EAAEK,iKAO1E,IAAiC,IAA7BoP,EAAUjU,QAAQuP,GAAc,CAChC,MAAM2E,EAAe1P,EAAEK,MAAMuD,MAAM,GAC7B+L,EAActP,EAAMuD,MAAM,GAChC,IAAK,IAAIqG,EAAI,EAAGA,EAAIyF,EAAavT,SAAU8N,EAAG,CAC1C,MAAM2F,EAAYF,EAAazF,GACzB4F,EAASF,EAAY1F,GAC3B,GAAc,MAAV4F,GAAkBD,IAAcC,EAChC,MAAM,IAAI,KAAW,8BAA8B7P,EAAEK,2CAC9BA,4FAG/B,CACJ,CAtBA,CAuBJ,CACJ,CAqtBQyP,CAAgC9P,EAAGnH,KAAK8S,YAAa9S,KAAK6S,kBACtD7S,KAAKmF,UAAyB,MAAb+D,GAAqBA,EAAY,GAC9CkF,EAAE,GAAG5G,MAAM,GAAK0B,IAAc,EAC9B,MAAM,IAAI,KAEN,mHAAGA,aAAqBkF,EAAE,GAAG5G,MAAM,iBAG/C,MAAO,CAAC4G,EAAGjH,EACf,CACA,yBAAMyH,CAAoBR,EAAGjH,EAAGC,EAAcJ,EAAa0H,GAAiB,EAAMxF,GAC9E,MAAOgO,EAAYC,GAAcnX,KAAKuU,sBAAsBnG,EAAGjH,EAAGuH,EAAgBxF,GAElF,GAAoB,MAAhB9B,EACA,MAAM,IAAIR,MAAM,uCAEpB,IAAIwQ,EAAwB,KAC5B,GAAmB,MAAfpQ,EAAqB,CACrB,MAAMqQ,EAAetQ,EAAwBC,EAAahH,KAAKqC,aAC/D+U,EAAwB,GACxB,IAAK,IAAI9S,EAAI,EAAGA,EAAI+S,EAAa/T,SAAUgB,EACvC8S,EAAsBxU,WAAWsE,EAAmBiQ,EAAW7S,GAAI,KAAM+S,EAAa/S,IAE9F,CAEA,MAAO,CAAC4S,EAAYC,EAAYC,EACpC,CAYA,QAAAlH,CAAST,EAAG/C,EAAKxD,EAAWkC,EAAU,EAAGqJ,GACrC,OAAO,QAAS,KACZ,MAAMU,EAAanV,KAAK6P,gBAAgBnD,EAAKxD,EAAWuL,EAAO,SACzD9H,EAAO,GACb,GAAIvB,EAAU,EACV,MAAM,IAAI,KAAoB,wCAGlC,GAAa,MAATqJ,EACA,MAAM,IAAI,KAAoB,mDAE7B,CACD,MAAMzH,EAAUgB,EAAYmH,EAAYjM,GAClC4G,GAAa,IAAA3H,WAAS,QAAM,EAAGgN,IACrC,IAAK,IAAI/I,EAAa,EAAGA,EAAaY,EAAQ1J,SAAU8I,EAAY,CAChE,MAAM6B,EAAajB,EAAQZ,GAAY,GACjC8B,EAAWlB,EAAQZ,GAAY,GAC/B4D,EAAW,KAAsBF,EAAY7B,EAAYC,EAAWD,GAGpEgC,EAAWpC,EAAqBnB,EAAKsD,GACrC8E,EAAYrF,EAAEQ,GACpB,GAAmB,IAAf7D,EACA,IAAK,IAAI9H,EAAI,EAAGA,EAAIwQ,EAAUxR,SAAUgB,EACpCqI,EAAK/J,MAAK,IAAAmS,QAAO,IAGzB,IAAK,IAAIzQ,EAAI,EAAGA,EAAIwQ,EAAUxR,SAAUgB,EAAG,CACvC,MAAM0Q,EAAWF,EAAUxQ,GAC3BqI,EAAKrI,GACD,MAAQqI,EAAKrI,GAAI,MAAQ4J,EAAWD,EAAY+G,GACxD,CACJ,CACA,IAAK,IAAI1Q,EAAI,EAAGA,EAAIqI,EAAKrJ,SAAUgB,EAC/BqI,EAAKrI,GAAK,MAAQqI,EAAKrI,GAAI6Q,EAEnC,CACA,OAAOxI,CAAI,GAEnB,CACA,sBAAA9B,GACI,MAAMD,EAAY5K,KAAKmN,aAGjBmK,EAAmB,GACzB,IAAK,IAAIhT,EAAI,EAAGA,EAAIsG,EAAUtH,SAAUgB,EAAG,CACvC,MAAMsI,EAAQhC,EAAUtG,GACxB,IAAIiT,EAAW3K,EACf,IAAI,QAAMhC,EAAWgC,GAAS,EAAG,CAE7B2K,GAAY,KADK,QAAM3M,EAAUG,MAAM,EAAGzG,GAAIsI,IAElD,CACA0K,EAAiB1U,KAAK2U,EAC1B,CACA,OAAOD,CACX,CAWA,iBAAA3M,GACI,OAAQ3C,IACJ,MAAMwP,EAAa,GACbvS,EAAS+C,EAAK+C,MAAM,EAAG/K,KAAKiF,OAAO3B,QACnC+K,EAAUrG,EAAK+C,MAAM/K,KAAKiF,OAAO3B,OAAQtD,KAAKiF,OAAO3B,OAAStD,KAAKmJ,QAAQ7F,QAC3EgF,EAAgBN,EAAK+C,MAAM/K,KAAKiF,OAAO3B,OAAStD,KAAKmJ,QAAQ7F,OAAQtD,KAAKiF,OAAO3B,OAA+B,EAAtBtD,KAAKmJ,QAAQ7F,QACvGmU,EAAgB,GAwDhBC,EAAY1X,KAAKqU,0BAA0B/R,KAAIqV,GAASA,EAAMC,SAGpE,MAAO,CADgB5X,KAAKmS,WAAW0F,UAtDb,KACtB,MAAM9X,EAAQ,GACd,IAAK,IAAIuE,EAAI,EAAGA,EAAItE,KAAKiF,OAAO3B,SAAUgB,EACtCvE,EAAM6C,KAAK,CAAErC,IAAKP,KAAKiF,OAAOX,GAAI9D,MAAOyE,EAAOX,KAEpD,MAAMzC,EAAW,IAAI,KAAS9B,GACxBoJ,GAAU,QAAQnJ,KAAKmJ,QAAStH,EAAU,CAAE,UAAY,IAG9D,IAAIiW,EACJ,IAAK,IAAIxT,EAAI,EAAGA,EAAItE,KAAKuS,cAAcjP,SAAUgB,EAAG,CAEhD,IAAI4N,GAAOQ,EADU1S,KAAKuS,cAAcjO,IAChB+J,EAAQ/J,GAAI6E,EAAQ7E,IACpB,MAApBgE,EAAchE,KACd4N,EAAO9J,EAAoB8J,EAAM5J,EAAchE,KAGnD,MAAMyT,EAAW,OAAS7F,GAE1BsF,EAAW5U,KAAKmV,GAEZD,EADM,IAANxT,EACY4N,EAGA,MAAQ4F,EAAW5F,EAEvC,CAIA,IAAK,IAAI5N,EAAI,EAAGA,EAAItE,KAAKkT,eAAe5P,SAAUgB,EAAG,CACjD,IAAI0T,EACJ,GAAIhY,KAAKmJ,QAAQ7F,OAAS,GAAKgB,EAAItE,KAAKmJ,QAAQ7F,OAC5C0U,EAAiBR,EAAWlT,OAE3B,CACD,MAAMyP,EAAS/T,KAAKkT,eAAe5O,GAAG,GAChCoP,EAAc1T,KAAKkT,eAAe5O,GAAG,GAC3C0T,EACI,OAASjE,EAAO1F,EAAQqF,GAAcvK,EAAQuK,IACtD,CACA,OAASsE,GAETP,EAAc7U,KAAKoV,EACvB,CAMA,OALAF,EAAY,OAASA,GAErB9X,KAAKiY,kBAAkBjU,SAAQkU,IAC3BJ,EAAY,MAAQA,EAAWI,EAAgB,IAE5CJ,CAAS,IAGD,EAC4DJ,IACvD1M,OAAOyM,EAAc,CAErD,CAMA,gBAAAlI,GACIvP,KAAKwP,aAAgBxH,GACV,QAAS,KACZ,MAAMmQ,EAAa,GACnB,IAAIL,EACJ,MAAM7S,EAAS+C,EAAK+C,MAAM,EAAG/K,KAAKiF,OAAO3B,QACnC+K,EAAUrG,EAAK+C,MAAM/K,KAAKiF,OAAO3B,OAAQtD,KAAKiF,OAAO3B,OAAStD,KAAKmJ,QAAQ7F,QAC3EvD,EAAQ,GACd,IAAK,IAAIuE,EAAI,EAAGA,EAAItE,KAAKiF,OAAO3B,SAAUgB,EACtCvE,EAAM6C,KAAK,CAAErC,IAAKP,KAAKiF,OAAOX,GAAI9D,MAAOyE,EAAOX,KAEpD,MAAMzC,EAAW,IAAI,KAAS9B,GACxBoJ,GAAU,QAAQnJ,KAAKmJ,QAAStH,GAEtC,IAAK,IAAIyC,EAAI,EAAGA,EAAItE,KAAKuS,cAAcjP,SAAUgB,EAAG,CAChD,MAAMoO,EAAe1S,KAAKuS,cAAcjO,GAGlC4N,EAAO,OAASQ,EAAarE,EAAQ/J,GAAI6E,EAAQ7E,KAEnDwT,EADM,IAANxT,EACY4N,EAGA,MAAQ4F,EAAW5F,GAEnCiG,EAAWvV,KAAKkV,EACpB,CAEA,IAAK,IAAIxT,EAAI,EAAGA,EAAItE,KAAKkT,eAAe5P,SAAUgB,EAAG,CACjD,MAAMyP,EAAS/T,KAAKkT,eAAe5O,GAAG,GAChCoP,EAAc1T,KAAKkT,eAAe5O,GAAG,GAErC8T,EAAa,OAASrE,EAAO1F,EAAQqF,GAAcvK,EAAQuK,KACjEyE,EAAWvV,KAAKwV,EACpB,CACA,OAAOD,CAAU,GAG7B,CAmCA,SAAME,CAAIjK,EAAGjH,EAAGwC,EAAO,CAAC,GACpB,OAAOwE,EAAWnO,KAAMoO,EAAGjH,EAAGwC,EAClC,CAwBA,gBAAMF,CAAWC,EAASC,GACtB,OAAOF,EAAWzJ,KAAM0J,EAASC,EACrC,CAwBA,kBAAM2O,CAAalK,EAAGjH,GAGlB,MAAMoR,QAAuBvY,KAAK4O,oBAAoBR,EAAGjH,GACnDlC,EAASsT,EAAe,GACxBlK,EAAUkK,EAAe,GAEzBlQ,EADgBrI,KAAK2K,mBACZD,CAAczF,EAAO+F,OAAOqD,IACrCmJ,EAAa,GACnB,IAAK,MAAMtF,KAAQ7J,EAAQ,CACvB,MAAMmQ,QAAUtG,EAAKlK,OACrBwP,EAAW5U,KAAK4V,EAAE,GACtB,CAEA,OADA,UAAYnQ,IACL,QAAiBmP,EAC5B,CAUA,eAAAiB,CAAgBC,GACZ,MAAMC,EAAe,GACfC,EAA0B,MAAVF,GAAkBA,EAAOE,cACzCC,EAAUD,EAAgB5Y,KAAKsU,iBAAmBtU,KAAK6Y,QACvDC,EAAe9Y,KAAK+Y,WAAWH,GACrC,IAAK,IAAItU,EAAI,EAAGA,EAAIuU,EAAQvV,SAAUgB,EAC9BsU,IAAkBC,EAAQvU,GAAG0U,WAIjCL,EAAa/V,KAAK,CAAElC,KAAMmY,EAAQvU,GAAG2U,aAAczI,OAAQsI,EAAaxU,KAE5E,OAAOqU,CACX,CA+BA,gBAAIO,CAAavL,GACb3N,KAAK4L,cAAgB+B,CACzB,CACA,gBAAIuL,GACA,OAAOlZ,KAAK4L,aAChB,CACA,aAAI9B,GACA,OAAO9J,KAAKmS,UAChB,CACA,aAAIrI,CAAUA,GACN9J,KAAKmS,aAAerI,IACpB9J,KAAKmS,WAAarI,EAClB9J,KAAKqS,kBAAmB,EAEhC,CACA,OAAA7Q,GACI,MAAMgI,EAASiI,MAAMjQ,UACrB,GAAoC,IAAhCgI,EAAO2P,sBAAgD,MAAlBnZ,KAAK8J,WAC1C9J,KAAKqS,iBAAkB,CACvB,MAAM+G,EAAmC,WAAa7U,WACtDvE,KAAKmS,WAAW3Q,UAChBgI,EAAO6P,sBACHD,EAAmC,WAAa7U,UACxD,CACA,OAAOiF,CACX,CACA,kBAAA8P,GACI,IAAIC,EACJ,GAAyB,kBAAdvZ,KAAKkS,KACZqH,GAAY,QAAYvZ,KAAKkS,WAE5B,GAAIhQ,MAAMC,QAAQnC,KAAKkS,MAAO,CAC/B,IAAK,MAAMA,KAAQlS,KAAKkS,KACpB,GAAoB,kBAATA,EACP,MAAM,IAAItL,MAAM,sDAGxB2S,EAAYvZ,KAAKkS,KAAK5P,KAAI5B,IAAQ,QAAYA,IAClD,KACK,CACD,MAAM2B,EAAclB,OAAOC,KAAKpB,KAAKkS,MACrCqH,EAAY,CAAC,EACb,MAAMlR,EAASrI,KAAKkS,KACpB,IAAK,MAAMxP,KAAcL,EAAa,CAClC,GAAkC,kBAAvBgG,EAAO3F,GAKd,MAAM,IAAIkE,MAAM,sDAJhB2S,EAAU7W,IACN,QAAY2F,EAAO3F,GAK/B,CACJ,CACA,OAAO6W,CACX,CACA,oBAAAC,GACI,GAA4B,kBAAjBxZ,KAAKiT,SACY,oBAAjBjT,KAAKiT,QACZ,MAAO,EAAC,QAAY,KAA4BjT,KAAKiT,WAEpD,GAAI/Q,MAAMC,QAAQnC,KAAKiT,SACxB,OAAOjT,KAAKiT,QAAQ3Q,KAAIyR,IAAU,QAAY,KAA4BA,MAEzE,CACD,MAAM0F,EAAqB,CAAC,EAC5B,IAAK,MAAMlZ,KAAOP,KAAKiT,QACnBwG,EAAmBlZ,IACf,QAAY,KAA4BP,KAAKiT,QAAQ1S,KAE7D,OAAOkZ,CACX,CACJ,CACA,iBAAAC,GACI,MAAO,CACHxH,KAAMlS,KAAKsZ,qBACXrG,QAASjT,KAAKwZ,uBACdG,iBAAkB,CACdC,WAAY5Z,KAAK8J,UAAU+P,eAC3BnB,OAAQ1Y,KAAK8J,UAAUgQ,aAMnC,CACA,kBAAAC,CAAmBC,GACf,GAAuC,MAAnCA,EAAeC,iBACf,MAAM,IAAIrT,MAAM,gDAEpB,GAAmC,MAA/BoT,EAAeE,aACf,MAAM,IAAItT,MAAM,8CAEpB,GAAyC,MAArCoT,EAAeG,mBACf,MAAM,IAAIvT,MAAM,oDAEpB,MAAMwT,GAAW,OAAoBJ,EAAeL,kBAC9C7P,GAAY,OAAYsQ,GAC9B,IAAIlI,EAaAe,EAZJ,GAAmC,kBAAxB+G,EAAe9H,KACtBA,GAAO,QAAY8H,EAAe9H,WAEjC,GAAIhQ,MAAMC,QAAQ6X,EAAe9H,MAClCA,EAAO8H,EAAe9H,KAAK5P,KAAI+X,IAAa,QAAYA,UAEvD,GAA2B,MAAvBL,EAAe9H,KAAc,CAClCA,EAAO,CAAC,EACR,IAAK,MAAM3R,KAAOyZ,EAAe9H,KAC7BA,EAAK3R,IAAO,QAAYyZ,EAAe9H,KAAK3R,GAEpD,CAEA,GAAI2B,MAAMC,QAAQ6X,EAAe/G,SAC7BA,EAAU+G,EAAe/G,QAAQ3Q,KAAIyR,IAAU,QAAYA,UAE1D,GAA8B,MAA1BiG,EAAe/G,QAAiB,CACrCA,EAAU,CAAC,EACX,IAAK,MAAM1S,KAAOyZ,EAAe/G,QAC7BA,EAAQ1S,IAAO,QAAYyZ,EAAe/G,QAAQ1S,GAE1D,CACAP,KAAKiS,QAAQ,CAAEC,OAAMe,UAASnJ,aAClC,CAkFA,UAAMwQ,CAAKC,EAAc7B,GACrB,GAA4B,kBAAjB6B,EAA2B,CAClC,MAAMC,EAAW,EAAAC,GAAA,gBAAmBF,GACpC,GAAwB,IAApBC,EAASlX,OACT,MAAM,IAAI,KAAW,0CAA0CiX,MAE9D,GAAIC,EAASlX,OAAS,EACvB,MAAM,IAAI,KAAW,wBAAwBkX,EAASlX,kCAC1CiX,MAEhBA,EAAeC,EAAS,EAC5B,CACA,GAAyB,MAArBD,EAAaD,KACb,MAAM,IAAI,KAAW,gHAGzB,MAAMI,QAA2B,EAAAD,GAAA,cAAiBza,KAAKyY,gBAAgBC,IAIjEiC,EAAiB,CACnBC,cAFgB5a,KAAK6a,OADP,MADG,GAKjBC,OAlrCqB,eAmrCrBC,YAAa,8BAA8BC,EAAA,IAC3CC,YAAa,MAGjB,GADmC,MAAVvC,GAAyBA,EAAOwC,kBACf,MAAlBlb,KAAK8J,UAAmB,CAC5C6Q,EAAeX,eAAiBha,KAAK0Z,oBACrC,MAAMhT,EAAa,aACXsB,KAAMmT,EAAqBC,MAAOC,SAA+B,EAAAZ,GAAA,oBAAuBza,KAAK8J,UAAUiP,aAAcrS,GAC7HgU,EAAmBU,MAAMxY,QAAQyY,GACjCX,EAAmB1S,KAAO,EAAAyS,GAAA,wBAA2B,CAACC,EAAmB1S,KAAMmT,GACnF,CACA,GAAgC,MAA5Bnb,KAAKsb,oBAA6B,CAElC,MAAMC,GAAY,GAClB,QAAyBvb,KAAKsb,oBAAqBtb,KAAKU,KAAM6a,GAC9DZ,EAAeW,oBAAsBtb,KAAKsb,mBAC9C,CAGA,OAFAX,EAAea,WAAad,EAAmB1S,KAC/C2S,EAAec,YAAcf,EAAmBU,MACzCb,EAAaD,KAAKK,EAC7B,CASA,sBAAAe,CAAuBJ,IACnB,QAAyBA,EAAqBtb,KAAKU,MACnDV,KAAKsb,oBAAsBA,CAC/B,CAYA,sBAAAK,GACI,OAAO3b,KAAKsb,mBAChB,EAKJ9J,EAAYoK,UAAY,QACxB,EAAAC,cAAA,cAA4BrK,GAQrB,MAAMsK,UAAmBtK,GAEhCsK,EAAWF,UAAY,aACvB,EAAAC,cAAA,cAA4BC,E,8HC5jDrB,MAAMC,UAAmB,KAC5B,WAAAjc,CAAY6J,GAeR,GAdA8H,MAAM,CACF7Q,MAAO+I,EAAK/I,MACZF,KAAmB,MAAbiJ,EAAKjJ,KAAeiJ,EAAKjJ,MAAO,OAAO,SAASsb,aAGpC,MAAlBrS,EAAKT,YACLS,EAAKT,UAAY,MAEF,MAAfS,EAAKsS,SACLtS,EAAKsS,QAAS,GAElBjc,KAAKgZ,WAAY,EACjBhZ,KAAKgS,OAAQ,EACbhS,KAAKic,OAAStS,EAAKsS,OACI,MAAnBtS,EAAKuS,YAA8C,MAAxBvS,EAAKwS,gBAChC,MAAM,IAAI,KAAW,qGAGzB,IAAIA,EAAkBxS,EAAKwS,gBAC3B,GAAuB,MAAnBA,EAAyB,CACzB,GAAuB,MAAnBxS,EAAKuS,WACL,MAAM,IAAI,KAAW,iFAIrBC,EAAkB,CAACxS,EAAKT,WAAW8B,OAAOrB,EAAKuS,WAEvD,MAGI,GAAsB,MAAlBvS,EAAKT,UACL,MAAM,IAAI,KAAW,yFAI7B,MAAMtI,EAAQ+I,EAAK/I,OAAS,UAC5BZ,KAAKmc,gBAAkBA,EACvBnc,KAAKY,MAAQA,EAEbZ,KAAKoc,UAAY,CAAC,CAAE5U,MAAO2U,IAC3B,MAAME,EAAc,IAAI,KAAerc,KAAKY,MAAOZ,KAAKmc,gBAAiBnc,KAAM,GAAI,CAAC,EAAGA,KAAKU,MAC5F2b,EAAYhW,UAAY,EACxBgW,EAAYC,YAAc,EAI1B,IAAI,KAAK,CACLC,cAAevc,KACfwc,cAAe,GACfC,YAAa,GACbC,cAAe,GACfC,aAAc,CAACN,GACfjX,cAAe,CAACiX,GAChBxX,WAAY,CAAC,MACb+X,YAAa,CAAC,MACdC,YAAa,CAACV,GACd/F,aAAc,CAAC+F,IAEvB,CACA,KAAA9W,CAAMJ,EAAQnD,GACV,MAAM,IAAI,KACN,6EAAiD9B,KAAKU,OAC9D,CACA,OAAAc,GAEI,MAAO,CAAE2X,qBAAsBnZ,KAAK8c,UAAWzD,qBAAsB,EACzE,CACA,SAAAS,GACI,MAAO,CACHqC,gBAAiBnc,KAAKmc,gBACtBvb,MAAOZ,KAAKY,MACZqb,OAAQjc,KAAKic,OACbvb,KAAMV,KAAKU,KAEnB,EAKG,SAASqc,EAAMrE,GAClB,GAAyB,MAArBA,EAAOsE,YAAsC,MAAhBtE,EAAOlR,MACpC,MAAM,IAAIZ,MAAM,gIAKpB,GAAyB,MAArB8R,EAAOsE,YAAsC,MAAhBtE,EAAOlR,MAEpC,MAAM,IAAI,KAAW,oFAGzB,IAAIwV,EAAatE,EAAOsE,WACJ,MAAhBtE,EAAOlR,OAA+B,MAAdwV,IACxBA,EAAa,CAAC,MAAMhS,OAAO0N,EAAOlR,QAEtC,IAAI5G,EAAQ8X,EAAO9X,MACN,MAATA,IACAA,EAAQ,WASZ,OAPmB,IAAImb,EAAW,CAC9BI,gBAAiBa,EACjBtc,KAAMgY,EAAOhY,KACbE,QACAqb,OAAQvD,EAAOuD,SAEQ9V,aAAa,GAAGf,cAC5B,EACnB,CA9BA2W,EAAWH,UAAY,aACvB,EAAAC,cAAA,cAA4BE,E,+PCjErB,MAAMkB,EACT,WAAAnd,CAAY6J,GACR3J,KAAKY,MAAQ+I,EAAK/I,MAClBZ,KAAKwH,MAAQmC,EAAKnC,MAKA,MAAdmC,EAAKnC,MACLxH,KAAKkd,KAAOvT,EAAKnC,MAAMlE,OAGvBtD,KAAKkd,KAAOvT,EAAKuT,KAErBld,KAAKmd,QAAUxT,EAAKwT,QACpBnd,KAAKod,QAAUzT,EAAKyT,QACpBpd,KAAKqd,KAAO1T,EAAK0T,MAAQ,CAAC,CAC9B,EAUG,MAAMC,EAcT,WAAAxd,CAAYc,EAAO4G,EAAO7C,EAAaM,EAAQsY,EAAU7c,EAAM8c,GAC3Dxd,KAAKY,MAAQA,EACbZ,KAAKwH,MAAQA,EACbxH,KAAK2E,YAAcA,EACnB3E,KAAKiF,OAASA,EACdjF,KAAKud,SAAWA,EAChBvd,KAAKwd,kBAAoBA,EACzBxd,KAAKI,IAAK,SACE,MAARM,IACAV,KAAKiZ,cAAe,QAAoBvY,GACxCV,KAAKU,MAAO,QAAoBV,KAAKiZ,eAEzCjZ,KAAK6H,KAAOL,EAAMlE,MACtB,EAEJ,IAAIma,EAAc,EAqBX,MAAMC,EACT,WAAA5d,CAAY6J,EAEZ4T,GACIvd,KAAKud,SAAWA,EAChBvd,KAAKI,GAAKqd,IAQVzd,KAAKuc,cAAgB5S,EAAK4S,cAQ1Bvc,KAAKwc,cAAgB7S,EAAK6S,cAE1Bxc,KAAKyc,YAAc9S,EAAK8S,YAExBzc,KAAK0c,cAAgB/S,EAAK+S,cAM1B1c,KAAK2c,aAAehT,EAAKgT,aAEzB3c,KAAKoF,cAAgBuE,EAAKvE,cAK1BpF,KAAK6E,WAAa8E,EAAK9E,WAEvB7E,KAAK4c,YAAcjT,EAAKiT,YAGxB5c,KAAK6c,YAAclT,EAAKkT,YAExB7c,KAAKoW,aAAezM,EAAKyM,aAEzB,IAAK,MAAMV,KAAS/L,EAAK6S,cACR,MAAT9G,GACAA,EAAMiI,cAAc/a,KAAK5C,MAGjC2J,EAAK4S,cAAcpW,aAAavD,KAAK5C,KACzC,CACA,SAAA8Z,GACI,MAAM8D,EAAe,GACrB,IAAK,MAAMlI,KAAS1V,KAAKwc,cACR,MAAT9G,EACAkI,EAAahb,KAAK8S,EAAMhV,MAGxBkd,EAAahb,KAAK,MAG1B,MAAO,CACH2Z,cAAevc,KAAKuc,cAAgBvc,KAAKuc,cAAc7b,KAAO,KAC9D8b,cAAeoB,EACfnB,YAAazc,KAAKyc,YAClBC,cAAe1c,KAAK0c,cAE5B,EAEJ,IAAImB,EAAe,EAUZ,MAAMC,UAAc,EAAAjC,cAAA,aACvB,WAAA/b,CAAY6J,EAAO,CAAC,GAChB8H,QACAzR,KAAK+d,UAAY,KACjB/d,KAAKge,kBAAoB,GAKzBhe,KAAKie,WAAY,EACjBje,KAAKI,GAAKyd,IACV7d,KAAKke,oBAAsB,KAC3Ble,KAAKoc,UAAY,KACjBpc,KAAKuF,iBAAkB,EAEvBvF,KAAKme,kBAAoB,GACzBne,KAAKoe,qBAAuB,GAC5Bpe,KAAKqe,QAAU,GACfre,KAAKse,SAAW,GAChBte,KAAKue,QAAS,EAKdve,KAAKmG,aAAe,GACpBnG,KAAK2d,cAAgB,GACrB,IAAIjd,EAAOiJ,EAAKjJ,KAChB,IAAKA,EAAM,CACP,MAAM8d,EAASxe,KAAK6Z,eACpBnZ,EAAO,KAA0B8d,GAAU,KAAM,OAAOA,EAC5D,CAGA,GAFAxe,KAAKU,KAAOA,EACZV,KAAKye,WAA+B,MAAlB9U,EAAKqP,WAA2BrP,EAAKqP,UAChC,MAAnBrP,EAAKuS,YAA8C,MAAxBvS,EAAKwS,gBAAyB,CAKzD,IAAIA,EACJ,GAA4B,MAAxBxS,EAAKwS,gBACLA,EAAkBxS,EAAKwS,qBAEtB,GAAuB,MAAnBxS,EAAKuS,WAAoB,CAC9B,IAAIhT,EAAY,KACM,MAAlBS,EAAKT,YACLA,EAAYS,EAAKT,WAErBiT,EAAkB,CAACjT,GAAW8B,OAAOrB,EAAKuS,WAC9C,CACAlc,KAAKmc,gBAAkBA,EAEvB,IAAIvb,EAAQ+I,EAAK/I,MACJ,MAATA,IACAA,EAAQ+I,EAAK+U,YAEJ,MAAT9d,IACAA,EAAQ,WAEZZ,KAAKY,MAAQA,CACjB,CACoB,MAAhB+I,EAAKkP,QACL7Y,KAAK2e,eAAiBhV,EAAKkP,QAG3B7Y,KAAK2e,eAAiB,KAI1B3e,KAAK8c,UAAY,KACjB9c,KAAK4e,2BAA4B,CACrC,CAUA,cAAOC,CAAQnJ,EAAOrP,GAClB,OAAOqP,EAAMhV,KAAO,OAAS2F,EAAU2V,UAC3C,CAQA,cAAA8C,CAAezY,EAAW0Y,GACtB,GAAiC,IAA7B/e,KAAKmG,aAAa7C,OAClB,MAAM,IAAI,KACN,2DAA2Byb,MAEnC,GAAI/e,KAAKmG,aAAa7C,QAAU+C,EAC5B,MAAM,IAAI,KAAW,gBAAgB0Y,aAAoB1Y,6BAC3BrG,KAAKmG,aAAa7C,yBAEpD,OAAOtD,KAAKmG,aAAaE,EAC7B,CAUA,UAAA2Y,CAAW3Y,GACP,OAAO,KAA+BrG,KAAK8e,eAAezY,EAAW,SAASsW,aAClF,CAUA,WAAApW,CAAYF,GACR,OAAO,KAA+BrG,KAAK8e,eAAezY,EAAW,UAAUjB,cACnF,CAaA,SAAIJ,GACA,GAAIhF,KAAKmG,aAAa7C,OAAS,EAC3B,MAAM,IAAI,KAAe,SAAStD,KAAKU,6HAMtC,GAAiC,IAA7BV,KAAKmG,aAAa7C,OACvB,MAAM,IAAI,KAAe,SAAStD,KAAKU,8CAG3C,OAAO,KAA+BV,KAAK8e,eAAe,EAAG,SAASnC,aAC1E,CAYA,UAAIvW,GACA,GAAiC,IAA7BpG,KAAKmG,aAAa7C,OAClB,MAAM,IAAI,KAAe,SAAStD,KAAKU,8BAG3C,GAAIV,KAAKmG,aAAa7C,OAAS,EAC3B,MAAM,IAAI,KAAe,SAAStD,KAAKU,+HAM3C,OAAO,KAA+BV,KAAK8e,eAAe,EAAG,UAAU1Z,cAC3E,CACA,UAAIiD,GACA,OAAOrI,KAAKqe,OAChB,CAMA,eAAApG,GAKI,OAAOjY,KAAKqI,OAAO/F,KAAI2c,GAAUA,KACrC,CACA,WAAIC,GACA,OAAOlf,KAAKse,QAChB,CACA,SAAItM,GACA,OAAOhS,KAAKue,MAChB,CACA,SAAIvM,CAAMA,GACNhS,KAAKue,OAASvM,CAClB,CACA,aAAIgH,GACA,OAAOhZ,KAAKye,UAChB,CACA,aAAIzF,CAAUA,GACVhZ,KAAKme,kBAAkBna,SAAQmb,GAAKA,EAAEnG,UAAYA,IAClDhZ,KAAKye,WAAazF,CACtB,CACA,oBAAI1E,GACA,OAAItU,KAAKye,WACEze,KAAKme,kBAAkBiB,QAAOD,GAAKA,EAAEnG,YAGrC,EAEf,CACA,oBAAI1E,CAAiBuE,GACjB7Y,KAAKme,kBAAoBtF,CAC7B,CACA,uBAAIwG,GACA,OAAIrf,KAAKgZ,UACEhZ,KAAKme,kBAAkBiB,QAAOD,IAAMA,EAAEnG,YACxChO,OAAOhL,KAAKoe,sBAGVpe,KAAKme,kBAAkBnT,OAAOhL,KAAKoe,qBAElD,CACA,uBAAIiB,CAAoBxG,GACpB7Y,KAAKoe,qBAAuBvF,CAChC,CAKA,WAAIA,GACA,OAAO7Y,KAAKsU,iBAAiBtJ,OAAOhL,KAAKqf,oBAC7C,CACA,YAAIla,GACA,OAAOnF,KAAKie,SAChB,CAQA,WAAAqB,GACI,IAAKtf,KAAKmF,SACN,MAAM,IAAIyB,MAAM,uEAGxB,CAaA,wBAAA2Y,CAAyBta,GAErB,GADAA,EAAS,KAAqBA,GACR,MAAlBjF,KAAKoc,WAA+C,IAA1Bpc,KAAKoc,UAAU9Y,OACzC,OAEJ,MAAM8Y,EAAY,KAAqBpc,KAAKoc,WAC5C,GAAInX,EAAO3B,SAAW8Y,EAAU9Y,OAC5B,MAAM,IAAI,KAAW,SAAStD,KAAKU,gBAAgB0b,EAAU9Y,kCACtC2B,EAAO3B,yCACP2B,KAE3B,IAAK,IAAIua,EAAa,EAAGA,EAAava,EAAO3B,OAAQkc,IAAc,CAC/D,MAAMpR,EAAInJ,EAAOua,GACXC,EAAOrD,EAAUoD,GACvB,GAAY,MAARC,EACA,SAGJ,MAAMvC,EAAO9O,EAAEvG,KACf,GAAiB,MAAb4X,EAAKvC,MACDA,IAASuC,EAAKvC,KACd,MAAM,IAAI,KAAW,SAASsC,gCAAyCxf,KAAKU,uBACvD+e,EAAKvC,oBAAoBA,KAGtD,GAAoB,MAAhBuC,EAAKtC,SACDD,EAAOuC,EAAKtC,QACZ,MAAM,IAAI,KAAW,SAASqC,gCAAyCxf,KAAKU,2BACjD+e,EAAKtC,uBAAuBD,KAG/D,GAAoB,MAAhBuC,EAAKrC,SACDF,EAAOuC,EAAKrC,QACZ,MAAM,IAAI,KAAW,SAASoC,gCAAyCxf,KAAKU,2BACjD+e,EAAKrC,uBAAuBF,MAI/D,GAAkB,MAAduC,EAAK7e,OACDwN,EAAExN,QAAU6e,EAAK7e,MACjB,MAAM,IAAI,KAAW,SAAS4e,gCAAyCxf,KAAKU,yBACpD+e,EAAK7e,sBAAsBwN,EAAExN,UAI7D,GAAI6e,EAAKpC,KAAM,CACX,MAAMqC,EAAStR,EAAE5G,MACjB,IAAK,MAAMjH,KAAOkf,EAAKpC,KAAM,CACzB,MAAM3V,EAAOsC,OAAOzJ,GACdC,EAAQif,EAAKpC,KAAK9c,GAIlBof,EAAejY,GAAQ,EAAIgY,EAAOhY,GAAQgY,EAAOA,EAAOpc,OAASoE,GACvE,GAAa,MAATlH,IAA0D,IAAzC,CAACA,EAAO,MAAMmC,QAAQgd,GACvC,MAAM,IAAI,KAAW,SAASH,gCACvBxf,KAAKU,uBAAuBgH,kCACjBlH,mBAAuBkf,KAEjD,CACJ,CAEA,GAAkB,MAAdD,EAAKjY,MACL,IAAK,IAAIlD,EAAI,EAAGA,EAAImb,EAAKjY,MAAMlE,SAAUgB,EAAG,CACxC,MAAMsb,EAAUH,EAAKjY,MAAMlD,GACrB+M,EAAMjD,EAAE5G,MAAMlD,GACpB,GAAe,MAAXsb,GAA0B,MAAPvO,GACfuO,IAAYvO,EACZ,MAAM,IAAI,KAAW,SAASmO,gCACvBxf,KAAKU,wBAAwB+e,EAAKjY,sBACtB4G,EAAE5G,SAGjC,CAER,CACJ,CASA,IAAAqY,CAAK5a,EAAQnD,GACT,OAAOmD,CACX,CACA,cAAA6a,CAAe7a,EAAQnD,GACG,MAAlB9B,KAAK+d,WACL/d,KAAK+d,UAAU9Y,EAAQnD,EAE/B,CAMA,WAAAie,CAAYC,GACRhgB,KAAK+d,UAAYiC,CACrB,CAKA,aAAAC,GACIjgB,KAAK+d,UAAY,IACrB,CAsEA,KAAA1Y,CAAMJ,EAAQnD,GACVA,EAASA,GAAU,CAAC,EACpB9B,KAAKkgB,oBAEL,MAAMC,EAAa,KAAqBlb,GACxC,IAAImb,GAAiB,EACrB,IAAK,MAAMpb,KAASmb,EAChB,KAAMnb,aAAiBsY,GAAiB,CACpC8C,GAAiB,EACjB,KACJ,CAEJ,IAAIC,GAAkB,EACtB,IAAK,MAAMrb,KAASmb,EAChB,GAAInb,aAAiBsY,EAAgB,CACjC+C,GAAkB,EAClB,KACJ,CAEJ,GAAID,IAAmBC,EACnB,MAAM,IAAI,KAAW,mEAIzB,OAAO,QAAUrgB,KAAKU,MAAM,KAExB,IAAKV,KAAKgS,MAAO,CAKbhS,KAAKuf,yBAAyBta,GAE9B,MAAM4X,EAAc,GACpB,IAAK,MAAMyD,KAAS,KAAqBrb,GACrC4X,EAAYja,KAAK0d,EAAM9Y,OAE3BxH,KAAKugB,MAAM,KAA+B1D,IAC1C7c,KAAKgS,OAAQ,EAEThS,KAAK2e,gBACL3e,KAAKwgB,WAAWxgB,KAAK2e,gBAEF,OAAnB3e,KAAK8c,WAAsBuD,IAI3BrgB,KAAK8c,UAAY,EAEzB,CASA,GAJA9c,KAAKuf,yBAAyBta,GAI1Bob,EAAiB,CACjB,IAAIja,EAASpG,KAAK6f,KAAK5a,EAAQnD,GAI/B,MAAM2e,EAAa,KAAqBra,GAClCsa,EAAiB,GAGvB,IAAK,IAAItS,KAAKqS,GACqB,IAA3BN,EAAWxd,QAAQyL,KACnBA,EAAIA,EAAE3G,SAEViZ,EAAe9d,KAAKwL,GAGxB,GADAhI,EAAS,KAA+Bsa,GACR,MAA5B1gB,KAAKke,oBACL,MAAM,IAAI,KAAoB,qFAIlC,OAAO9X,CACX,CACK,CACD,MAAM8V,EAudtB,SAA2BS,GACvBA,EACI,KAAqBA,GACzB,MAAM3L,EAAS,GACf,IAAK,MAAM5C,KAAKuO,EACZ3L,EAAOpO,KAAKwL,EAAE5G,OAElB,OAAO,KAA+BwJ,EAC1C,CA/dmC2P,CAAkB1b,GAC/B+O,EAAchU,KAAK4gB,mBAAmB1E,GAC5C,IAAI9V,EACJ,MAAMya,EAueX,UAldK,GApBA7gB,KAAK8gB,6BAA6B5e,MAAMC,QAAQ8C,GAAUiX,EAAW,GACjEA,GAIA9V,EAHe,MAAf4N,GAAuBA,EAAY1Q,OAAS,GAC5CpB,MAAMC,QAAQ6R,EAAY,IAEjBA,EACJ1R,KAAI,CAACkF,EAAO5B,IAAU,IAAI0X,EAAeuD,EAAarZ,EAAOxH,KAAM,KAAqBiF,GAASnD,EAAQ9B,KAAKU,KAAMkF,KAGhH,IAAI0X,EAAeuD,EAAa7M,EAAahU,KAAM,KAAqBiF,GAASnD,EAAQ9B,KAAKU,MAS3GV,KAAK+gB,eAAe9b,EAAQmB,EAAQ,KAAM,KAAM8V,EAAYlI,EAAalS,GACzE9B,KAAK8c,YAC2B,MAA5B9c,KAAKke,oBACL,MAAM,IAAI,KAAoB,qFAGlC,OAAO9X,CACX,IAER,CAQA,4BAAA0a,CAA6B5E,GACzB,GAA4B,MAAxBlc,KAAKmc,gBAGJ,GAAID,EAAW5Y,SAAWtD,KAAKmc,gBAAgB7Y,YAM/C,CACD,IAAI0d,GAAc,EAClBhhB,KAAKmc,gBAAgBnY,SAAQ,CAACid,EAAW3c,KACpB,MAAb2c,GAAsC,MAAjB/E,EAAW5X,IAChC4X,EAAW5X,KAAO2c,IAClBD,GAAc,EAClB,GAQR,CACJ,CAaA,eAAIhN,GACA,GAAyB,MAArBhU,KAAKmG,cAAqD,IAA7BnG,KAAKmG,aAAa7C,OAC/C,MAAM,IAAI,KAAe,aAAatD,KAAKU,oEAG/C,MAAMwgB,EAAkB,GACxB,IAAK,MAAMC,KAAQnhB,KAAKmG,aAAc,CAClC,MAAMib,EAAcva,KAAKC,UAAUqa,EAAK/K,eACM,IAA1C8K,EAAgBve,QAAQye,IACxBF,EAAgBte,KAAKwe,EAE7B,CACA,GAA+B,IAA3BF,EAAgB5d,OAAc,CAC9B,MAAM8S,EAAepW,KAAKmG,aAAa,GAAGiQ,aAC1C,OAAIlU,MAAMC,QAAQiU,IAAiBlU,MAAMC,QAAQiU,EAAa,KAClC,IAAxBA,EAAa9S,OACN8S,EAAa,GAGbA,CAEf,CAEI,MAAM,IAAI,KAAe,aAAapW,KAAKU,iIAKnD,CAWA,WAAA2gB,GACI,IAAKrhB,KAAKgS,MACN,MAAM,IAAI,KAAa,sCAAsChS,KAAKU,2FAItE,OAAO,IAAoCV,KAAK6Y,QACpD,CAYA,KAAA0H,CAAMrE,GACFlc,KAAKgS,OAAQ,CACjB,CASA,UAAA+G,CAAWH,GAAgB,GACvB,OAAO,QAAcA,EAAgB5Y,KAAKsU,iBAAmBtU,KAAK6Y,QACtE,CAaA,UAAA2H,CAAW3H,IACP,IAAAtR,OAAK,KACD,MAAM+Z,EAASthB,KAAK6Y,QACpB,GAAIyI,EAAOhe,SAAWuV,EAAQvV,OAK1B,MAAM,IAAI,KAAW,4CAA4CtD,KAAKU,sCAClCmY,EAAQvV,uCACTge,EAAOhe,qCACjBuV,QAE7B,GAAsB,IAAlByI,EAAOhe,OACP,OAEJ,MAAMie,EAAoB,GACpBC,GAAc,QAAcF,GAClC,IAAK,IAAIhd,EAAI,EAAGA,EAAIkd,EAAYle,SAAUgB,EAAG,CACzC,MAAMmd,EAAKD,EAAYld,GACjBod,EAAIJ,EAAOhd,GACX6a,EAAItG,EAAQvU,GAClB,IAAK,EAAAjB,KAAA,YAAiBoe,EAAGja,MAAO2X,EAAE3X,OAC9B,MAAM,IAAI,KAAW,sBAAsBia,EAAGja,mDACG2X,EAAE3X,SAEvD+Z,EAAkB3e,KAAK,CAAC8e,EAAGvC,GAC/B,EACA,QAAcoC,EAAkB,GAExC,CAgBA,SAAAI,CAAUjhB,EAAM8G,EAAO5G,EAAOghB,EAAaC,EAAa7I,EAAW8I,GAE/D,IAA8C,IAA1C9hB,KAAKge,kBAAkBrb,QAAQjC,GAC/B,MAAM,IAAI,KAAW,yBAAyBA,eAAkBV,KAAKU,QAEzEV,KAAKge,kBAAkBpb,KAAKlC,GACf,MAATE,IACAA,EAAQ,WAERZ,KAAK4e,4BACLgD,GAAc,QAAe,UAEjC,MAAMG,EAAYH,EAAYvc,MAAMmC,EAAO5G,GACrCohB,EAAS,IAAI,KAAcD,EAAWnhB,EAAOF,EAAMsY,EAAW8I,GAepE,OAdAC,EAAUvgB,UAES,MAAfqgB,GACA7hB,KAAKiiB,SAAQ,IAAMJ,EAAYxc,MAAM2c,EAAOpK,UAE/B,MAAboB,IACAA,GAAY,GAEZA,EACAhZ,KAAKme,kBAAkBvb,KAAKof,GAG5BhiB,KAAKoe,qBAAqBxb,KAAKof,GAE5BA,CACX,CAWA,4BAAAE,CAA6B1hB,GACzBR,KAAK4e,0BAA4Bpe,CACrC,CASA,OAAAyhB,CAAQ5Z,GACU,MAAVA,GAAkBnG,MAAMC,QAAQkG,IAA6B,IAAlBA,EAAO/E,SAItD+E,EAAS,KAAqBA,QACT8Z,IAAjBniB,KAAKqe,SAA0C,OAAjBre,KAAKqe,SACnCre,KAAKqI,OAAOzF,QAAQyF,GAE5B,CAYA,kBAAAuY,CAAmB1E,GACf,OAAOA,CACX,CAUA,WAAA1W,CAAYP,EAAQxE,GAChB,IAAKT,KAAKuF,gBAAiB,CACvB,GAAY,MAAR9E,EAAc,CACd,IAAIyB,MAAMC,QAAQ1B,GASd,MAAM,IAAI6S,UAAU,SAAStT,KAAKU,+DARlCD,EAAKuD,SAAQoe,IACT,GAAmB,MAAfA,EACA,MAAM,IAAI9O,UAAU,SAAStT,KAAKU,8DAEtC,GAOZ,CAEA,OAAO,IACX,CAGA,OAAOD,CACX,CAaA,cAAAsgB,CAAepE,EAAcvX,EAAeP,EAAY+X,EAAaC,EAAazG,EAActU,EAAS,MACrG,MAAMugB,EAAkB,KAAqB1F,GAC7CvX,EAAgB,KAAqBA,GACrCP,EAAa,KAAqBA,GAClC+X,EAAc,KAAqBA,GACnCC,EAAc,KAA+BA,GAC7CzG,EAAe,KAA+BA,GAE9C,MAAMoG,EAAgB,GAChBC,EAAc,GACdC,EAAgB,GACtB,IAAK,MAAMtO,KAAKiU,EAKZ7F,EAAc5Z,KAAKwL,EAAEzJ,aACrB8X,EAAY7Z,KAAKwL,EAAE/H,WACnBqW,EAAc9Z,KAAKwL,EAAEkO,aAKzB,IAAIoB,EAAK,CACLnB,cAAevc,KACfwc,gBACAC,cACAC,gBACAC,aAAc0F,EACdjd,gBACAP,aACA+X,cACAC,cACAzG,gBACDtU,GAEH,IAAK,IAAIwC,EAAI,EAAGA,EAAIc,EAAc9B,OAAQgB,IAEtCc,EAAcd,GAAGK,YAAc3E,KAC/BoF,EAAcd,GAAG+B,UAAYrG,KAAKmG,aAAa7C,OAAS,EACxD8B,EAAcd,GAAGgY,YAAchY,CAEvC,CAsBA,SAAAwV,GACI,MAAMpB,EAAS,CAAEhY,KAAMV,KAAKU,KAAMsY,UAAWhZ,KAAKgZ,WAOlD,OAN4B,MAAxBhZ,KAAKmc,kBACLzD,EAAwB,gBAAI1Y,KAAKmc,iBAEnB,MAAdnc,KAAKY,QACL8X,EAAc,MAAI1Y,KAAKY,OAEpB8X,CACX,CAMA,cAAA4J,GAEI,OADAtiB,KAAK6Y,QAAQ7U,SAAQge,GAAUA,EAAOxgB,YAC/BxB,KAAK6Y,QAAQvV,MACxB,CACA,iBAAA4c,GACI,GAAuB,IAAnBlgB,KAAK8c,UACL,MAAM,IAAIlW,MAAM,UAAU5G,KAAKU,6BAEvC,CA+BA,OAAAc,GACI,IAAKxB,KAAKgS,MACN,MAAM,IAAIpL,MAAM,wBAAwB5G,KAAKU,2CAGjD,GAAuB,OAAnBV,KAAK8c,UACL,MAAM,IAAIlW,MAAM,wBAAwB5G,KAAKU,0CAGjDV,KAAKkgB,oBACL,IAAI7G,EAAuB,EAI3B,OAHyB,MAAnBrZ,KAAK8c,YACPzD,EAAuBrZ,KAAKsiB,kBAEzB,CAAEnJ,qBAAsBnZ,KAAK8c,UAAWzD,uBACnD,EA4CG,SAASkJ,EAAgB/R,EAAQkF,EAAOrP,GAK3C,IAJa,MAATqP,GAA+B,MAAbrP,GAAqBA,EAAY,KACnDqP,EAAQlF,EAAO7L,YACf0B,EAAYmK,EAAOnK,WAEW,IAA9BqP,EAAMvP,aAAa7C,OACnB,MAAO,CAACkN,GAEP,CACD,MAAM2Q,EAAOzL,EAAMvP,aAAaE,GAChC,GAAkC,IAA9B8a,EAAK3E,cAAclZ,OACnB,OAAO6d,EAAKxE,aAEX,CACD,MAAM6F,EAAgB,GACtB,IAAK,IAAIle,EAAI,EAAGA,EAAI6c,EAAK3E,cAAclZ,OAAQgB,IAAK,CAChD,MAGMme,EAAkBF,EAHdpB,EAAKxE,aAAarY,GACd6c,EAAK3E,cAAclY,GACf6c,EAAK1E,YAAYnY,IAGnC,IAAK,MAAM8J,KAAKqU,GACsB,IAA9BD,EAAc7f,QAAQyL,IACtBoU,EAAc5f,KAAKwL,EAG/B,CACA,OAAOoU,CACX,CACJ,CACJ,C","sources":["webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/engine/executor.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/engine/training_utils.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/engine/training_tensors.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/engine/input_layer.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/engine/topology.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\nimport { cast, dispose, memory, util } from '@tensorflow/tfjs-core';\nimport { ValueError } from '../errors';\nimport { toList } from '../utils/generic_utils';\nimport { InputLayer } from './input_layer';\nimport { SymbolicTensor } from './topology';\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\nfunction assertFeedCompatibility(key, val) {\n    // Check dtype compatibility.\n    if (key.dtype == null || key.dtype === val.dtype) {\n        //  a.  If types match, return val tensor as is.\n        return val;\n    }\n    try {\n        //  b. Attempt to convert to expected type.\n        return cast(val, key.dtype);\n    }\n    catch (err) {\n        //  c. If conversion fails, return helpful error.\n        throw new ValueError(`The dtype of the feed (${val.dtype}) can not be cast to the dtype ` +\n            `of the key '${key.name}' (${key.dtype}).`);\n    }\n}\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\nexport class FeedDict {\n    /**\n     * Constructor, optionally does copy-construction.\n     * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n     *   copy-construction will be performed.\n     */\n    constructor(feeds) {\n        this.id2Value = {};\n        this.id2Mask = {};\n        this.name2Id = {};\n        if (feeds instanceof FeedDict) {\n            for (const id in feeds.id2Value) {\n                this.id2Value[id] = feeds.id2Value[id];\n                if (id in feeds.id2Mask) {\n                    this.id2Mask[id] = feeds.id2Mask[id];\n                }\n            }\n        }\n        else {\n            if (feeds == null) {\n                return;\n            }\n            for (const feed of feeds) {\n                this.add(feed.key, feed.value);\n            }\n        }\n    }\n    /**\n     * Add a key-value pair to the FeedDict.\n     *\n     * @param key The key of the feed.\n     * @param value The value of the tensor feed.\n     * @param mask The value of the mask feed (optional).\n     * @returns This `FeedDict`.\n     * @throws ValueError: If the key `SymbolicTensor` already exists in the\n     *   `FeedDict`.\n     */\n    add(key, value, mask) {\n        if (this.id2Value[key.id] == null) {\n            this.id2Value[key.id] = assertFeedCompatibility(key, value);\n            this.name2Id[key.name] = key.id;\n            if (mask != null) {\n                this.id2Mask[key.id] = mask;\n            }\n        }\n        else {\n            throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);\n        }\n        return this;\n    }\n    /**\n     * Add a Feed to the FeedDict.\n     * @param feed The new `Feed` to add.\n     * @returns This `FeedDict`.\n     */\n    addFeed(feed) {\n        this.add(feed.key, feed.value);\n    }\n    /**\n     * Probe whether a key already exists in the FeedDict.\n     * @param key\n     */\n    hasKey(key) {\n        return this.id2Value[key.id] != null;\n    }\n    /**\n     * Get all the SymbolicTensor available in this FeedDict.\n     */\n    names() {\n        return Object.keys(this.name2Id);\n    }\n    /**\n     * Get the feed value for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed value.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n    getValue(key) {\n        if (key instanceof SymbolicTensor) {\n            if (this.id2Value[key.id] == null) {\n                throw new ValueError(`Nonexistent key: ${key.name}`);\n            }\n            else {\n                return this.id2Value[key.id];\n            }\n        }\n        else {\n            const id = this.name2Id[key];\n            if (id == null) {\n                throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n            }\n            return this.id2Value[id];\n        }\n    }\n    /**\n     * Get the feed mask for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed mask.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n    getMask(key) {\n        if (key instanceof SymbolicTensor) {\n            if (this.id2Value[key.id] == null) {\n                throw new ValueError(`Nonexistent key: ${key.name}`);\n            }\n            else {\n                return this.id2Mask[key.id];\n            }\n        }\n        else {\n            const id = this.name2Id[key];\n            if (id == null) {\n                throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n            }\n            return this.id2Mask[id];\n        }\n    }\n    /** Dispose all mask Tensors held by this object. */\n    disposeMasks() {\n        if (this.id2Mask != null) {\n            dispose(this.id2Mask);\n        }\n    }\n}\n// Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\nconst cachedSorted = {};\n// Cache for recipient count maps for given execution targets (i.e., fetches).\nconst cachedRecipientCounts = {};\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\nexport function execute(fetches, feedDict, kwargs, probe) {\n    const training = kwargs == null ? false : kwargs['training'];\n    const arrayFetches = Array.isArray(fetches);\n    const fetchArray = arrayFetches ? fetches : [fetches];\n    const outputNames = fetchArray.map(t => t.name);\n    const finalOutputs = [];\n    const feedNames = feedDict.names();\n    for (const outputName of outputNames) {\n        if (feedNames.indexOf(outputName) !== -1) {\n            finalOutputs.push(feedDict.getValue(outputName));\n        }\n        else {\n            finalOutputs.push(null);\n        }\n    }\n    if (probe != null) {\n        // For optional probing of memory footprint during execution.\n        probe.maxNumTensors = -Infinity;\n        probe.minNumTensors = Infinity;\n    }\n    // Check cache.\n    const fetchAndFeedKey = outputNames.join(',') + '|' + feedDict.names().join(',');\n    let sorted;\n    let recipientCounts;\n    if (cachedSorted[fetchAndFeedKey] == null) {\n        // Cache doesn't contain the desired combination of fetches. Compute\n        // topological sort for the combination for the first time.\n        const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n        sorted = out.sorted;\n        recipientCounts = out.recipientCounts;\n        // Store results in cache for future use.\n        cachedSorted[fetchAndFeedKey] = sorted;\n        cachedRecipientCounts[fetchAndFeedKey] = recipientCounts;\n    }\n    sorted = cachedSorted[fetchAndFeedKey];\n    recipientCounts = {};\n    if (!training) {\n        Object.assign(recipientCounts, cachedRecipientCounts[fetchAndFeedKey]);\n    }\n    const internalFeedDict = new FeedDict(feedDict);\n    // Start iterative execution on the topologically-sorted SymbolicTensors.\n    for (let i = 0; i < sorted.length; ++i) {\n        if (probe != null) {\n            // For optional probing of memory usage during execution.\n            const numTensors = memory().numTensors;\n            if (numTensors > probe.maxNumTensors) {\n                probe.maxNumTensors = numTensors;\n            }\n            if (numTensors < probe.minNumTensors) {\n                probe.minNumTensors = numTensors;\n            }\n        }\n        const symbolic = sorted[i];\n        const srcLayer = symbolic.sourceLayer;\n        if (srcLayer instanceof InputLayer) {\n            continue;\n        }\n        const inputValues = [];\n        const inputMasks = [];\n        const tensorsToDispose = [];\n        let maskExists = false;\n        for (const input of symbolic.inputs) {\n            const value = internalFeedDict.getValue(input);\n            const mask = internalFeedDict.getMask(input);\n            inputValues.push(value);\n            inputMasks.push(mask);\n            if (mask != null) {\n                maskExists = true;\n            }\n            if (!training) {\n                recipientCounts[input.name]--;\n                if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) &&\n                    outputNames.indexOf(input.name) === -1 && !value.isDisposed &&\n                    input.sourceLayer.stateful !== true) {\n                    tensorsToDispose.push(value);\n                }\n            }\n        }\n        if (maskExists) {\n            kwargs = kwargs || {};\n            kwargs['mask'] = inputMasks[0];\n        }\n        const outputTensors = toList(srcLayer.apply(inputValues, kwargs));\n        let outputMask = null;\n        if (srcLayer.supportsMasking) {\n            outputMask = srcLayer.computeMask(inputValues, inputMasks);\n        }\n        const layerOutputs = getNodeOutputs(symbolic);\n        const outputSymbolicTensors = Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n        for (let i = 0; i < outputSymbolicTensors.length; ++i) {\n            if (!internalFeedDict.hasKey(outputSymbolicTensors[i])) {\n                internalFeedDict.add(outputSymbolicTensors[i], outputTensors[i], Array.isArray(outputMask) ? outputMask[0] : outputMask);\n            }\n            const index = outputNames.indexOf(outputSymbolicTensors[i].name);\n            if (index !== -1) {\n                finalOutputs[index] = outputTensors[i];\n            }\n        }\n        if (!training) {\n            // Clean up Tensors that are no longer needed.\n            dispose(tensorsToDispose);\n        }\n    }\n    // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n    // tensors as we go, because these tensors are sometimes passed over a\n    // series of mutliple layers, i.e., not obeying the immediate input\n    // relations in the graph. If this becomes a memory-usage concern,\n    // we can improve this in the future.\n    internalFeedDict.disposeMasks();\n    return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\nfunction getTopologicalSortAndRecipientCounts(fetches, feedDict) {\n    util.assert(fetches != null && fetches.length > 0, () => `Expected at least one fetch, got none`);\n    let finalSorted = [];\n    let finalRecipientMap = {};\n    if (fetches.length === 1) {\n        // Special-casing 1 fetch for efficiency.\n        const out = getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n        finalSorted = out.sorted;\n        finalRecipientMap = out.recipientMap;\n    }\n    else {\n        const visited = new Set();\n        for (const fetch of fetches) {\n            const { sorted, recipientMap } = getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict);\n            // Merge sorted SymbolicTensor Arrays.\n            for (const symbolicTensor of sorted) {\n                if (!visited.has(symbolicTensor.name)) {\n                    finalSorted.push(symbolicTensor);\n                    visited.add(symbolicTensor.name);\n                }\n            }\n            // Merge recipient maps.\n            for (const name in recipientMap) {\n                if (finalRecipientMap[name] == null) {\n                    finalRecipientMap[name] = new Set();\n                }\n                recipientMap[name].forEach(recipient => finalRecipientMap[name].add(recipient));\n            }\n        }\n    }\n    return {\n        sorted: finalSorted,\n        recipientCounts: recipientMap2Counts(finalRecipientMap)\n    };\n}\nfunction recipientMap2Counts(recipientMap) {\n    const recipientCounts = {};\n    for (const name in recipientMap) {\n        recipientCounts[name] = recipientMap[name].size;\n    }\n    return recipientCounts;\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\nexport function getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict) {\n    const visited = new Set();\n    const sorted = [];\n    const recipientMap = {};\n    // Put keys of the feedDict into visited first, so they don't have to be\n    // walked. This is needed in case where there are feeds for intermediate\n    // SymbolicTensors of the graph.\n    for (const key of feedDict.names()) {\n        visited.add(key);\n    }\n    const stack = [];\n    const marks = [];\n    // Initial population of stack and marks.\n    stack.push(fetch);\n    while (stack.length > 0) {\n        const top = stack[stack.length - 1];\n        if (visited.has(top.name)) {\n            stack.pop();\n            continue;\n        }\n        const topIsMarked = marks[marks.length - 1] === stack.length - 1;\n        if (top.inputs.length === 0 || topIsMarked) {\n            // Input SymbolicTensor or all children have been visited.\n            stack.pop();\n            sorted.push(top);\n            visited.add(top.name);\n            if (topIsMarked) {\n                marks.pop();\n            }\n        }\n        else {\n            // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n            // been visited yet. Push them onto the stack.\n            marks.push(stack.length - 1);\n            for (const input of top.inputs) {\n                // Increment the recipient count. Note that this needs to happen\n                // regardless of whether the SymbolicTensor has been visited before.\n                if (recipientMap[input.name] == null) {\n                    recipientMap[input.name] = new Set();\n                }\n                recipientMap[input.name].add(top.name);\n                if (visited.has(input.name)) {\n                    continue; // Avoid repeated visits to the same SymbolicTensor.\n                }\n                stack.push(input);\n            }\n        }\n    }\n    return { sorted, recipientMap };\n}\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\nfunction getNodeOutputs(fetch) {\n    let layerOutputs;\n    if (fetch.sourceLayer.inboundNodes.length === 1) {\n        layerOutputs = fetch.sourceLayer.output;\n    }\n    else {\n        let nodeIndex = null;\n        for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n            for (const outputTensor of fetch.sourceLayer.inboundNodes[i]\n                .outputTensors) {\n                if (outputTensor.id === fetch.id) {\n                    nodeIndex = i;\n                    break;\n                }\n            }\n        }\n        layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n    }\n    return layerOutputs;\n}\n//# sourceMappingURL=executor.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport { dispose, mul, tensor1d, tidy } from '@tensorflow/tfjs-core';\nfunction standardizeSampleOrClassWeights(xWeight, outputNames, weightType) {\n    const numOutputs = outputNames.length;\n    if (xWeight == null || (Array.isArray(xWeight) && xWeight.length === 0)) {\n        return outputNames.map(name => null);\n    }\n    if (numOutputs === 1) {\n        if (Array.isArray(xWeight) && xWeight.length === 1) {\n            return xWeight;\n        }\n        else if (typeof xWeight === 'object' && outputNames[0] in xWeight) {\n            return [xWeight[outputNames[0]]];\n        }\n        else {\n            return [xWeight];\n        }\n    }\n    if (Array.isArray(xWeight)) {\n        if (xWeight.length !== numOutputs) {\n            throw new Error(`Provided ${weightType} is an array of ${xWeight.length} ` +\n                `element(s), but the model has ${numOutputs} outputs. ` +\n                `Make sure a set of weights is provided for each model output.`);\n        }\n        return xWeight;\n    }\n    else if (typeof xWeight === 'object' && Object.keys(xWeight).length > 0 &&\n        typeof xWeight[Object.keys(xWeight)[0]] ===\n            'object') {\n        const output = [];\n        outputNames.forEach(outputName => {\n            if (outputName in xWeight) {\n                output.push(xWeight[outputName]);\n            }\n            else {\n                output.push(null);\n            }\n        });\n        return output;\n    }\n    else {\n        throw new Error(`The model has multiple (${numOutputs}) outputs, ` +\n            `so ${weightType} must be either an array with ` +\n            `${numOutputs} elements or an object with ${outputNames} keys. ` +\n            `Provided ${weightType} not understood: ${JSON.stringify(xWeight)}`);\n    }\n}\n/**\n * Standardize class weighting objects.\n *\n * This function takes a single class-weighting object, an array of them,\n * or a map from output name to class-weighting object. It compares it to the\n * output name(s) of the model, base on which it outputs an array of\n * class-weighting objects of which the length matches the number of outputs.\n *\n * @param classWeight Input class-weighting object(s).\n * @param outputNames All output name(s) of the model.\n * @return An array of class-weighting objects. The length of the array matches\n *   the model's number of outputs.\n */\nexport function standardizeClassWeights(classWeight, outputNames) {\n    return standardizeSampleOrClassWeights(classWeight, outputNames, 'classWeight');\n}\nexport function standardizeSampleWeights(classWeight, outputNames) {\n    return standardizeSampleOrClassWeights(classWeight, outputNames, 'sampleWeight');\n}\n/**\n * Standardize by-sample and/or by-class weights for training.\n *\n * Note that this function operates on one model output at a time. For a model\n * with multiple outputs, you must call this function multiple times.\n *\n * @param y The target tensor that the by-sample and/or by-class weight is for.\n *     The values of y are assumed to encode the classes, either directly\n *     as an integer index, or as one-hot encoding.\n * @param sampleWeight By-sample weights.\n * @param classWeight By-class weights: an object mapping class indices\n *     (integers) to a weight (float) to apply to the model's loss for the\n *     samples from this class during training. This can be useful to tell the\n *     model to \"pay more attention\" to samples from an under-represented class.\n * @param sampleWeightMode The mode for the sample weights.\n * @return A Promise of weight tensor, of which the size of the first dimension\n *     matches that of `y`.\n */\nexport async function standardizeWeights(y, sampleWeight, classWeight, sampleWeightMode) {\n    if (sampleWeight != null || sampleWeightMode != null) {\n        // TODO(cais): Once 'temporal' mode is implemented, document it in the doc\n        // string.\n        throw new Error('Support sampleWeight is not implemented yet');\n    }\n    if (classWeight != null) {\n        // Apply class weights per sample.\n        const yClasses = tidy(() => {\n            if (y.shape.length === 1) {\n                // Assume class indices.\n                return y.clone();\n            }\n            else if (y.shape.length === 2) {\n                if (y.shape[1] > 1) {\n                    // Assume one-hot encoding of classes.\n                    const axis = 1;\n                    return y.argMax(axis);\n                }\n                else if (y.shape[1] === 1) {\n                    // Class index.\n                    return y.reshape([y.shape[0]]);\n                }\n                else {\n                    throw new Error(`Encountered unexpected last-dimension size (${y.shape[1]}) ` +\n                        `during handling of class weights. The size is expected to be ` +\n                        `>= 1.`);\n                }\n            }\n            else {\n                throw new Error(`Unexpected rank of target (y) tensor (${y.rank}) during ` +\n                    `handling of class weights. The rank is expected to be 1 or 2.`);\n            }\n        });\n        const yClassIndices = Array.from(await yClasses.data());\n        dispose(yClasses);\n        const classSampleWeight = [];\n        yClassIndices.forEach(classIndex => {\n            if (classWeight[classIndex] == null) {\n                throw new Error(`classWeight must contain all classes in the training data. ` +\n                    `The class ${classIndex} exists in the data but not in ` +\n                    `classWeight`);\n            }\n            else {\n                classSampleWeight.push(classWeight[classIndex]);\n            }\n        });\n        return tensor1d(classSampleWeight, 'float32');\n    }\n    else {\n        return null;\n    }\n}\n/**\n * Apply per-sample weights on the loss values from a number of samples.\n *\n * @param losses Loss tensor of shape `[batchSize]`.\n * @param sampleWeights Per-sample weight tensor of shape `[batchSize]`.\n * @returns Tensor of the same shape as`losses`.\n */\nexport function computeWeightedLoss(losses, sampleWeights) {\n    return mul(losses, sampleWeights);\n}\n//# sourceMappingURL=training_utils.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils';\n// Default batch size used during tensor-based validation.\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n    let xs;\n    let ys;\n    const iteratorOutObj = iteratorOut;\n    xs = iteratorOutObj['xs'];\n    ys = iteratorOutObj['ys'];\n    tfc.util.assert(xs != null && ys != null, () => 'A Dataset iterator for fitDataset() is expected to generate ' +\n        'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +\n        'values may be `tf.Tensor`, an array of Tensors, or a map of ' +\n        'string to Tensor.  The provided Dataset instead generates ' +\n        `${iteratorOut}`);\n    const flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n    const flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n    const batchSize = flattenedXs[0].shape[0];\n    tfc.util.assert(flattenedXs.length === model.inputs.length, () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` +\n        `provides ${flattenedXs.length} inputs.  (Expected input keys: ` +\n        `${JSON.stringify(model.inputNames)})`);\n    tfc.util.assert(flattenedYs.length === model.outputs.length, () => `LayersModel has ${model.outputs.length} outputs, but the dataset ` +\n        `provides ${flattenedYs.length} outputs.  (Expected output keys: ` +\n        `${JSON.stringify(model.outputNames)})`);\n    for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n        tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, () => `Batch size mismatch: input ` +\n            `${model.inputNames[xIndex]} has ${flattenedXs[xIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n    }\n    for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n        tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, () => `Batch size mismatch: output ` +\n            `${model.outputNames[yIndex]} has ${flattenedYs[yIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n    }\n    return { xs: flattenedXs, ys: flattenedYs };\n}\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n    if (values instanceof tfc.Tensor) {\n        return [values];\n    }\n    else if (Array.isArray(values)) {\n        tfc.util.assert(values.length === names.length, () => `Received an array of ${values.length} Tensors, but expected ${names.length} to match the ${inputOrOutput} keys ${names}.`);\n        return values;\n    }\n    else {\n        const result = [];\n        // Check that all the required keys are available.\n        for (const name of names) {\n            if (values[name] == null) {\n                throw new ValueError(`The feature data generated by the dataset lacks the required ` +\n                    `${inputOrOutput} key '${name}'.`);\n            }\n            result.push(values[name]);\n        }\n        return result;\n    }\n}\nfunction standardizeTensorValidationData(data) {\n    if (data.length === 3) {\n        throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n    }\n    return { xs: data[0], ys: data[1] };\n}\nexport async function fitDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n    tfc.util.assert(model.optimizer != null, () => 'You must compile a model before training/testing. Use ' +\n        'LayersModel.compile(modelCompileConfig).');\n    tfc.util.assert(args != null, () => `For fitDataset(), the 2nd argument (config) is required, ` +\n        `but it is not provided in this call.`);\n    tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), () => `For fitDataset(), config.epochs is expected to be a positive ` +\n        `integer, but got ${args.epochs}`);\n    tfc.util.assert(!hasBatchesPerEpoch ||\n        (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)), () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` +\n        `positive integer if specified, but got ${args.batchesPerEpoch}`);\n    tfc.util.assert(\n    // tslint:disable-next-line:no-any\n    args['validationSplit'] == null, () => '`validationSplit` is not supported by `fitDataset()`. ' +\n        'Use validationData instead.');\n    if (model.isTraining) {\n        throw new Error('Cannot start training because another fit() call is ongoing.');\n    }\n    model.isTraining = true;\n    try {\n        const doValidation = args.validationData != null;\n        let valXs;\n        let valYs;\n        if (doValidation) {\n            if (isDatasetObject(args.validationData)) {\n                tfc.util.assert(args.validationBatches == null ||\n                    (args.validationBatches > 0 &&\n                        Number.isInteger(args.validationBatches)), () => `For fitDataset() with dataset-based validation, ` +\n                    `config.validationBatches is expected not to be provided, ` +\n                    `or to be a positive integer, ` +\n                    `but got ${args.validationBatches}`);\n            }\n            else {\n                const validationData = standardizeTensorValidationData(args.validationData);\n                valXs = validationData.xs;\n                valYs = validationData.ys;\n            }\n        }\n        const trainFunction = model.makeTrainFunction();\n        const outLabels = model.getDedupedMetricsNames();\n        let callbackMetrics;\n        if (doValidation) {\n            callbackMetrics =\n                outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n        }\n        else {\n            callbackMetrics = outLabels.slice();\n        }\n        const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n        const verbose = args.verbose == null ? 1 : args.verbose;\n        const { callbackList, history } = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n        doValidation, callbackMetrics);\n        callbackList.setModel(model);\n        model.history = history;\n        await callbackList.onTrainBegin();\n        model.stopTraining_ = false;\n        let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n        let dataIterator = await dataset.iterator();\n        while (epoch < args.epochs) {\n            const epochLogs = {};\n            await callbackList.onEpochBegin(epoch);\n            let stepsDone = 0;\n            let batchIndex = 0;\n            if (!hasBatchesPerEpoch) {\n                dataIterator = await dataset.iterator();\n            }\n            while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n                const iteratorOut = await dataIterator.next();\n                // If `batchesPerEpoch` is specified, the dataset should not be\n                // exhausted until all epoches are done.\n                if (hasBatchesPerEpoch && iteratorOut.done) {\n                    console.warn('You provided `batchesPerEpoch` as ' +\n                        `${args.batchesPerEpoch}, ` +\n                        'but your dataset iterator ran out of data after ' +\n                        `${stepsDone} batches; ` +\n                        'interrupting training. Make sure that your ' +\n                        'dataset can generate at least `batchesPerEpoch * epochs` ' +\n                        'batches (in this case, ' +\n                        `${args.batchesPerEpoch * args.epochs} batches). ` +\n                        'You may need to use the repeat() function when building ' +\n                        'your dataset.');\n                    break;\n                }\n                if (iteratorOut.value != null) {\n                    const { xs, ys } = standardizeDataIteratorOutput(model, iteratorOut.value);\n                    const batchLogs = {};\n                    batchLogs['batch'] = batchIndex;\n                    batchLogs['size'] = xs[0].shape[0];\n                    await callbackList.onBatchBegin(batchIndex, batchLogs);\n                    const sampleWeights = [];\n                    if (args.classWeight != null) {\n                        const standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n                        for (let i = 0; i < standardClassWeights.length; ++i) {\n                            sampleWeights.push(await standardizeWeights(ys[i], null, standardClassWeights[i]));\n                        }\n                    }\n                    // Train on batch.\n                    const ins = xs.concat(ys).concat(sampleWeights);\n                    const outs = trainFunction(ins);\n                    tfc.dispose(ins);\n                    for (let i = 0; i < outLabels.length; ++i) {\n                        const label = outLabels[i];\n                        const out = outs[i];\n                        batchLogs[label] = out;\n                        tfc.keep(out);\n                    }\n                    await callbackList.onBatchEnd(batchIndex, batchLogs);\n                    disposeTensorsInLogs(batchLogs);\n                    batchIndex++;\n                    stepsDone++;\n                }\n                if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :\n                    iteratorOut.done) {\n                    // Epoch finished. Perform validation.\n                    if (doValidation) {\n                        let valOuts;\n                        if (isDatasetObject(args.validationData)) {\n                            valOuts = toList(await model.evaluateDataset(args.validationData, { batches: args.validationBatches }));\n                        }\n                        else {\n                            valOuts = toList(model.evaluate(valXs, valYs, {\n                                batchSize: args.validationBatchSize == null ?\n                                    DEFAULT_VALIDATION_BATCH_SIZE :\n                                    args.validationBatchSize,\n                                verbose: 0\n                            }));\n                        }\n                        for (let i = 0; i < model.metricsNames.length; ++i) {\n                            epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n                        }\n                    }\n                    // Call `break` to exit one epoch lopp after validation is done. If\n                    // config.batchesPerEpoch is specified, an epoch while loop will\n                    // stop when `stepsDone >= config.batchesPerEpoch`. When\n                    // config.batchesPerEpoch is not provided, the following `break` is\n                    // required to exit the while lopp after dataset is exhausted.\n                    break;\n                }\n                if (model.stopTraining_) {\n                    break;\n                }\n            }\n            await callbackList.onEpochEnd(epoch, epochLogs);\n            epoch++;\n            if (model.stopTraining_) {\n                break;\n            }\n        }\n        await callbackList.onTrainEnd();\n        await model.history.syncData();\n        return model.history;\n    }\n    finally {\n        model.isTraining = false;\n    }\n}\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction getStepsPerEpoch(dataset, args) {\n    // Attempt to determine # of batches in an epoch.\n    let stepsPerEpoch = null;\n    if (args.batchesPerEpoch != null) {\n        stepsPerEpoch = args.batchesPerEpoch;\n    }\n    else if (Number.isFinite(dataset.size)) {\n        stepsPerEpoch = dataset.size;\n    }\n    return stepsPerEpoch;\n}\n// Check if provided object is a Dataset object by checking its .iterator\n// element.\nfunction isDatasetObject(dataset) {\n    return (typeof dataset.iterator === 'function');\n}\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject(iterator) {\n    return (typeof iterator.next === 'function');\n}\nexport async function evaluateDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    args = args || {};\n    const hasBatches = args.batches != null;\n    const f = model.testFunction;\n    let outs = [];\n    if (args.verbose > 0) {\n        throw new NotImplementedError('Verbose mode is not implemented yet.');\n    }\n    tfc.util.assert(!hasBatches || (args.batches > 0 && Number.isInteger(args.batches)), () => 'Test loop expects `batches` to be a positive integer, but ' +\n        `received ${JSON.stringify(args.batches)}`);\n    const dataIterator = isLazyIteratorObject(dataset) ?\n        dataset :\n        await dataset.iterator();\n    // Keeps track of number of examples used in this evaluation.\n    let numExamples = 0;\n    let batch = 0;\n    while (hasBatches ? batch < args.batches : true) {\n        const iteratorOut = await dataIterator.next();\n        outs = tfc.tidy(() => {\n            if (iteratorOut.value) {\n                // TODO(cais): Once real dataset is available, use\n                //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n                const { xs, ys } = standardizeDataIteratorOutput(model, iteratorOut.value);\n                const xsAndYs = xs.concat(ys);\n                const batchOuts = tfc.tidy(() => f(xsAndYs));\n                tfc.dispose(xsAndYs);\n                if (batch === 0) {\n                    for (let i = 0; i < batchOuts.length; ++i) {\n                        outs.push(scalar(0));\n                    }\n                }\n                const batchSize = xsAndYs[0].shape[0];\n                for (let i = 0; i < batchOuts.length; ++i) {\n                    const batchOut = batchOuts[i];\n                    const oldScalar = outs[i];\n                    outs[i] =\n                        tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n                    if (batch > 0) {\n                        tfc.dispose(oldScalar);\n                    }\n                }\n                tfc.dispose(batchOuts);\n                numExamples += batchSize;\n                ++batch;\n            }\n            return outs;\n        });\n        if (iteratorOut.done) {\n            if (hasBatches) {\n                console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' +\n                    'Interrupting evalution. Make sure that your ' +\n                    'dataset can generate at least `batches` ' +\n                    `batches (in this case, ${args.batches} batches). ` +\n                    'You may need to use the repeat() function when building ' +\n                    'your dataset.');\n            }\n            break;\n        }\n    }\n    for (let i = 0; i < outs.length; ++i) {\n        const oldScalar = outs[i];\n        outs[i] = tfc.div(outs[i], numExamples);\n        tfc.dispose(oldScalar);\n    }\n    return singletonOrArray(outs);\n}\n//# sourceMappingURL=training_dataset.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Interfaces and methods for training models using tf.Tensor objects.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { Tensor, tensor1d, util } from '@tensorflow/tfjs-core';\nimport { expandDims, gather, sliceAlongFirstAxis } from '../backend/tfjs_backend';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { range } from '../utils/math_utils';\nexport function checkBatchSize(batchSize) {\n    tfc.util.assert(batchSize > 0 && Number.isInteger(batchSize), () => `batchSize is required to be a positive integer, but got ${batchSize}`);\n}\n/**\n * Slice a Tensor or an Array of Tensors, by start and stop indices.\n *\n * Porting Note: The `_slice_arrays` function in PyKeras is covered by this\n *   function and `sliceArraysByIndices()` together.\n *\n * @param arrays: the input.\n * @param start: the starting index (inclusive).\n * @param stop: the stopping index (exclusive).\n * @returns The result of the slicing. If `arrays` is an `Array` of\n *   `tf.Tensor`s, the slicing will be applied to all elements of the `Array`\n *   in the same way.\n */\nexport function sliceArrays(arrays, start, stop) {\n    if (arrays == null) {\n        return [null];\n    }\n    else if (Array.isArray(arrays)) {\n        return arrays.map(array => sliceAlongFirstAxis(array, start, stop - start));\n    }\n    else { // Tensor.\n        return sliceAlongFirstAxis(arrays, start, stop - start);\n    }\n}\n/**\n * Slice a Tensor or an Array of Tensors, by random-order indices.\n *\n * Porting Note: The `_slice_arrays` function in PyKeras is covered by this\n *   function and `sliceArrays()` together.\n *\n * @param arrays The input `tf.Tensor` or `Array` of `tf.Tensor`s to slice.\n *   If an `Array` of `tf.Tensor`s, all `tf.Tensor`s will be sliced in the\n *   same fashion.\n * @param indices The indices to use for slicing along the first (batch)\n *   dimension.\n * @returns Result(s) of the slicing.\n */\nexport function sliceArraysByIndices(arrays, indices) {\n    return tfc.tidy(() => {\n        if (arrays == null) {\n            return null;\n        }\n        else if (Array.isArray(arrays)) {\n            return arrays.map(array => sliceArraysByIndices(array, indices));\n        }\n        else {\n            // TODO(cais): indices should be a pre-constructed Tensor1D to avoid\n            //   tensor1d() calls.\n            return gather(arrays, indices.dtype === 'int32' ? indices : indices.toInt());\n        }\n    });\n}\n/**\n * Returns a list of batch indices (tuples of indices).\n * @param size: Integer, total size of the data to slice into batches.\n * @param batchSize: Integer, batch size.\n * @returns An Array of [batchStart, batchEnd] tuples. batchStart is\n *   inclusive; batchEnd is exclusive. I.e., each batch consists of indices x\n *   that satisfy batchStart <= x < batchEnd.\n */\nexport function makeBatches(size, batchSize) {\n    const output = [];\n    let batchStart = 0;\n    let batchEnd = null;\n    while (batchStart < size) {\n        batchEnd = batchStart + batchSize;\n        if (batchEnd >= size) {\n            batchEnd = size;\n        }\n        output.push([batchStart, batchEnd]);\n        batchStart = batchEnd;\n    }\n    return output;\n}\n/**\n * Abstract fit function for `f(ins)`.\n * @param f A Function returning a list of tensors. For training, this\n *   function is expected to perform the updates to the variables.\n * @param ins List of tensors to be fed to `f`.\n * @param outLabels List of strings, display names of the outputs of `f`.\n * @param batchSize Integer batch size or `== null` if unknown. Default : 32.\n * @param epochs Number of times to iterate over the data. Default : 1.\n * @param verbose Verbosity mode: 0, 1, or 2. Default: 1.\n * @param callbacks List of callbacks to be called during training.\n * @param valF Function to call for validation.\n * @param valIns List of tensors to be fed to `valF`.\n * @param shuffle Whether to shuffle the data at the beginning of every\n * epoch. Default : true.\n * @param callbackMetrics List of strings, the display names of the metrics\n *   passed to the callbacks. They should be the concatenation of the\n *   display names of the outputs of `f` and the list of display names\n *   of the outputs of `valF`.\n * @param initialEpoch Epoch at which to start training (useful for\n *   resuming a previous training run). Default : 0.\n * @param stepsPerEpoch Total number of steps (batches on samples) before\n *   declaring one epoch finished and starting the next epoch. Ignored with\n *   the default value of `undefined` or `null`.\n * @param validationSteps Number of steps to run validation for (only if\n *   doing validation from data tensors). Not applicable for tfjs-layers.\n * @returns A `History` object.\n */\nasync function fitLoop(\n// Type `model` as `any` here to avoid circular dependency w/ training.ts.\n// tslint:disable-next-line:no-any\nmodel, f, ins, outLabels, batchSize, epochs, verbose, callbacks, valF, valIns, shuffle, callbackMetrics, initialEpoch, stepsPerEpoch, validationSteps) {\n    if (batchSize == null) {\n        batchSize = 32;\n    }\n    if (epochs == null) {\n        epochs = 1;\n    }\n    if (shuffle == null) {\n        shuffle = true;\n    }\n    if (initialEpoch == null) {\n        initialEpoch = 0;\n    }\n    // TODO(cais): Change const to let below when implementing validation.\n    let doValidation = false;\n    if (valF != null && valIns != null) {\n        doValidation = true;\n        // TODO(cais): verbose message.\n    }\n    if (validationSteps != null) {\n        doValidation = true;\n        if (stepsPerEpoch == null) {\n            throw new ValueError('Can only use `validationSteps` when doing step-wise training, ' +\n                'i.e., `stepsPerEpoch` must be set.');\n        }\n    }\n    const numTrainSamples = model.checkNumSamples(ins, batchSize, stepsPerEpoch, 'steps_per_epoch');\n    let indexArray;\n    if (numTrainSamples != null) {\n        indexArray = range(0, numTrainSamples);\n    }\n    if (verbose == null) {\n        verbose = 1;\n    }\n    const { callbackList, history } = configureCallbacks(callbacks, verbose, epochs, initialEpoch, numTrainSamples, stepsPerEpoch, batchSize, doValidation, callbackMetrics);\n    callbackList.setModel(model);\n    model.history = history;\n    await callbackList.onTrainBegin();\n    model.stopTraining_ = false;\n    // TODO(cais): Take care of callbacks.validation_data as in PyKeras.\n    // TODO(cais): Pre-convert feeds for performance as in PyKeras.\n    for (let epoch = initialEpoch; epoch < epochs; ++epoch) {\n        await callbackList.onEpochBegin(epoch);\n        const epochLogs = {};\n        if (stepsPerEpoch != null) {\n            throw new NotImplementedError('stepsPerEpoch mode is not implemented yet.');\n        }\n        else {\n            if (shuffle === 'batch') {\n                throw new NotImplementedError('batch shuffling is not implemneted yet');\n            }\n            else if (shuffle) {\n                util.shuffle(indexArray);\n            }\n            // Convert the potentially shuffled indices to Tensor1D, to avoid the\n            // cost of repeated creation of Array1Ds later on.\n            const epochIndexArray1D = tensor1d(indexArray);\n            const batches = makeBatches(numTrainSamples, batchSize);\n            for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n                const batchLogs = {};\n                await callbackList.onBatchBegin(batchIndex, batchLogs);\n                tfc.tidy(() => {\n                    const batchStart = batches[batchIndex][0];\n                    const batchEnd = batches[batchIndex][1];\n                    const batchIds = sliceAlongFirstAxis(epochIndexArray1D, batchStart, batchEnd - batchStart);\n                    batchLogs['batch'] = batchIndex;\n                    batchLogs['size'] = batchEnd - batchStart;\n                    // TODO(cais): In ins, train flag can be a number, instead of an\n                    //   Tensor? Do we need to handle this in tfjs-layers?\n                    const insBatch = sliceArraysByIndices(ins, batchIds);\n                    const outs = f(insBatch);\n                    for (let i = 0; i < outLabels.length; ++i) {\n                        const label = outLabels[i];\n                        const out = outs[i];\n                        batchLogs[label] = out;\n                        tfc.keep(out);\n                        // TODO(cais): Use scope() to avoid ownership.\n                    }\n                    if (batchIndex === batches.length - 1) { // Last batch.\n                        if (doValidation) {\n                            const valOuts = model.testLoop(valF, valIns, batchSize);\n                            // Porting Notes: In tfjs-layers, valOuts is always an Array.\n                            for (let i = 0; i < outLabels.length; ++i) {\n                                const label = outLabels[i];\n                                const out = valOuts[i];\n                                tfc.keep(out);\n                                // TODO(cais): Use scope() to avoid ownership.\n                                epochLogs['val_' + label] = out;\n                            }\n                        }\n                    }\n                });\n                await callbackList.onBatchEnd(batchIndex, batchLogs);\n                disposeTensorsInLogs(batchLogs);\n                if (model.stopTraining_) {\n                    break;\n                }\n                // TODO(cais): return outs as list of Tensor.\n            }\n            epochIndexArray1D.dispose();\n        }\n        // TODO(cais): Run validation at the end of the epoch.\n        await callbackList.onEpochEnd(epoch, epochLogs);\n        if (model.stopTraining_) {\n            break;\n        }\n    }\n    await callbackList.onTrainEnd();\n    await model.history.syncData();\n    return model.history;\n}\nexport async function fitTensors(\n// Type `model` as `any` here to avoid circular dependency w/ training.ts.\n// tslint:disable-next-line:no-any\nmodel, x, y, args = {}) {\n    if (model.isTraining) {\n        throw new Error('Cannot start training because another fit() call is ongoing.');\n    }\n    model.isTraining = true;\n    let inputs;\n    let targets;\n    let inputValX;\n    let inputValY;\n    let valX;\n    let valY;\n    let sampleWeights;\n    try {\n        const batchSize = args.batchSize == null ? 32 : args.batchSize;\n        checkBatchSize(batchSize);\n        // Validate user data.\n        // TODO(cais): Support sampleWeight.\n        const checkBatchAxis = false;\n        const standardizedOuts = await model.standardizeUserData(x, y, args.sampleWeight, args.classWeight, checkBatchAxis, batchSize);\n        inputs = standardizedOuts[0];\n        targets = standardizedOuts[1];\n        sampleWeights = standardizedOuts[2];\n        // Prepare validation data.\n        let doValidation = false;\n        let valIns;\n        if (args.validationData != null && args.validationData.length > 0) {\n            doValidation = true;\n            if (args.validationData.length === 2) {\n                // config.validationData consists of valX and valY.\n                inputValX = args.validationData[0];\n                inputValY = args.validationData[1];\n            }\n            else if (args.validationData.length === 3) {\n                throw new NotImplementedError('validationData including sample weights is not supported yet.');\n            }\n            else {\n                throw new ValueError(`When passing validation data, it must contain 2 (valX, valY) ` +\n                    `or 3 (valX, valY, valSampleWeight) items; ` +\n                    `${args.validationData} is invalid.`);\n            }\n            const checkBatchAxis = true;\n            const valStandardized = await model.standardizeUserData(inputValX, inputValY, null, /** Unused sample weights. */ null, /** Unused class weights. */ checkBatchAxis, batchSize);\n            valX = valStandardized[0];\n            valY = valStandardized[1];\n            valIns = valX.concat(valY);\n            // TODO(cais): Add useLearningPhase data properly.\n        }\n        else if (args.validationSplit != null && args.validationSplit > 0 &&\n            args.validationSplit < 1) {\n            doValidation = true;\n            // Porting Note: In tfjs-layers, inputs[0] is always a Tensor.\n            const splitAt = Math.floor(inputs[0].shape[0] * (1 - args.validationSplit));\n            const originalBatchSize = inputs[0].shape[0];\n            valX = sliceArrays(inputs, splitAt, originalBatchSize);\n            inputs = sliceArrays(inputs, 0, splitAt);\n            valY = sliceArrays(targets, splitAt, originalBatchSize);\n            targets = sliceArrays(targets, 0, splitAt);\n            // TODO(cais): Once sampleWeights becomes available, slice it to get\n            //   valSampleWeights.\n            valIns = valX.concat(valY);\n            // TODO(cais): Add useLearningPhase data properly.\n        }\n        else if (args.validationSteps != null) {\n            doValidation = true;\n            // TODO(cais): Add useLearningPhase.\n        }\n        const ins = inputs.concat(targets).concat(sampleWeights);\n        model.checkTrainableWeightsConsistency();\n        // TODO(cais): Handle use_learning_phase and learning_phase?\n        // Porting Note: Here we see a key deviation of tfjs-layers from\n        // Keras.\n        //  Due to the imperative nature of tfjs-layers' backend (tfjs-core),\n        //  we do not construct symbolic computation graphs to embody the\n        //  training process. Instead, we define a function that performs the\n        //  training action. In PyKeras, the data (inputs and targets) are fed\n        //  through graph placeholders. In tfjs-layers, the data are fed as\n        //  function arguments. Since the function are defined below in the\n        //  scope, we don't have equivalents of PyKeras's\n        //  `_make_train_funciton`.\n        const trainFunction = model.makeTrainFunction();\n        const outLabels = model.getDedupedMetricsNames();\n        let valFunction;\n        let callbackMetrics;\n        if (doValidation) {\n            model.makeTestFunction();\n            valFunction = model.testFunction;\n            callbackMetrics =\n                outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n        }\n        else {\n            valFunction = null;\n            valIns = [];\n            callbackMetrics = outLabels.slice();\n        }\n        const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n        const out = await fitLoop(model, trainFunction, ins, outLabels, batchSize, args.epochs, args.verbose, callbacks, valFunction, valIns, args.shuffle, callbackMetrics, args.initialEpoch, null, null);\n        return out;\n    }\n    finally {\n        model.isTraining = false;\n        // Memory clean up.\n        disposeNewTensors(inputs, x);\n        disposeNewTensors(targets, y);\n        disposeNewTensors(valX, inputValX);\n        disposeNewTensors(valY, inputValY);\n        if (sampleWeights != null) {\n            tfc.dispose(sampleWeights);\n        }\n    }\n    // TODO(cais): Add value to outLabels.\n}\n/**\n * Ensure tensors all have a rank of at least 2.\n *\n * If a tensor has a rank of 1, it is dimension-expanded to rank 2.\n * If any tensor has a rank of 0 (i.e., is a scalar), an error will be thrown.\n */\nexport function ensureTensorsRank2OrHigher(tensors) {\n    const outs = [];\n    if (tensors instanceof Tensor) {\n        tensors = [tensors];\n    }\n    // Make Tensors at least 2D.\n    for (let i = 0; i < tensors.length; ++i) {\n        const tensor = tensors[i];\n        if (tensor.rank === 1) {\n            outs.push(expandDims(tensor, 1));\n        }\n        else if (tensor.rank === 0) {\n            throw new Error('Expected tensor to be at least 1D, but received a 0D tensor ' +\n                '(scalar).');\n        }\n        else {\n            outs.push(tensor);\n        }\n    }\n    return outs;\n}\n/**\n * Compare a set of tensors with a reference (old) set, discard the ones\n * in the new set that are not present in the reference set.\n *\n * This method is used for memory clenaup during calls such as\n * LayersModel.fit().\n *\n * @param tensors New set which may contain Tensors not present in\n *   `refTensors`.\n * @param refTensors Reference Tensor set.\n */\n// TODO(cais, kangyizhang): Deduplicate with tfjs-data.\nexport function disposeNewTensors(tensors, refTensors) {\n    if (tensors == null) {\n        return;\n    }\n    const oldTensorIds = [];\n    if (refTensors instanceof Tensor) {\n        oldTensorIds.push(refTensors.id);\n    }\n    else if (Array.isArray(refTensors)) {\n        refTensors.forEach(t => oldTensorIds.push(t.id));\n    }\n    else if (refTensors != null) {\n        // `oldTensors` is a map from string name to Tensor.\n        for (const name in refTensors) {\n            const oldTensor = refTensors[name];\n            oldTensorIds.push(oldTensor.id);\n        }\n    }\n    const tensorsToDispose = [];\n    if (tensors instanceof Tensor) {\n        if (oldTensorIds.indexOf(tensors.id) === -1) {\n            tensorsToDispose.push(tensors);\n        }\n    }\n    else if (Array.isArray(tensors)) {\n        tensors.forEach(t => {\n            if (oldTensorIds.indexOf(t.id) === -1) {\n                tensorsToDispose.push(t);\n            }\n        });\n    }\n    else if (tensors != null) {\n        // `oldTensors` is a map from string name to Tensor.\n        for (const name in tensors) {\n            const tensor = tensors[name];\n            if (oldTensorIds.indexOf(tensor.id) === -1) {\n                tensorsToDispose.push(tensor);\n            }\n        }\n    }\n    tensorsToDispose.forEach(t => {\n        if (!t.isDisposed) {\n            t.dispose();\n        }\n    });\n}\n//# sourceMappingURL=training_tensors.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/* Original Source: engine/training.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { io, Optimizer, scalar, serialization, Tensor, tensor1d, util } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { NotImplementedError, RuntimeError, ValueError } from '../errors';\nimport { deserialize } from '../layers/serialization';\nimport * as losses from '../losses';\nimport * as Metrics from '../metrics';\nimport * as optimizers from '../optimizers';\nimport { checkUserDefinedMetadata } from '../user_defined_metadata';\nimport { count, pyListRepeat, singletonOrArray, toCamelCase, toSnakeCase, unique } from '../utils/generic_utils';\nimport { printSummary } from '../utils/layer_utils';\nimport { range } from '../utils/math_utils';\nimport { convertPythonicToTs } from '../utils/serialization_utils';\nimport { version } from '../version';\nimport { Container } from './container';\nimport { execute, FeedDict } from './executor';\nimport { evaluateDataset, fitDataset } from './training_dataset';\nimport { checkBatchSize, disposeNewTensors, ensureTensorsRank2OrHigher, fitTensors, makeBatches, sliceArrays, sliceArraysByIndices } from './training_tensors';\nimport { computeWeightedLoss, standardizeClassWeights, standardizeWeights } from './training_utils';\n/**\n * Helper function for polymorphic input data: 1. singleton Tensor.\n */\nexport function isDataTensor(x) {\n    return x instanceof Tensor;\n}\n/**\n * Helper function for polymorphic input data: 2. Array of Tensor.\n */\nexport function isDataArray(x) {\n    return Array.isArray(x);\n}\n/**\n * Helper function for polymorphic input data: 3. \"dict\" of Tensor.\n */\nexport function isDataDict(x) {\n    return !isDataTensor(x) && !isDataArray(x);\n}\n/**\n * Normalizes inputs and targets provided by users.\n * @param data User-provided input data (polymorphic).\n * @param names An Array of expected Tensor names.\n * @param shapes Optional Array of expected Tensor shapes.\n * @param checkBatchAxis Whether to check that the batch axis of the arrays\n *   match  the expected value found in `shapes`.\n * @param exceptionPrefix String prefix used for exception formatting.\n * @returns List of standardized input Tensors (one Tensor per model input).\n * @throws ValueError: in case of improperly formatted user data.\n */\nexport function standardizeInputData(data, names, shapes, checkBatchAxis = true, exceptionPrefix = '') {\n    if (names == null || names.length === 0) {\n        // Check for the case where the model expected no data, but some data got\n        // sent.\n        if (data != null) {\n            let gotUnexpectedData = false;\n            if (isDataArray(data) && data.length > 0) {\n                gotUnexpectedData = true;\n            }\n            else if (isDataDict(data)) {\n                for (const key in data) {\n                    if (data.hasOwnProperty(key)) {\n                        gotUnexpectedData = true;\n                        break;\n                    }\n                }\n            }\n            else {\n                // `data` is a singleton Tensor in this case.\n                gotUnexpectedData = true;\n            }\n            if (gotUnexpectedData) {\n                throw new ValueError(`Error when checking model ${exceptionPrefix} expected no data, ` +\n                    `but got ${data}`);\n            }\n        }\n        return [];\n    }\n    if (data == null) {\n        return names.map(name => null);\n    }\n    let arrays;\n    if (isDataDict(data)) {\n        data = data;\n        arrays = [];\n        for (const name of names) {\n            if (data[name] == null) {\n                throw new ValueError(`No data provided for \"${name}\". Need data for each key in: ` +\n                    `${names}`);\n            }\n            arrays.push(data[name]);\n        }\n    }\n    else if (isDataArray(data)) {\n        data = data;\n        if (data.length !== names.length) {\n            throw new ValueError(`Error when checking model ${exceptionPrefix}: the Array of ` +\n                `Tensors that you are passing to your model is not the size the ` +\n                `model expected. Expected to see ${names.length} Tensor(s), but ` +\n                `instead got the following list of Tensor(s): ${data}`);\n        }\n        arrays = data;\n    }\n    else {\n        data = data;\n        if (names.length > 1) {\n            throw new ValueError(`The model ${exceptionPrefix} expects ${names.length} Tensor(s), ` +\n                `but only received one Tensor. Found: Tensor with shape ${data.shape}`);\n        }\n        arrays = [data];\n    }\n    arrays = ensureTensorsRank2OrHigher(arrays);\n    // Check shape compatibility.\n    if (shapes != null) {\n        for (let i = 0; i < names.length; ++i) {\n            if (shapes[i] == null) {\n                continue;\n            }\n            const array = arrays[i];\n            if (array.shape.length !== shapes[i].length) {\n                throw new ValueError(`Error when checking ${exceptionPrefix}: expected ${names[i]} ` +\n                    `to have ${shapes[i].length} dimension(s). but got array with ` +\n                    `shape ${array.shape}`);\n            }\n            for (let j = 0; j < shapes[i].length; ++j) {\n                if (j === 0 && !checkBatchAxis) {\n                    // Skip the first (batch) axis.\n                    continue;\n                }\n                const dim = array.shape[j];\n                const refDim = shapes[i][j];\n                if (refDim != null && refDim >= 0 && dim !== refDim) {\n                    throw new ValueError(`Error when checking ${exceptionPrefix}: expected ${names[i]} ` +\n                        `to have shape [${shapes[i]}], but got array with shape ` +\n                        `[${array.shape}].`);\n                }\n            }\n        }\n    }\n    return arrays;\n}\n/**\n * User input validation for Tensors.\n * @param inputs `Array` of `tf.Tensor`s for inputs.\n * @param targets `Array` of `tf.Tensor`s for targets.\n * @param weights Optional `Array` of `tf.Tensor`s for sample weights.\n * @throws ValueError: in case of incorrectly formatted data.\n */\nexport function checkArrayLengths(inputs, targets, weights) {\n    const setX = unique(inputs.map(input => input.shape[0]));\n    setX.sort();\n    const setY = unique(targets.map(target => target.shape[0]));\n    setY.sort();\n    // TODO(cais): Check `weights` as well.\n    if (setX.length > 1) {\n        throw new ValueError(`All input Tensors (x) should have the same number of samples. ` +\n            `Got array shapes: ` +\n            `${JSON.stringify(inputs.map(input => input.shape))}`);\n    }\n    if (setY.length > 1) {\n        throw new ValueError(`All target Tensors (y) should have the same number of samples. ` +\n            `Got array shapes: ` +\n            `${JSON.stringify(targets.map(target => target.shape))}`);\n    }\n    if (setX.length > 0 && setY.length > 0 && !util.arraysEqual(setX, setY)) {\n        throw new ValueError(`Input Tensors should have the same number of samples as target ` +\n            `Tensors. Found ${setX[0]} input sample(s) and ${setY[0]} target ` +\n            `sample(s).`);\n    }\n}\n/**\n * Validation on the compatibility of targes and loss functions.\n *\n * This helps prevent users from using loss functions incorrectly.\n *\n * @param targets `Array` of `tf.Tensor`s of targets.\n * @param lossFns `Array` of loss functions.\n * @param outputShapes `Array` of shapes of model outputs.\n */\nfunction checkLossAndTargetCompatibility(targets, lossFns, outputShapes) {\n    // TODO(cais): Dedicated test coverage?\n    const keyLosses = [\n        losses.meanSquaredError, losses.binaryCrossentropy,\n        losses.categoricalCrossentropy\n    ];\n    for (let i = 0; i < targets.length; ++i) {\n        const y = targets[i];\n        const loss = lossFns[i];\n        const shape = outputShapes[i];\n        if (loss == null) {\n            continue;\n        }\n        if (loss === losses.categoricalCrossentropy) {\n            if (y.shape[y.shape.length - 1] === 1) {\n                throw new ValueError(`You are passing a target array of shape ${y.shape} while using ` +\n                    `a loss 'categorical_crossentropy'. 'categorical_crossentropy'` +\n                    `expects targets to be binary matrices (1s and 0s) of shape ` +\n                    `[samples, classes].`);\n                // TODO(cais): Example code in error message.\n            }\n        }\n        if (keyLosses.indexOf(loss) !== -1) {\n            const slicedYShape = y.shape.slice(1);\n            const slicedShape = shape.slice(1);\n            for (let j = 0; j < slicedYShape.length; ++j) {\n                const targetDim = slicedYShape[j];\n                const outDim = slicedShape[j];\n                if (outDim != null && targetDim !== outDim) {\n                    throw new ValueError(`A target Tensor with shape ${y.shape} was passed for an ` +\n                        `output of shape ${shape}, while using a loss function that ` +\n                        `expects targets to have the same shape as the output.`);\n                }\n            }\n        }\n    }\n}\n/**\n * Check inputs provided by the user.\n *\n * Porting Note: This corresponds to _standardize_input_data() in Python\n *   Keras. Because of the strong typing in TF.js, we do not need to convert\n *   the data. Specifically:\n *   1) in PyKeras, `data` can be `DataFrame` instances from pandas, for\n *      example. We don't need to worry about that here because there is no\n *      widely popular javascript/typesdcript equivalent of pandas (so far).\n *      If one becomes available in the future, we can add support.\n *   2) in PyKeras, inputs can be Python dict. But here we are stipulating\n * that the data is either a single `tf.Tensor` or an Array of `tf.Tensor`s. We\n * may add support for `Object` data inputs in the future when the need\n * arises.\n *\n * Instead, we perform basic checks for number of parameters and shapes.\n *\n * @param data: The input data.\n * @param names: Name for the inputs, from the model.\n * @param shapes: Expected shapes for the input data, from the model.\n * @param checkBatchAxis: Whether the size along the batch axis (i.e., the\n *   first dimension) will be checked for matching.\n * @param exceptionPrefix: Execption prefix message, used in generating error\n *   messages.\n * @throws ValueError: on incorrect number of inputs or mismatches in shapes.\n */\nfunction checkInputData(data, names, shapes, checkBatchAxis = true, exceptionPrefix = '') {\n    let arrays;\n    if (Array.isArray(data)) {\n        if (data.length !== names.length) {\n            throw new ValueError(`Error when checking model ${exceptionPrefix}: the Array of ` +\n                `Tensors that you are passing to your model is not the size the ` +\n                `the model expected. Expected to see ${names.length} Tensor(s),` +\n                ` but instead got ${data.length} Tensors(s).`);\n        }\n        arrays = data;\n    }\n    else {\n        if (names.length > 1) {\n            throw new ValueError(`The model expects ${names.length} ${exceptionPrefix} Tensors, ` +\n                `but only received one Tensor. Found: array with shape ` +\n                `${JSON.stringify(data.shape)}.`);\n        }\n        arrays = [data];\n    }\n    if (shapes != null) {\n        for (let i = 0; i < names.length; ++i) {\n            if (shapes[i] == null) {\n                continue;\n            }\n            const array = arrays[i];\n            if (array.shape.length !== shapes[i].length) {\n                throw new ValueError(`Error when checking ${exceptionPrefix}: expected ${names[i]} ` +\n                    `to have ${shapes[i].length} dimension(s), but got array with ` +\n                    `shape ${JSON.stringify(array.shape)}`);\n            }\n            for (let j = 0; j < shapes[i].length; ++j) {\n                if (j === 0 && !checkBatchAxis) {\n                    continue;\n                }\n                const dim = array.shape[j];\n                const refDim = shapes[i][j];\n                if (refDim != null) {\n                    if (refDim !== dim) {\n                        throw new ValueError(`Error when checking ${exceptionPrefix}: expected ` +\n                            `${names[i]} to have shape ${JSON.stringify(shapes[i])} but ` +\n                            `got array with shape ${JSON.stringify(array.shape)}.`);\n                    }\n                }\n            }\n        }\n    }\n}\n/**\n * Maps metric functions to model outputs.\n * @param metrics An shortcut strings name, metric function, `Array` or dict\n *   (`Object`) of metric functions.\n * @param outputNames An `Array` of the names of model outputs.\n * @returns An `Array` (one entry per model output) of `Array` of metric\n *   functions. For instance, if the model has 2 outputs, and for the first\n *   output we want to compute `binaryAccuracy` and `binaryCrossentropy`,\n *   and just `binaryAccuracy` for the second output, the `Array` would look\n *   like:\n *     `[[binaryAccuracy, binaryCrossentropy],  [binaryAccuracy]]`\n * @throws TypeError: incompatible metrics format.\n */\nexport function collectMetrics(metrics, outputNames) {\n    if (metrics == null || Array.isArray(metrics) && metrics.length === 0) {\n        return outputNames.map(name => []);\n    }\n    let wrappedMetrics;\n    if (typeof metrics === 'string' || typeof metrics === 'function') {\n        wrappedMetrics = [metrics];\n    }\n    else if (Array.isArray(metrics) || typeof metrics === 'object') {\n        wrappedMetrics = metrics;\n    }\n    else {\n        throw new TypeError('Type of metrics argument not understood. Expected an string,' +\n            `function, Array, or Object, found: ${metrics}`);\n    }\n    if (Array.isArray(wrappedMetrics)) {\n        // We then apply all metrics to all outputs.\n        return outputNames.map(name => wrappedMetrics);\n    }\n    else {\n        // In this case, metrics is a dict.\n        const nestedMetrics = [];\n        for (const name of outputNames) {\n            let outputMetrics = wrappedMetrics.hasOwnProperty(name) ? wrappedMetrics[name] : [];\n            if (!Array.isArray(outputMetrics)) {\n                outputMetrics = [outputMetrics];\n            }\n            nestedMetrics.push(outputMetrics);\n        }\n        return nestedMetrics;\n    }\n}\nconst LAYERS_MODEL_FORMAT_NAME = 'layers-model';\n/**\n * A `tf.LayersModel` is a directed, acyclic graph of `tf.Layer`s plus methods\n * for training, evaluation, prediction and saving.\n *\n * `tf.LayersModel` is the basic unit of training, inference and evaluation in\n * TensorFlow.js. To create a `tf.LayersModel`, use `tf.LayersModel`.\n *\n * See also:\n *   `tf.Sequential`, `tf.loadLayersModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport class LayersModel extends Container {\n    constructor(args) {\n        super(args);\n        this.isTraining = false;\n    }\n    /**\n     * Print a text summary of the model's layers.\n     *\n     * The summary includes\n     * - Name and type of all layers that comprise the model.\n     * - Output shape(s) of the layers\n     * - Number of weight parameters of each layer\n     * - If the model has non-sequential-like topology, the inputs each layer\n     *   receives\n     * - The total number of trainable and non-trainable parameters of the model.\n     *\n     * ```js\n     * const input1 = tf.input({shape: [10]});\n     * const input2 = tf.input({shape: [20]});\n     * const dense1 = tf.layers.dense({units: 4}).apply(input1);\n     * const dense2 = tf.layers.dense({units: 8}).apply(input2);\n     * const concat = tf.layers.concatenate().apply([dense1, dense2]);\n     * const output =\n     *     tf.layers.dense({units: 3, activation: 'softmax'}).apply(concat);\n     *\n     * const model = tf.model({inputs: [input1, input2], outputs: output});\n     * model.summary();\n     * ```\n     *\n     * @param lineLength Custom line length, in number of characters.\n     * @param positions Custom widths of each of the columns, as either\n     *   fractions of `lineLength` (e.g., `[0.5, 0.75, 1]`) or absolute number\n     *   of characters (e.g., `[30, 50, 65]`). Each number corresponds to\n     *   right-most (i.e., ending) position of a column.\n     * @param printFn Custom print function. Can be used to replace the default\n     *   `console.log`. For example, you can use `x => {}` to mute the printed\n     *   messages in the console.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    summary(lineLength, positions, printFn = console.log) {\n        if (!this.built) {\n            throw new ValueError(`This model has never been called, thus its weights have not been ` +\n                `created yet. So no summary can be displayed. Build the model ` +\n                `first (e.g., by calling it on some test data).`);\n        }\n        printSummary(this, lineLength, positions, printFn);\n    }\n    /**\n     * Configures and prepares the model for training and evaluation.  Compiling\n     * outfits the model with an optimizer, loss, and/or metrics.  Calling `fit`\n     * or `evaluate` on an un-compiled model will throw an error.\n     *\n     * @param args a `ModelCompileArgs` specifying the loss, optimizer, and\n     * metrics to be used for fitting and evaluating this model.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    compile(args) {\n        if (args.loss == null) {\n            args.loss = [];\n        }\n        this.loss = args.loss;\n        if (typeof args.optimizer === 'string') {\n            this.optimizer_ = optimizers.getOptimizer(args.optimizer);\n            this.isOptimizerOwned = true;\n        }\n        else {\n            if (!(args.optimizer instanceof Optimizer)) {\n                throw new ValueError(`User-defined optimizer must be an instance of tf.Optimizer.`);\n            }\n            this.optimizer_ = args.optimizer;\n            this.isOptimizerOwned = false;\n        }\n        // TODO(cais): Add lossWeights.\n        // TODO(cais): Add sampleWeightMode.\n        // Prepare loss functions.\n        let lossFunctions = [];\n        if (!Array.isArray(args.loss) && typeof args.loss !== 'string' &&\n            typeof args.loss !== 'function') {\n            args.loss = args.loss;\n            for (const name in args.loss) {\n                if (this.outputNames.indexOf(name) === -1) {\n                    throw new ValueError(`Unknown entry in loss dictionary: \"${name}\". ` +\n                        `Only expected the following keys: ${this.outputNames}`);\n                }\n            }\n            for (const name of this.outputNames) {\n                if (args.loss[name] == null) {\n                    console.warn(`Output \"${name}\" is missing from loss dictionary. We assume ` +\n                        `this was done on purpose, and we will not be expecting data ` +\n                        `to be passed to ${name} during training`);\n                }\n                lossFunctions.push(losses.get(args.loss[name]));\n            }\n        }\n        else if (Array.isArray(args.loss)) {\n            if (args.loss.length !== this.outputs.length) {\n                throw new ValueError(`When passing an Array as loss, it should have one entry per ` +\n                    `model output. The model has ${this.outputs.length} output(s), ` +\n                    `but you passed loss=${args.loss}.`);\n            }\n            const theLosses = args.loss;\n            lossFunctions = theLosses.map(l => losses.get(l));\n        }\n        else {\n            const lossFunction = losses.get(args.loss);\n            this.outputs.forEach(_ => {\n                lossFunctions.push(lossFunction);\n            });\n        }\n        this.lossFunctions = lossFunctions;\n        this.feedOutputNames = [];\n        this.feedOutputShapes = [];\n        this.feedLossFns = [];\n        for (let i = 0; i < this.outputs.length; ++i) {\n            // TODO(cais): Logic for skipping target(s).\n            const shape = this.internalOutputShapes[i];\n            const name = this.outputNames[i];\n            this.feedOutputNames.push(name);\n            this.feedOutputShapes.push(shape);\n            this.feedLossFns.push(this.lossFunctions[i]);\n        }\n        // TODO(cais): Add logic for output masks.\n        // TODO(cais): Add logic for sample weights.\n        const skipTargetIndices = [];\n        // Prepare metrics.\n        this.metrics = args.metrics;\n        // TODO(cais): Add weightedMetrics.\n        this.metricsNames = ['loss'];\n        this.metricsTensors = [];\n        // Compute total loss.\n        // Porting Note: In PyKeras, metrics_tensors are symbolic tensor objects.\n        //   Here, metricsTensors are TypeScript functions. This difference is due\n        //   to the difference in symbolic/imperative property of the backends.\n        nameScope('loss', () => {\n            for (let i = 0; i < this.outputs.length; ++i) {\n                if (skipTargetIndices.indexOf(i) !== -1) {\n                    continue;\n                }\n                // TODO(cais): Add weightedLoss, sampleWeight and mask.\n                //   The following line should be weightedLoss\n                const weightedLoss = this.lossFunctions[i];\n                if (this.outputs.length > 1) {\n                    this.metricsTensors.push([weightedLoss, i]);\n                    this.metricsNames.push(this.outputNames[i] + '_loss');\n                }\n            }\n            // Porting Note: Due to the imperative nature of the backend, we calculate\n            //   the regularizer penalties in the totalLossFunction, instead of here.\n        });\n        const nestedMetrics = collectMetrics(args.metrics, this.outputNames);\n        // TODO(cais): Add nestedWeightedMetrics.\n        /**\n         * Helper function used in loop below.\n         */\n        const appendMetric = (outputIndex, metricName, metricTensor) => {\n            if (this.outputNames.length > 1) {\n                metricName = this.outputNames[outputIndex] + '_' + metricName;\n            }\n            this.metricsNames.push(metricName);\n            this.metricsTensors.push([metricTensor, outputIndex]);\n        };\n        nameScope('metric', () => {\n            for (let i = 0; i < this.outputs.length; ++i) {\n                if (skipTargetIndices.indexOf(i) !== -1) {\n                    continue;\n                }\n                const outputMetrics = nestedMetrics[i];\n                // TODO(cais): Add weights and outputWeightedMetrics.\n                // TODO(cais): Add optional arg `weights` to the following function.\n                const handleMetrics = (metrics) => {\n                    const metricNamePrefix = '';\n                    let metricName;\n                    let accFn;\n                    let weightedMetricFn;\n                    //  TODO(cais): Use 'weights_' for weighted metrics.\n                    for (const metric of metrics) {\n                        if (typeof metric === 'string' &&\n                            ['accuracy', 'acc', 'crossentropy', 'ce'].indexOf(metric) !==\n                                -1) {\n                            const outputShape = this.internalOutputShapes[i];\n                            if (outputShape[outputShape.length - 1] === 1 ||\n                                this.lossFunctions[i] === losses.binaryCrossentropy) {\n                                // case: binary accuracy/crossentropy.\n                                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                                    accFn = Metrics.binaryAccuracy;\n                                }\n                                else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                                    accFn = Metrics.binaryCrossentropy;\n                                }\n                            }\n                            else if (this.lossFunctions[i] ===\n                                losses.sparseCategoricalCrossentropy) {\n                                // case: categorical accuracy / crossentropy with sparse\n                                // targets.\n                                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                                    accFn = Metrics.sparseCategoricalAccuracy;\n                                }\n                                else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                                    accFn = Metrics.sparseCategoricalCrossentropy;\n                                }\n                            }\n                            else {\n                                // case: categorical accuracy / crossentropy.\n                                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                                    accFn = Metrics.categoricalAccuracy;\n                                }\n                                else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                                    accFn = Metrics.categoricalCrossentropy;\n                                }\n                            }\n                            let suffix;\n                            if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                                suffix = 'acc';\n                            }\n                            else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                                suffix = 'ce';\n                            }\n                            // TODO(cais): Add weighting actually.\n                            weightedMetricFn = accFn;\n                            metricName = metricNamePrefix + suffix;\n                        }\n                        else {\n                            const metricFn = Metrics.get(metric);\n                            // TODO(cais): Add weighting actually.\n                            weightedMetricFn = metricFn;\n                            metricName =\n                                metricNamePrefix + Metrics.getLossOrMetricName(metric);\n                        }\n                        // TODO(cais): Add weighting and masking to metricResult.\n                        let metricResult;\n                        nameScope(metricName, () => {\n                            metricResult = weightedMetricFn;\n                        });\n                        appendMetric(i, metricName, metricResult);\n                    }\n                };\n                handleMetrics(outputMetrics);\n                // TODO(cais): Call handleMetrics with weights.\n            }\n        });\n        // Porting Notes: Given the imperative backend of tfjs-core,\n        //   there is no need for constructing the symbolic graph and placeholders.\n        this.collectedTrainableWeights = this.trainableWeights;\n    }\n    /**\n     * Check trainable weights count consistency.\n     *\n     * This will raise a warning if `this.trainableWeights` and\n     * `this.collectedTrainableWeights` are inconsistent (i.e., have different\n     * numbers of parameters).\n     * Inconsistency will typically arise when one modifies `model.trainable`\n     * without calling `model.compile()` again.\n     */\n    checkTrainableWeightsConsistency() {\n        if (this.collectedTrainableWeights == null) {\n            return;\n        }\n        if (this.trainableWeights.length !==\n            this.collectedTrainableWeights.length) {\n            console.warn('Discrepancy between trainableweights and collected trainable ' +\n                'weights. Did you set `model.trainable` without calling ' +\n                '`model.compile()` afterwards?');\n        }\n    }\n    /**\n     * Returns the loss value & metrics values for the model in test mode.\n     *\n     * Loss and metrics are specified during `compile()`, which needs to happen\n     * before calls to `evaluate()`.\n     *\n     * Computation is done in batches.\n     *\n     * ```js\n     * const model = tf.sequential({\n     *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n     * });\n     * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n     * const result = model.evaluate(\n     *     tf.ones([8, 10]), tf.ones([8, 1]), {batchSize: 4});\n     * result.print();\n     * ```\n     *\n     * @param x `tf.Tensor` of test data, or an `Array` of `tf.Tensor`s if the\n     * model has multiple inputs.\n     * @param y `tf.Tensor` of target data, or an `Array` of `tf.Tensor`s if the\n     * model has multiple outputs.\n     * @param args A `ModelEvaluateArgs`, containing optional fields.\n     *\n     * @return `Scalar` test loss (if the model has a single output and no\n     *   metrics) or `Array` of `Scalar`s (if the model has multiple outputs\n     *   and/or metrics). The attribute `model.metricsNames`\n     *   will give you the display labels for the scalar outputs.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    evaluate(x, y, args = {}) {\n        const batchSize = args.batchSize == null ? 32 : args.batchSize;\n        checkBatchSize(batchSize);\n        // TODO(cais): Standardize `config.sampleWeights` as well.\n        // Validate user data.\n        const checkBatchAxis = true;\n        const standardizedOuts = this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n        try {\n            // TODO(cais): If uses `useLearningPhase`, set the corresponding element\n            // of the input to 0.\n            const ins = standardizedOuts[0].concat(standardizedOuts[1]);\n            this.makeTestFunction();\n            const f = this.testFunction;\n            const testOuts = this.testLoop(f, ins, batchSize, args.verbose, args.steps);\n            return singletonOrArray(testOuts);\n        }\n        finally {\n            disposeNewTensors(standardizedOuts[0], x);\n            disposeNewTensors(standardizedOuts[1], y);\n        }\n    }\n    // TODO(cais): Add code snippet below once real dataset objects are\n    //   available.\n    /**\n     * Evaluate model using a dataset object.\n     *\n     * Note: Unlike `evaluate()`, this method is asynchronous (`async`);\n     *\n     * @param dataset A dataset object. Its `iterator()` method is expected\n     *   to generate a dataset iterator object, the `next()` method of which\n     *   is expected to produce data batches for evaluation. The return value\n     *   of the `next()` call ought to contain a boolean `done` field and a\n     *   `value` field. The `value` field is expected to be an array of two\n     *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n     *   case is for models with exactly one input and one output (e.g..\n     *   a sequential model). The latter case is for models with multiple\n     *   inputs and/or multiple outputs. Of the two items in the array, the\n     *   first is the input feature(s) and the second is the output target(s).\n     * @param args A configuration object for the dataset-based evaluation.\n     * @returns Loss and metric values as an Array of `Scalar` objects.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    async evaluateDataset(dataset, args) {\n        this.makeTestFunction();\n        return evaluateDataset(this, dataset, args);\n    }\n    /**\n     * Get number of samples provided for training, evaluation or prediction.\n     *\n     * @param ins Input `tf.Tensor`.\n     * @param batchSize Integer batch size, optional.\n     * @param steps Total number of steps (batches of samples) before\n     * declaring loop finished. Optional.\n     * @param stepsName The public API's parameter name for `steps`.\n     * @returns Number of samples provided.\n     */\n    checkNumSamples(ins, batchSize, steps, stepsName = 'steps') {\n        let numSamples;\n        if (steps != null) {\n            numSamples = null;\n            if (batchSize != null) {\n                throw new ValueError(`If ${stepsName} is set, batchSize must be null or undefined.` +\n                    `Got batchSize = ${batchSize}`);\n            }\n        }\n        else if (ins != null) {\n            if (Array.isArray(ins)) {\n                numSamples = ins[0].shape[0];\n            }\n            else {\n                numSamples = ins.shape[0];\n            }\n        }\n        else {\n            throw new ValueError(`Either the input data should have a defined shape, or ` +\n                `${stepsName} shoud be specified.`);\n        }\n        return numSamples;\n    }\n    /**\n     * Execute internal tensors of the model with input data feed.\n     * @param inputs Input data feed. Must match the inputs of the model.\n     * @param outputs Names of the output tensors to be fetched. Must match\n     *   names of the SymbolicTensors that belong to the graph.\n     * @returns Fetched values for `outputs`.\n     */\n    execute(inputs, outputs) {\n        if (Array.isArray(outputs) && outputs.length === 0) {\n            throw new ValueError('`outputs` is an empty Array, which is not allowed.');\n        }\n        const outputsIsArray = Array.isArray(outputs);\n        const outputNames = (outputsIsArray ? outputs : [outputs]);\n        const outputSymbolicTensors = this.retrieveSymbolicTensors(outputNames);\n        // Format the input into a FeedDict.\n        const feedDict = new FeedDict();\n        if (inputs instanceof Tensor) {\n            inputs = [inputs];\n        }\n        if (Array.isArray(inputs)) {\n            if (inputs.length !== this.inputs.length) {\n                throw new ValueError(`The number of inputs provided (${inputs.length}) ` +\n                    `does not match the number of inputs of this model ` +\n                    `(${this.inputs.length}).`);\n            }\n            for (let i = 0; i < this.inputs.length; ++i) {\n                feedDict.add(this.inputs[i], inputs[i]);\n            }\n        }\n        else {\n            for (const input of this.inputs) {\n                const tensorValue = inputs[input.name];\n                if (tensorValue == null) {\n                    throw new ValueError(`No value is provided for the model's input ${input.name}`);\n                }\n                feedDict.add(input, tensorValue);\n            }\n        }\n        // Run execution.\n        const executeOutputs = execute(outputSymbolicTensors, feedDict);\n        return outputsIsArray ? executeOutputs : executeOutputs[0];\n    }\n    /**\n     * Retrieve the model's internal symbolic tensors from symbolic-tensor names.\n     */\n    retrieveSymbolicTensors(symbolicTensorNames) {\n        const outputSymbolicTensors = pyListRepeat(null, symbolicTensorNames.length);\n        let outputsRemaining = symbolicTensorNames.length;\n        for (const layer of this.layers) {\n            const layerOutputs = Array.isArray(layer.output) ? layer.output : [layer.output];\n            const layerOutputNames = layerOutputs.map(output => output.name);\n            for (let i = 0; i < symbolicTensorNames.length; ++i) {\n                const index = layerOutputNames.indexOf(symbolicTensorNames[i]);\n                if (index !== -1) {\n                    outputSymbolicTensors[i] = layerOutputs[index];\n                    outputsRemaining--;\n                }\n                if (outputsRemaining === 0) {\n                    break;\n                }\n            }\n            if (outputsRemaining === 0) {\n                break;\n            }\n        }\n        if (outputsRemaining > 0) {\n            const remainingNames = [];\n            outputSymbolicTensors.forEach((tensor, i) => {\n                if (tensor == null) {\n                    remainingNames.push(symbolicTensorNames[i]);\n                }\n            });\n            throw new ValueError(`Cannot find SymbolicTensors for output name(s): ` +\n                `${JSON.stringify(remainingNames)}`);\n        }\n        return outputSymbolicTensors;\n    }\n    /**\n     * Helper method to loop over some data in batches.\n     *\n     * Porting Note: Not using the functional approach in the Python equivalent\n     *   due to the imperative backend.\n     * Porting Note: Does not support step mode currently.\n     *\n     * @param ins: input data\n     * @param batchSize: integer batch size.\n     * @param verbose: verbosity model\n     * @returns: Predictions as `tf.Tensor` (if a single output) or an `Array` of\n     *   `tf.Tensor` (if multipe outputs).\n     */\n    predictLoop(ins, batchSize = 32, verbose = false) {\n        return tfc.tidy(() => {\n            const numSamples = this.checkNumSamples(ins);\n            if (verbose) {\n                throw new NotImplementedError('Verbose predictLoop() is not implemented yet.');\n            }\n            // Sample-based predictions.\n            // Porting Note: Tensor currently does not support sliced assignments as\n            //   in numpy, e.g., x[1:3] = y. Therefore we use concatenation while\n            //   iterating over the batches.\n            const batches = makeBatches(numSamples, batchSize);\n            const outsBatches = this.outputs.map(output => []);\n            // TODO(cais): Can the scope() be pushed down inside the for loop?\n            for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n                const batchOuts = tfc.tidy(() => {\n                    const batchStart = batches[batchIndex][0];\n                    const batchEnd = batches[batchIndex][1];\n                    // TODO(cais): Take care of the case of the last element is a flag for\n                    //   training/test.\n                    const insBatch = sliceArrays(ins, batchStart, batchEnd);\n                    // Construct the feeds for execute();\n                    const feeds = [];\n                    if (Array.isArray(insBatch)) {\n                        for (let i = 0; i < insBatch.length; ++i) {\n                            feeds.push({ key: this.inputs[i], value: insBatch[i] });\n                        }\n                    }\n                    else {\n                        feeds.push({ key: this.inputs[0], value: insBatch });\n                    }\n                    const feedDict = new FeedDict(feeds);\n                    return execute(this.outputs, feedDict);\n                });\n                batchOuts.forEach((batchOut, i) => outsBatches[i].push(batchOut));\n            }\n            return singletonOrArray(outsBatches.map(batches => tfc.concat(batches, 0)));\n        });\n    }\n    /**\n     * Generates output predictions for the input samples.\n     *\n     * Computation is done in batches.\n     *\n     * Note: the \"step\" mode of predict() is currently not supported.\n     *   This is because the TensorFlow.js core backend is imperative only.\n     *\n     * ```js\n     * const model = tf.sequential({\n     *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n     * });\n     * model.predict(tf.ones([8, 10]), {batchSize: 4}).print();\n     * ```\n     *\n     * @param x The input data, as a Tensor, or an `Array` of `tf.Tensor`s if\n     *   the model has multiple inputs.\n     * @param args A `ModelPredictArgs` object containing optional fields.\n     *\n     * @return Prediction results as a `tf.Tensor`(s).\n     *\n     * @exception ValueError In case of mismatch between the provided input data\n     *   and the model's expectations, or in case a stateful model receives a\n     *   number of samples that is not a multiple of the batch size.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    predict(x, args = {}) {\n        const xsRank2OrHigher = ensureTensorsRank2OrHigher(x);\n        checkInputData(xsRank2OrHigher, this.inputNames, this.feedInputShapes, false);\n        try {\n            // TODO(cais): Take care of stateful models.\n            //   if (this.stateful) ...\n            // TODO(cais): Take care of the learning_phase boolean flag.\n            //   if (this.useLearningPhase) ...\n            const batchSize = args.batchSize == null ? 32 : args.batchSize;\n            checkBatchSize(batchSize);\n            return this.predictLoop(xsRank2OrHigher, batchSize);\n        }\n        finally {\n            disposeNewTensors(xsRank2OrHigher, x);\n        }\n    }\n    /**\n     * Returns predictions for a single batch of samples.\n     *\n     * ```js\n     * const model = tf.sequential({\n     *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n     * });\n     * model.predictOnBatch(tf.ones([8, 10])).print();\n     * ```\n     * @param x: Input samples, as a Tensor (for models with exactly one\n     *   input) or an array of Tensors (for models with more than one input).\n     * @return Tensor(s) of predictions\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    predictOnBatch(x) {\n        checkInputData(x, this.inputNames, this.feedInputShapes, true);\n        // TODO(cais): Take care of the learning_phase boolean flag.\n        //   if (this.useLearningPhase) ...\n        const batchSize = (Array.isArray(x) ? x[0] : x).shape[0];\n        return this.predictLoop(x, batchSize);\n    }\n    standardizeUserDataXY(x, y, checkBatchAxis = true, batchSize) {\n        // TODO(cais): Add sampleWeight, classWeight\n        if (this.optimizer_ == null) {\n            throw new RuntimeError('You must compile a model before training/testing. Use ' +\n                'LayersModel.compile(modelCompileArgs).');\n        }\n        const outputShapes = [];\n        for (let i = 0; i < this.feedOutputShapes.length; ++i) {\n            const outputShape = this.feedOutputShapes[i];\n            const lossFn = this.feedLossFns[i];\n            if (lossFn === losses.sparseCategoricalCrossentropy) {\n                outputShapes.push(outputShape.slice(0, outputShape.length - 1).concat([1]));\n            }\n            else {\n                // Porting Note: Because of strong typing `lossFn` must be a function.\n                outputShapes.push(outputShape);\n            }\n        }\n        x = standardizeInputData(x, this.feedInputNames, this.feedInputShapes, false, 'input');\n        y = standardizeInputData(y, this.feedOutputNames, outputShapes, false, 'target');\n        // TODO(cais): Standardize sampleWeights & classWeights.\n        checkArrayLengths(x, y, null);\n        // TODO(cais): Check sampleWeights as well.\n        checkLossAndTargetCompatibility(y, this.feedLossFns, this.feedOutputShapes);\n        if (this.stateful && batchSize != null && batchSize > 0) {\n            if (x[0].shape[0] % batchSize !== 0) {\n                throw new ValueError(`In a stateful network, you should only pass inputs with a ` +\n                    `number of samples that is divisible by the batch size ` +\n                    `${batchSize}. Found: ${x[0].shape[0]} sample(s).`);\n            }\n        }\n        return [x, y];\n    }\n    async standardizeUserData(x, y, sampleWeight, classWeight, checkBatchAxis = true, batchSize) {\n        const [standardXs, standardYs] = this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n        // TODO(cais): Handle sampleWeights.\n        if (sampleWeight != null) {\n            throw new Error('sample weight is not supported yet.');\n        }\n        let standardSampleWeights = null;\n        if (classWeight != null) {\n            const classWeights = standardizeClassWeights(classWeight, this.outputNames);\n            standardSampleWeights = [];\n            for (let i = 0; i < classWeights.length; ++i) {\n                standardSampleWeights.push(await standardizeWeights(standardYs[i], null, classWeights[i]));\n            }\n        }\n        // TODO(cais): Deal with the case of model.stateful == true.\n        return [standardXs, standardYs, standardSampleWeights];\n    }\n    /**\n     * Loop over some test data in batches.\n     * @param f A Function returning a list of tensors.\n     * @param ins Array of tensors to be fed to `f`.\n     * @param batchSize Integer batch size or `null` / `undefined`.\n     * @param verbose verbosity mode.\n     * @param steps Total number of steps (batches of samples) before\n     * declaring test finished. Ignored with the default value of `null` /\n     * `undefined`.\n     * @returns Array of Scalars.\n     */\n    testLoop(f, ins, batchSize, verbose = 0, steps) {\n        return tfc.tidy(() => {\n            const numSamples = this.checkNumSamples(ins, batchSize, steps, 'steps');\n            const outs = [];\n            if (verbose > 0) {\n                throw new NotImplementedError('Verbose mode is not implemented yet.');\n            }\n            // TODO(cais): Use `indicesForConversionToDense' to prevent slow down.\n            if (steps != null) {\n                throw new NotImplementedError('steps mode in testLoop() is not implemented yet');\n            }\n            else {\n                const batches = makeBatches(numSamples, batchSize);\n                const indexArray = tensor1d(range(0, numSamples));\n                for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n                    const batchStart = batches[batchIndex][0];\n                    const batchEnd = batches[batchIndex][1];\n                    const batchIds = K.sliceAlongFirstAxis(indexArray, batchStart, batchEnd - batchStart);\n                    // TODO(cais): In ins, train flag can be a number, instead of an\n                    //   Tensor? Do we need to handle this in tfjs-layers?\n                    const insBatch = sliceArraysByIndices(ins, batchIds);\n                    const batchOuts = f(insBatch);\n                    if (batchIndex === 0) {\n                        for (let i = 0; i < batchOuts.length; ++i) {\n                            outs.push(scalar(0));\n                        }\n                    }\n                    for (let i = 0; i < batchOuts.length; ++i) {\n                        const batchOut = batchOuts[i];\n                        outs[i] =\n                            tfc.add(outs[i], tfc.mul(batchEnd - batchStart, batchOut));\n                    }\n                }\n                for (let i = 0; i < outs.length; ++i) {\n                    outs[i] = tfc.div(outs[i], numSamples);\n                }\n            }\n            return outs;\n        });\n    }\n    getDedupedMetricsNames() {\n        const outLabels = this.metricsNames;\n        // Rename duplicated metrics names (can happen with an output layer\n        // shared among multiple dataflows).\n        const dedupedOutLabels = [];\n        for (let i = 0; i < outLabels.length; ++i) {\n            const label = outLabels[i];\n            let newLabel = label;\n            if (count(outLabels, label) > 1) {\n                const dupIndex = count(outLabels.slice(0, i), label);\n                newLabel += `_${dupIndex}`;\n            }\n            dedupedOutLabels.push(newLabel);\n        }\n        return dedupedOutLabels;\n    }\n    /**\n     * Creates a function that performs the following actions:\n     *\n     * 1. computes the losses\n     * 2. sums them to get the total loss\n     * 3. call the optimizer computes the gradients of the LayersModel's\n     *    trainable weights w.r.t. the total loss and update the variables\n     * 4. calculates the metrics\n     * 5. returns the values of the losses and metrics.\n     */\n    makeTrainFunction() {\n        return (data) => {\n            const lossValues = [];\n            const inputs = data.slice(0, this.inputs.length);\n            const targets = data.slice(this.inputs.length, this.inputs.length + this.outputs.length);\n            const sampleWeights = data.slice(this.inputs.length + this.outputs.length, this.inputs.length + this.outputs.length * 2);\n            const metricsValues = [];\n            // Create a function that computes the total loss based on the\n            // inputs. This function is used for obtaining gradients through\n            // backprop.\n            const totalLossFunction = () => {\n                const feeds = [];\n                for (let i = 0; i < this.inputs.length; ++i) {\n                    feeds.push({ key: this.inputs[i], value: inputs[i] });\n                }\n                const feedDict = new FeedDict(feeds);\n                const outputs = execute(this.outputs, feedDict, { 'training': true });\n                // TODO(cais): Take care of the case of multiple outputs from a\n                //   single layer?\n                let totalLoss;\n                for (let i = 0; i < this.lossFunctions.length; ++i) {\n                    const lossFunction = this.lossFunctions[i];\n                    let loss = lossFunction(targets[i], outputs[i]);\n                    if (sampleWeights[i] != null) {\n                        loss = computeWeightedLoss(loss, sampleWeights[i]);\n                    }\n                    // TODO(cais): push Scalar instead.\n                    const meanLoss = tfc.mean(loss);\n                    // TODO(cais): Use a scope() instead, to avoid ownership.\n                    lossValues.push(meanLoss);\n                    if (i === 0) {\n                        totalLoss = loss;\n                    }\n                    else {\n                        totalLoss = tfc.add(totalLoss, loss);\n                    }\n                }\n                // Compute the metrics.\n                // TODO(cais): These should probably be calculated outside\n                //   totalLossFunction to benefit speed?\n                for (let i = 0; i < this.metricsTensors.length; ++i) {\n                    let weightedMetric;\n                    if (this.outputs.length > 1 && i < this.outputs.length) {\n                        weightedMetric = lossValues[i];\n                    }\n                    else {\n                        const metric = this.metricsTensors[i][0];\n                        const outputIndex = this.metricsTensors[i][1];\n                        weightedMetric =\n                            tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n                    }\n                    tfc.keep(weightedMetric);\n                    // TODO(cais): Use a scope() instead, to avoid ownership.\n                    metricsValues.push(weightedMetric);\n                }\n                totalLoss = tfc.mean(totalLoss);\n                // Add regularizer penalties.\n                this.calculateLosses().forEach(regularizerLoss => {\n                    totalLoss = tfc.add(totalLoss, regularizerLoss);\n                });\n                return totalLoss;\n            };\n            const variables = this.collectedTrainableWeights.map(param => param.read());\n            const returnCost = true;\n            const totalLossValue = this.optimizer_.minimize(totalLossFunction, returnCost, variables);\n            return [totalLossValue].concat(metricsValues);\n        };\n    }\n    /**\n     * Create a function which, when invoked with an array of `tf.Tensor`s as a\n     * batch of inputs, returns the prespecified loss and metrics of the model\n     * under the batch of input data.\n     */\n    makeTestFunction() {\n        this.testFunction = (data) => {\n            return tfc.tidy(() => {\n                const valOutputs = [];\n                let totalLoss;\n                const inputs = data.slice(0, this.inputs.length);\n                const targets = data.slice(this.inputs.length, this.inputs.length + this.outputs.length);\n                const feeds = [];\n                for (let i = 0; i < this.inputs.length; ++i) {\n                    feeds.push({ key: this.inputs[i], value: inputs[i] });\n                }\n                const feedDict = new FeedDict(feeds);\n                const outputs = execute(this.outputs, feedDict);\n                // Compute total loss.\n                for (let i = 0; i < this.lossFunctions.length; ++i) {\n                    const lossFunction = this.lossFunctions[i];\n                    // TODO(cais): Add sample weighting and replace the simple\n                    // averaging.\n                    const loss = tfc.mean(lossFunction(targets[i], outputs[i]));\n                    if (i === 0) {\n                        totalLoss = loss;\n                    }\n                    else {\n                        totalLoss = tfc.add(totalLoss, loss);\n                    }\n                    valOutputs.push(totalLoss);\n                }\n                // Compute the metrics.\n                for (let i = 0; i < this.metricsTensors.length; ++i) {\n                    const metric = this.metricsTensors[i][0];\n                    const outputIndex = this.metricsTensors[i][1];\n                    // TODO(cais): Replace K.mean() with a proper weighting function.\n                    const meanMetric = tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n                    valOutputs.push(meanMetric);\n                }\n                return valOutputs;\n            });\n        };\n    }\n    /**\n     * Trains the model for a fixed number of epochs (iterations on a\n     * dataset).\n     *\n     * ```js\n     * const model = tf.sequential({\n     *     layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n     * });\n     * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n     * for (let i = 1; i < 5 ; ++i) {\n     *   const h = await model.fit(tf.ones([8, 10]), tf.ones([8, 1]), {\n     *       batchSize: 4,\n     *       epochs: 3\n     *   });\n     *   console.log(\"Loss after Epoch \" + i + \" : \" + h.history.loss[0]);\n     * }\n     * ```\n     *\n     * @param x `tf.Tensor` of training data, or an array of `tf.Tensor`s if the\n     * model has multiple inputs. If all inputs in the model are named, you\n     * can also pass a dictionary mapping input names to `tf.Tensor`s.\n     * @param y `tf.Tensor` of target (label) data, or an array of `tf.Tensor`s if\n     * the model has multiple outputs. If all outputs in the model are named,\n     * you can also pass a dictionary mapping output names to `tf.Tensor`s.\n     * @param args A `ModelFitArgs`, containing optional fields.\n     *\n     * @return A `History` instance. Its `history` attribute contains all\n     *   information collected during training.\n     *\n     * @exception ValueError In case of mismatch between the provided input\n     * data and what the model expects.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    async fit(x, y, args = {}) {\n        return fitTensors(this, x, y, args);\n    }\n    // TODO(cais): Add code snippet below when it's possible to instantiate\n    //   actual dataset objects.\n    /**\n     * Trains the model using a dataset object.\n     *\n     * @param dataset A dataset object. Its `iterator()` method is expected\n     *   to generate a dataset iterator object, the `next()` method of which\n     *   is expected to produce data batches for training. The return value\n     *   of the `next()` call ought to contain a boolean `done` field and a\n     *   `value` field. The `value` field is expected to be an array of two\n     *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n     *   case is for models with exactly one input and one output (e.g..\n     *   a sequential model). The latter case is for models with multiple\n     *   inputs and/or multiple outputs.\n     *   Of the two items in the array, the first is the input feature(s) and\n     *   the second is the output target(s).\n     * @param args A `ModelFitDatasetArgs`, containing optional fields.\n     *\n     * @return A `History` instance. Its `history` attribute contains all\n     *   information collected during training.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    async fitDataset(dataset, args) {\n        return fitDataset(this, dataset, args);\n    }\n    /**\n     * Runs a single gradient update on a single batch of data.\n     *\n     * This method differs from `fit()` and `fitDataset()` in the following\n     * regards:\n     *   - It operates on exactly one batch of data.\n     *   - It returns only the loss and matric values, instead of\n     *     returning the batch-by-batch loss and metric values.\n     *   - It doesn't support fine-grained options such as verbosity and\n     *     callbacks.\n     *\n     * @param x Input data. It could be one of the following:\n     *   - A `tf.Tensor`, or an Array of `tf.Tensor`s (in case the model has\n     *     multiple inputs).\n     *   - An Object mapping input names to corresponding `tf.Tensor` (if the\n     *     model has named inputs).\n     * @param y Target darta. It could be either a `tf.Tensor` a multiple\n     *   `tf.Tensor`s. It should be consistent with `x`.\n     * @returns Training loss or losses (in case the model has\n     *   multiple outputs), along with metrics (if any), as numbers.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    async trainOnBatch(x, y) {\n        // TODO(cais): Support sampleWeight and classWeight.\n        // TODO(cais): Support Dataset objects.\n        const standardizeOut = await this.standardizeUserData(x, y);\n        const inputs = standardizeOut[0];\n        const targets = standardizeOut[1];\n        const trainFunction = this.makeTrainFunction();\n        const losses = trainFunction(inputs.concat(targets));\n        const lossValues = [];\n        for (const loss of losses) {\n            const v = await loss.data();\n            lossValues.push(v[0]);\n        }\n        tfc.dispose(losses);\n        return singletonOrArray(lossValues);\n    }\n    /**\n     * Extract weight values of the model.\n     *\n     * @param config: An instance of `io.SaveConfig`, which specifies\n     * model-saving options such as whether only trainable weights are to be\n     * saved.\n     * @returns A `NamedTensorMap` mapping original weight names (i.e.,\n     *   non-uniqueified weight names) to their values.\n     */\n    getNamedWeights(config) {\n        const namedWeights = [];\n        const trainableOnly = config != null && config.trainableOnly;\n        const weights = trainableOnly ? this.trainableWeights : this.weights;\n        const weightValues = this.getWeights(trainableOnly);\n        for (let i = 0; i < weights.length; ++i) {\n            if (trainableOnly && !weights[i].trainable) {\n                // Optionally skip non-trainable weights.\n                continue;\n            }\n            namedWeights.push({ name: weights[i].originalName, tensor: weightValues[i] });\n        }\n        return namedWeights;\n    }\n    /**\n     * Setter used for force stopping of LayersModel.fit() (i.e., training).\n     *\n     * Example:\n     *\n     * ```js\n     * const input = tf.input({shape: [10]});\n     * const output = tf.layers.dense({units: 1}).apply(input);\n     * const model = tf.model({inputs: [input], outputs: [output]});\n     * model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n     * const xs = tf.ones([8, 10]);\n     * const ys = tf.zeros([8, 1]);\n     *\n     * const history = await model.fit(xs, ys, {\n     *   epochs: 10,\n     *   callbacks: {\n     *     onEpochEnd: async (epoch, logs) => {\n     *       if (epoch === 2) {\n     *         model.stopTraining = true;\n     *       }\n     *     }\n     *   }\n     * });\n     *\n     * // There should be only 3 values in the loss array, instead of 10\n     * values,\n     * // due to the stopping after 3 epochs.\n     * console.log(history.history.loss);\n     * ```\n     */\n    set stopTraining(stop) {\n        this.stopTraining_ = stop;\n    }\n    get stopTraining() {\n        return this.stopTraining_;\n    }\n    get optimizer() {\n        return this.optimizer_;\n    }\n    set optimizer(optimizer) {\n        if (this.optimizer_ !== optimizer) {\n            this.optimizer_ = optimizer;\n            this.isOptimizerOwned = false;\n        }\n    }\n    dispose() {\n        const result = super.dispose();\n        if (result.refCountAfterDispose === 0 && this.optimizer != null &&\n            this.isOptimizerOwned) {\n            const numTensorsBeforeOptmizerDisposal = tfc.memory().numTensors;\n            this.optimizer_.dispose();\n            result.numDisposedVariables +=\n                numTensorsBeforeOptmizerDisposal - tfc.memory().numTensors;\n        }\n        return result;\n    }\n    getLossIdentifiers() {\n        let lossNames;\n        if (typeof this.loss === 'string') {\n            lossNames = toSnakeCase(this.loss);\n        }\n        else if (Array.isArray(this.loss)) {\n            for (const loss of this.loss) {\n                if (typeof loss !== 'string') {\n                    throw new Error('Serialization of non-string loss is not supported.');\n                }\n            }\n            lossNames = this.loss.map(name => toSnakeCase(name));\n        }\n        else {\n            const outputNames = Object.keys(this.loss);\n            lossNames = {};\n            const losses = this.loss;\n            for (const outputName of outputNames) {\n                if (typeof losses[outputName] === 'string') {\n                    lossNames[outputName] =\n                        toSnakeCase(losses[outputName]);\n                }\n                else {\n                    throw new Error('Serialization of non-string loss is not supported.');\n                }\n            }\n        }\n        return lossNames;\n    }\n    getMetricIdentifiers() {\n        if (typeof this.metrics === 'string' ||\n            typeof this.metrics === 'function') {\n            return [toSnakeCase(Metrics.getLossOrMetricName(this.metrics))];\n        }\n        else if (Array.isArray(this.metrics)) {\n            return this.metrics.map(metric => toSnakeCase(Metrics.getLossOrMetricName(metric)));\n        }\n        else {\n            const metricsIdentifiers = {};\n            for (const key in this.metrics) {\n                metricsIdentifiers[key] =\n                    toSnakeCase(Metrics.getLossOrMetricName(this.metrics[key]));\n            }\n            return metricsIdentifiers;\n        }\n    }\n    getTrainingConfig() {\n        return {\n            loss: this.getLossIdentifiers(),\n            metrics: this.getMetricIdentifiers(),\n            optimizer_config: {\n                class_name: this.optimizer.getClassName(),\n                config: this.optimizer.getConfig()\n            }\n        };\n        // TODO(cais): Add weight_metrics when they are supported.\n        // TODO(cais): Add sample_weight_mode when it's supported.\n        // TODO(cais): Add loss_weights when it's supported.\n    }\n    loadTrainingConfig(trainingConfig) {\n        if (trainingConfig.weighted_metrics != null) {\n            throw new Error('Loading weight_metrics is not supported yet.');\n        }\n        if (trainingConfig.loss_weights != null) {\n            throw new Error('Loading loss_weights is not supported yet.');\n        }\n        if (trainingConfig.sample_weight_mode != null) {\n            throw new Error('Loading sample_weight_mode is not supported yet.');\n        }\n        const tsConfig = convertPythonicToTs(trainingConfig.optimizer_config);\n        const optimizer = deserialize(tsConfig);\n        let loss;\n        if (typeof trainingConfig.loss === 'string') {\n            loss = toCamelCase(trainingConfig.loss);\n        }\n        else if (Array.isArray(trainingConfig.loss)) {\n            loss = trainingConfig.loss.map(lossEntry => toCamelCase(lossEntry));\n        }\n        else if (trainingConfig.loss != null) {\n            loss = {};\n            for (const key in trainingConfig.loss) {\n                loss[key] = toCamelCase(trainingConfig.loss[key]);\n            }\n        }\n        let metrics;\n        if (Array.isArray(trainingConfig.metrics)) {\n            metrics = trainingConfig.metrics.map(metric => toCamelCase(metric));\n        }\n        else if (trainingConfig.metrics != null) {\n            metrics = {};\n            for (const key in trainingConfig.metrics) {\n                metrics[key] = toCamelCase(trainingConfig.metrics[key]);\n            }\n        }\n        this.compile({ loss, metrics, optimizer });\n    }\n    /**\n     * Save the configuration and/or weights of the LayersModel.\n     *\n     * An `IOHandler` is an object that has a `save` method of the proper\n     * signature defined. The `save` method manages the storing or\n     * transmission of serialized data (\"artifacts\") that represent the\n     * model's topology and weights onto or via a specific medium, such as\n     * file downloads, local storage, IndexedDB in the web browser and HTTP\n     * requests to a server. TensorFlow.js provides `IOHandler`\n     * implementations for a number of frequently used saving mediums, such as\n     * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n     * for more details.\n     *\n     * This method also allows you to refer to certain types of `IOHandler`s\n     * as URL-like string shortcuts, such as 'localstorage://' and\n     * 'indexeddb://'.\n     *\n     * Example 1: Save `model`'s topology and weights to browser [local\n     * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n     * then load it back.\n     *\n     * ```js\n     * const model = tf.sequential(\n     *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n     * console.log('Prediction from original model:');\n     * model.predict(tf.ones([1, 3])).print();\n     *\n     * const saveResults = await model.save('localstorage://my-model-1');\n     *\n     * const loadedModel = await tf.loadLayersModel('localstorage://my-model-1');\n     * console.log('Prediction from loaded model:');\n     * loadedModel.predict(tf.ones([1, 3])).print();\n     * ```\n     *\n     * Example 2. Saving `model`'s topology and weights to browser\n     * [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API);\n     * then load it back.\n     *\n     * ```js\n     * const model = tf.sequential(\n     *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n     * console.log('Prediction from original model:');\n     * model.predict(tf.ones([1, 3])).print();\n     *\n     * const saveResults = await model.save('indexeddb://my-model-1');\n     *\n     * const loadedModel = await tf.loadLayersModel('indexeddb://my-model-1');\n     * console.log('Prediction from loaded model:');\n     * loadedModel.predict(tf.ones([1, 3])).print();\n     * ```\n     *\n     * Example 3. Saving `model`'s topology and weights as two files\n     * (`my-model-1.json` and `my-model-1.weights.bin`) downloaded from\n     * browser.\n     *\n     * ```js\n     * const model = tf.sequential(\n     *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n     * const saveResults = await model.save('downloads://my-model-1');\n     * ```\n     *\n     * Example 4. Send  `model`'s topology and weights to an HTTP server.\n     * See the documentation of `tf.io.http` for more details\n     * including specifying request parameters and implementation of the\n     * server.\n     *\n     * ```js\n     * const model = tf.sequential(\n     *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n     * const saveResults = await model.save('http://my-server/model/upload');\n     * ```\n     *\n     * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n     * scheme-based string shortcut for `IOHandler`.\n     * @param config Options for saving the model.\n     * @returns A `Promise` of `SaveResult`, which summarizes the result of\n     * the saving, such as byte sizes of the saved artifacts for the model's\n     *   topology and weight values.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n     */\n    async save(handlerOrURL, config) {\n        if (typeof handlerOrURL === 'string') {\n            const handlers = io.getSaveHandlers(handlerOrURL);\n            if (handlers.length === 0) {\n                throw new ValueError(`Cannot find any save handlers for URL '${handlerOrURL}'`);\n            }\n            else if (handlers.length > 1) {\n                throw new ValueError(`Found more than one (${handlers.length}) save handlers for ` +\n                    `URL '${handlerOrURL}'`);\n            }\n            handlerOrURL = handlers[0];\n        }\n        if (handlerOrURL.save == null) {\n            throw new ValueError('LayersModel.save() cannot proceed because the IOHandler ' +\n                'provided does not have the `save` attribute defined.');\n        }\n        const weightDataAndSpecs = await io.encodeWeights(this.getNamedWeights(config));\n        const returnString = false;\n        const unusedArg = null;\n        const modelConfig = this.toJSON(unusedArg, returnString);\n        const modelArtifacts = {\n            modelTopology: modelConfig,\n            format: LAYERS_MODEL_FORMAT_NAME,\n            generatedBy: `TensorFlow.js tfjs-layers v${version}`,\n            convertedBy: null,\n        };\n        const includeOptimizer = config == null ? false : config.includeOptimizer;\n        if (includeOptimizer && this.optimizer != null) {\n            modelArtifacts.trainingConfig = this.getTrainingConfig();\n            const weightType = 'optimizer';\n            const { data: optimizerWeightData, specs: optimizerWeightSpecs } = await io.encodeWeights(await this.optimizer.getWeights(), weightType);\n            weightDataAndSpecs.specs.push(...optimizerWeightSpecs);\n            weightDataAndSpecs.data = io.concatenateArrayBuffers([weightDataAndSpecs.data, optimizerWeightData]);\n        }\n        if (this.userDefinedMetadata != null) {\n            // Check serialized size of user-defined metadata.\n            const checkSize = true;\n            checkUserDefinedMetadata(this.userDefinedMetadata, this.name, checkSize);\n            modelArtifacts.userDefinedMetadata = this.userDefinedMetadata;\n        }\n        modelArtifacts.weightData = weightDataAndSpecs.data;\n        modelArtifacts.weightSpecs = weightDataAndSpecs.specs;\n        return handlerOrURL.save(modelArtifacts);\n    }\n    /**\n     * Set user-defined metadata.\n     *\n     * The set metadata will be serialized together with the topology\n     * and weights of the model during `save()` calls.\n     *\n     * @param setUserDefinedMetadata\n     */\n    setUserDefinedMetadata(userDefinedMetadata) {\n        checkUserDefinedMetadata(userDefinedMetadata, this.name);\n        this.userDefinedMetadata = userDefinedMetadata;\n    }\n    /**\n     * Get user-defined metadata.\n     *\n     * The metadata is supplied via one of the two routes:\n     *   1. By calling `setUserDefinedMetadata()`.\n     *   2. Loaded during model loading (if the model is constructed\n     *      via `tf.loadLayersModel()`.)\n     *\n     * If no user-defined metadata is available from either of the\n     * two routes, this function will return `undefined`.\n     */\n    getUserDefinedMetadata() {\n        return this.userDefinedMetadata;\n    }\n}\n// The class name is 'Model' rather than 'LayersModel' for backwards\n// compatibility since this class name shows up in the serialization format.\n/** @nocollapse */\nLayersModel.className = 'Model';\nserialization.registerClass(LayersModel);\n/**\n * A `tf.Functional` is an alias to `tf.LayersModel`.\n *\n * See also:\n *   `tf.LayersModel`, `tf.Sequential`, `tf.loadLayersModel`.\n */\n/** @doc {heading: 'Models', subheading: 'Classes'} */\nexport class Functional extends LayersModel {\n}\nFunctional.className = 'Functional';\nserialization.registerClass(Functional);\n//# sourceMappingURL=training.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport { serialization } from '@tensorflow/tfjs-core';\nimport { getUid } from '../backend/state';\nimport { ValueError } from '../errors';\nimport { Layer, Node, SymbolicTensor } from './topology';\nexport class InputLayer extends Layer {\n    constructor(args) {\n        super({\n            dtype: args.dtype,\n            name: args.name != null ? args.name : getUid('input').toString()\n        });\n        // Normalize config.batchSize and config.sparse\n        if (args.batchSize == null) {\n            args.batchSize = null;\n        }\n        if (args.sparse == null) {\n            args.sparse = false;\n        }\n        this.trainable = false;\n        this.built = true;\n        this.sparse = args.sparse;\n        if (args.inputShape != null && args.batchInputShape != null) {\n            throw new ValueError('Only provide the inputShape OR ' +\n                'batchInputShape argument to inputLayer, not both at the same time.');\n        }\n        let batchInputShape = args.batchInputShape;\n        if (batchInputShape == null) {\n            if (args.inputShape == null) {\n                throw new ValueError('An InputLayer should be passed either a ' +\n                    '`batchInputShape` or an `inputShape`.');\n            }\n            else {\n                batchInputShape = [args.batchSize].concat(args.inputShape);\n            }\n        }\n        else {\n            // TODO(michaelterry): Backport to PyKeras\n            if (args.batchSize != null) {\n                throw new ValueError('Cannot specify batchSize if batchInputShape is ' +\n                    'specified when creating an InputLayer.');\n            }\n        }\n        const dtype = args.dtype || 'float32';\n        this.batchInputShape = batchInputShape;\n        this.dtype = dtype;\n        // TODO(michaelterry): Backport this to PyKeras?\n        this.inputSpec = [{ shape: batchInputShape }];\n        const inputTensor = new SymbolicTensor(this.dtype, this.batchInputShape, this, [], {}, this.name);\n        inputTensor.nodeIndex = 0;\n        inputTensor.tensorIndex = 0;\n        // Create an input node to add to this.outboundNode.\n        // (This call has side effects.)\n        // tslint:disable-next-line:no-unused-expression\n        new Node({\n            outboundLayer: this,\n            inboundLayers: [],\n            nodeIndices: [],\n            tensorIndices: [],\n            inputTensors: [inputTensor],\n            outputTensors: [inputTensor],\n            inputMasks: [null],\n            outputMasks: [null],\n            inputShapes: [batchInputShape],\n            outputShapes: [batchInputShape]\n        });\n    }\n    apply(inputs, kwargs) {\n        throw new ValueError('Cannot pass any input to an ' +\n            `InputLayer's apply() method. InputLayer name: ${this.name}`);\n    }\n    dispose() {\n        // dispose() for InputLayer is overridden as no-op.\n        return { refCountAfterDispose: this._refCount, numDisposedVariables: 0 };\n    }\n    getConfig() {\n        return {\n            batchInputShape: this.batchInputShape,\n            dtype: this.dtype,\n            sparse: this.sparse,\n            name: this.name\n        };\n    }\n}\n/** @nocollapse */\nInputLayer.className = 'InputLayer';\nserialization.registerClass(InputLayer);\nexport function Input(config) {\n    if (config.batchShape == null && config.shape == null) {\n        throw new Error('Please provide to Input either a `shape`' +\n            ' or a `batchShape` argument. Note that ' +\n            '`shape` does not include the batch ' +\n            'dimension.');\n    }\n    if (config.batchShape != null && config.shape != null) {\n        // TODO(michaelterry): Backport to PyKeras.\n        throw new ValueError('Please provide either a `shape` or `batchShape` ' +\n            'argument to Input, but not both.');\n    }\n    let batchShape = config.batchShape;\n    if (config.shape != null && batchShape == null) {\n        batchShape = [null].concat(config.shape);\n    }\n    let dtype = config.dtype;\n    if (dtype == null) {\n        dtype = 'float32';\n    }\n    const inputLayer = new InputLayer({\n        batchInputShape: batchShape,\n        name: config.name,\n        dtype,\n        sparse: config.sparse\n    });\n    const outputs = inputLayer.inboundNodes[0].outputTensors;\n    return outputs[0];\n}\n//# sourceMappingURL=input_layer.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/* Original source: keras/engine/topology.py */\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getNextUniqueTensorId, getUid } from '../backend/state';\nimport { getScopedTensorName, getUniqueTensorName, nameScope } from '../common';\nimport { AttributeError, NotImplementedError, RuntimeError, ValueError } from '../errors';\nimport { getInitializer } from '../initializers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as types_utils from '../utils/types_utils';\nimport * as variable_utils from '../utils/variable_utils';\nimport { batchGetValue, batchSetValue, LayerVariable } from '../variables';\n/**\n * Specifies the ndim, dtype and shape of every input to a layer.\n *\n * Every layer should expose (if appropriate) an `inputSpec` attribute:\n * a list of instances of InputSpec (one per input tensor).\n *\n * A null entry in a shape is compatible with any dimension,\n * a null shape is compatible with any shape.\n */\nexport class InputSpec {\n    constructor(args) {\n        this.dtype = args.dtype;\n        this.shape = args.shape;\n        /*\n          TODO(michaelterry): Could throw error if ndim and shape are both defined\n            (then backport).\n        */\n        if (args.shape != null) {\n            this.ndim = args.shape.length;\n        }\n        else {\n            this.ndim = args.ndim;\n        }\n        this.maxNDim = args.maxNDim;\n        this.minNDim = args.minNDim;\n        this.axes = args.axes || {};\n    }\n}\n/**\n * `tf.SymbolicTensor` is a placeholder for a Tensor without any concrete value.\n *\n * They are most often encountered when building a graph of `Layer`s for a\n * a `tf.LayersModel` and the input data's shape, but not values are known.\n *\n * @doc {heading: 'Models', 'subheading': 'Classes'}\n */\nexport class SymbolicTensor {\n    /**\n     *\n     * @param dtype\n     * @param shape\n     * @param sourceLayer The Layer that produced this symbolic tensor.\n     * @param inputs The inputs passed to sourceLayer's __call__() method.\n     * @param nodeIndex\n     * @param tensorIndex\n     * @param callArgs The keyword arguments passed to the __call__() method.\n     * @param name\n     * @param outputTensorIndex The index of this tensor in the list of outputs\n     *   returned by apply().\n     */\n    constructor(dtype, shape, sourceLayer, inputs, callArgs, name, outputTensorIndex) {\n        this.dtype = dtype;\n        this.shape = shape;\n        this.sourceLayer = sourceLayer;\n        this.inputs = inputs;\n        this.callArgs = callArgs;\n        this.outputTensorIndex = outputTensorIndex;\n        this.id = getNextUniqueTensorId();\n        if (name != null) {\n            this.originalName = getScopedTensorName(name);\n            this.name = getUniqueTensorName(this.originalName);\n        }\n        this.rank = shape.length;\n    }\n}\nlet _nextNodeID = 0;\n/**\n * A `Node` describes the connectivity between two layers.\n *\n * Each time a layer is connected to some new input,\n * a node is added to `layer.inboundNodes`.\n *\n * Each time the output of a layer is used by another layer,\n * a node is added to `layer.outboundNodes`.\n *\n * `nodeIndices` and `tensorIndices` are basically fine-grained coordinates\n * describing the origin of the `inputTensors`, verifying the following:\n *\n * `inputTensors[i] ==\n * inboundLayers[i].inboundNodes[nodeIndices[i]].outputTensors[\n *   tensorIndices[i]]`\n *\n * A node from layer A to layer B is added to:\n *     A.outboundNodes\n *     B.inboundNodes\n */\nexport class Node {\n    constructor(args, \n    // TODO(michaelterry): Define actual type for this.\n    callArgs) {\n        this.callArgs = callArgs;\n        this.id = _nextNodeID++;\n        /*\n          Layer instance (NOT a list).\n          this is the layer that takes a list of input tensors\n          and turns them into a list of output tensors.\n          the current node will be added to\n          the inboundNodes of outboundLayer.\n        */\n        this.outboundLayer = args.outboundLayer;\n        /*\n            The following 3 properties describe where\n            the input tensors come from: which layers,\n            and for each layer, which node and which\n            tensor output of each node.\n        */\n        // List of layer instances.\n        this.inboundLayers = args.inboundLayers;\n        // List of integers, 1:1 mapping with inboundLayers.\n        this.nodeIndices = args.nodeIndices;\n        // List of integers, 1:1 mapping with inboundLayers.\n        this.tensorIndices = args.tensorIndices;\n        /*\n            Following 2 properties:\n            tensor inputs and outputs of outboundLayer.\n        */\n        // List of tensors. 1:1 mapping with inboundLayers.\n        this.inputTensors = args.inputTensors;\n        // List of tensors, created by outboundLayer.call().\n        this.outputTensors = args.outputTensors;\n        /*\n            Following 2 properties: input and output masks.\n            List of tensors, 1:1 mapping with inputTensor.\n        */\n        this.inputMasks = args.inputMasks;\n        // List of tensors, created by outboundLayer.computeMask().\n        this.outputMasks = args.outputMasks;\n        // Following 2 properties: input and output shapes.\n        // List of shape tuples, shapes of inputTensors.\n        this.inputShapes = args.inputShapes;\n        // List of shape tuples, shapes of outputTensors.\n        this.outputShapes = args.outputShapes;\n        // Add nodes to all layers involved.\n        for (const layer of args.inboundLayers) {\n            if (layer != null) {\n                layer.outboundNodes.push(this);\n            }\n        }\n        args.outboundLayer.inboundNodes.push(this);\n    }\n    getConfig() {\n        const inboundNames = [];\n        for (const layer of this.inboundLayers) {\n            if (layer != null) {\n                inboundNames.push(layer.name);\n            }\n            else {\n                inboundNames.push(null);\n            }\n        }\n        return {\n            outboundLayer: this.outboundLayer ? this.outboundLayer.name : null,\n            inboundLayers: inboundNames,\n            nodeIndices: this.nodeIndices,\n            tensorIndices: this.tensorIndices\n        };\n    }\n}\nlet _nextLayerID = 0;\n/**\n * A layer is a grouping of operations and weights that can be composed to\n * create a `tf.LayersModel`.\n *\n * Layers are constructed by using the functions under the\n * [tf.layers](#Layers-Basic) namespace.\n *\n * @doc {heading: 'Layers', subheading: 'Classes', namespace: 'layers'}\n */\nexport class Layer extends serialization.Serializable {\n    constructor(args = {}) {\n        super();\n        this._callHook = null;\n        this._addedWeightNames = [];\n        // Porting Notes: PyKeras does not have this property in this base Layer\n        //   class. Instead lets Layer subclass set it dynamically and checks the\n        //   value with `hasattr`. In tfjs-layers, we let this be a member of this\n        //   base class.\n        this._stateful = false;\n        this.id = _nextLayerID++;\n        this.activityRegularizer = null;\n        this.inputSpec = null;\n        this.supportsMasking = false;\n        // These properties will be set upon call of this.build()\n        this._trainableWeights = [];\n        this._nonTrainableWeights = [];\n        this._losses = [];\n        this._updates = [];\n        this._built = false;\n        /*\n          These lists will be filled via successive calls\n          to this.addInboundNode().\n         */\n        this.inboundNodes = [];\n        this.outboundNodes = [];\n        let name = args.name;\n        if (!name) {\n            const prefix = this.getClassName();\n            name = generic_utils.toSnakeCase(prefix) + '_' + getUid(prefix);\n        }\n        this.name = name;\n        this.trainable_ = args.trainable == null ? true : args.trainable;\n        if (args.inputShape != null || args.batchInputShape != null) {\n            /*\n              In this case we will later create an input layer\n              to insert before the current layer\n             */\n            let batchInputShape;\n            if (args.batchInputShape != null) {\n                batchInputShape = args.batchInputShape;\n            }\n            else if (args.inputShape != null) {\n                let batchSize = null;\n                if (args.batchSize != null) {\n                    batchSize = args.batchSize;\n                }\n                batchInputShape = [batchSize].concat(args.inputShape);\n            }\n            this.batchInputShape = batchInputShape;\n            // Set dtype.\n            let dtype = args.dtype;\n            if (dtype == null) {\n                dtype = args.inputDType;\n            }\n            if (dtype == null) {\n                dtype = 'float32';\n            }\n            this.dtype = dtype;\n        }\n        if (args.weights != null) {\n            this.initialWeights = args.weights;\n        }\n        else {\n            this.initialWeights = null;\n        }\n        // The value of `_refCount` is initialized to null. When the layer is used\n        // in a symbolic way for the first time, it will be set to 1.\n        this._refCount = null;\n        this.fastWeightInitDuringBuild = false;\n    }\n    /**\n     * Converts a layer and its index to a unique (immutable type) name.\n     * This function is used internally with `this.containerNodes`.\n     * @param layer The layer.\n     * @param nodeIndex The layer's position (e.g. via enumerate) in a list of\n     *   nodes.\n     *\n     * @returns The unique name.\n     */\n    static nodeKey(layer, nodeIndex) {\n        return layer.name + '_ib-' + nodeIndex.toString();\n    }\n    /**\n     * Returns this.inboundNode at index nodeIndex.\n     *\n     * Porting note: This is a replacement for _get_node_attribute_at_index()\n     * @param nodeIndex\n     * @param attrName The name of the attribute related to request for this node.\n     */\n    getNodeAtIndex(nodeIndex, attrName) {\n        if (this.inboundNodes.length === 0) {\n            throw new RuntimeError('The layer has never been called ' +\n                `and thus has no defined ${attrName}.`);\n        }\n        if (this.inboundNodes.length <= nodeIndex) {\n            throw new ValueError(`Asked to get ${attrName} at node ${nodeIndex}, ` +\n                `but the layer has only ${this.inboundNodes.length} inbound nodes.`);\n        }\n        return this.inboundNodes[nodeIndex];\n    }\n    /**\n     * Retrieves the input tensor(s) of a layer at a given node.\n     *\n     * @param nodeIndex Integer, index of the node from which to retrieve the\n     *   attribute. E.g. `nodeIndex=0` will correspond to the first time the layer\n     *   was called.\n     *\n     * @return A tensor (or list of tensors if the layer has multiple inputs).\n     */\n    getInputAt(nodeIndex) {\n        return generic_utils.singletonOrArray(this.getNodeAtIndex(nodeIndex, 'input').inputTensors);\n    }\n    /**\n     * Retrieves the output tensor(s) of a layer at a given node.\n     *\n     * @param nodeIndex Integer, index of the node from which to retrieve the\n     *   attribute. E.g. `nodeIndex=0` will correspond to the first time the layer\n     *   was called.\n     *\n     * @return A tensor (or list of tensors if the layer has multiple outputs).\n     */\n    getOutputAt(nodeIndex) {\n        return generic_utils.singletonOrArray(this.getNodeAtIndex(nodeIndex, 'output').outputTensors);\n    }\n    // Properties\n    /**\n     * Retrieves the input tensor(s) of a layer.\n     *\n     * Only applicable if the layer has exactly one inbound node,\n     * i.e. if it is connected to one incoming layer.\n     *\n     * @return Input tensor or list of input tensors.\n     *\n     * @exception AttributeError if the layer is connected to more than one\n     *   incoming layers.\n     */\n    get input() {\n        if (this.inboundNodes.length > 1) {\n            throw new AttributeError(`Layer ${this.name}` +\n                ' has multiple inbound nodes, ' +\n                'hence the notion of \"layer input\" ' +\n                'is ill-defined. ' +\n                'Use `getInputAt(nodeIndex)` instead.');\n        }\n        else if (this.inboundNodes.length === 0) {\n            throw new AttributeError(`Layer ${this.name}` +\n                ' is not connected, no input to return.');\n        }\n        return generic_utils.singletonOrArray(this.getNodeAtIndex(0, 'input').inputTensors);\n    }\n    /**\n     * Retrieves the output tensor(s) of a layer.\n     *\n     * Only applicable if the layer has exactly one inbound node,\n     * i.e. if it is connected to one incoming layer.\n     *\n     * @return Output tensor or list of output tensors.\n     *\n     * @exception AttributeError if the layer is connected to more than one\n     *   incoming layers.\n     */\n    get output() {\n        if (this.inboundNodes.length === 0) {\n            throw new AttributeError(`Layer ${this.name}` +\n                ' has no inbound nodes.');\n        }\n        if (this.inboundNodes.length > 1) {\n            throw new AttributeError(`Layer ${this.name}` +\n                ' has multiple inbound nodes, ' +\n                'hence the notion of \"layer output\" ' +\n                'is ill-defined. ' +\n                'Use `getOutputAt(nodeIndex)` instead.');\n        }\n        return generic_utils.singletonOrArray(this.getNodeAtIndex(0, 'output').outputTensors);\n    }\n    get losses() {\n        return this._losses;\n    }\n    /**\n     * Retrieves the Layer's current loss values.\n     *\n     * Used for regularizers during training.\n     */\n    calculateLosses() {\n        // Porting Node: This is an augmentation to Layer.loss in PyKeras.\n        //   In PyKeras, Layer.loss returns symbolic tensors. Here a concrete\n        //   Tensor (specifically Scalar) values are returned. This is due to the\n        //   imperative backend.\n        return this.losses.map(lossFn => lossFn());\n    }\n    get updates() {\n        return this._updates;\n    }\n    get built() {\n        return this._built;\n    }\n    set built(built) {\n        this._built = built;\n    }\n    get trainable() {\n        return this.trainable_;\n    }\n    set trainable(trainable) {\n        this._trainableWeights.forEach(w => w.trainable = trainable);\n        this.trainable_ = trainable;\n    }\n    get trainableWeights() {\n        if (this.trainable_) {\n            return this._trainableWeights.filter(w => w.trainable);\n        }\n        else {\n            return [];\n        }\n    }\n    set trainableWeights(weights) {\n        this._trainableWeights = weights;\n    }\n    get nonTrainableWeights() {\n        if (this.trainable) {\n            return this._trainableWeights.filter(w => !w.trainable)\n                .concat(this._nonTrainableWeights);\n        }\n        else {\n            return this._trainableWeights.concat(this._nonTrainableWeights);\n        }\n    }\n    set nonTrainableWeights(weights) {\n        this._nonTrainableWeights = weights;\n    }\n    /**\n     * The concatenation of the lists trainableWeights and nonTrainableWeights\n     * (in this order).\n     */\n    get weights() {\n        return this.trainableWeights.concat(this.nonTrainableWeights);\n    }\n    get stateful() {\n        return this._stateful;\n    }\n    /**\n     * Reset the states of the layer.\n     *\n     * This method of the base Layer class is essentially a no-op.\n     * Subclasses that are stateful (e.g., stateful RNNs) should override this\n     * method.\n     */\n    resetStates() {\n        if (!this.stateful) {\n            throw new Error('Cannot call the resetStates() method of a non-stateful Layer ' +\n                'object.');\n        }\n    }\n    /**\n     * Checks compatibility between the layer and provided inputs.\n     *\n     * This checks that the tensor(s) `input`\n     * verify the input assumptions of the layer\n     * (if any). If not, exceptions are raised.\n     *\n     * @param inputs Input tensor or list of input tensors.\n     *\n     * @exception ValueError in case of mismatch between\n     *   the provided inputs and the expectations of the layer.\n     */\n    assertInputCompatibility(inputs) {\n        inputs = generic_utils.toList(inputs);\n        if (this.inputSpec == null || this.inputSpec.length === 0) {\n            return;\n        }\n        const inputSpec = generic_utils.toList(this.inputSpec);\n        if (inputs.length !== inputSpec.length) {\n            throw new ValueError(`Layer ${this.name} expects ${inputSpec.length} inputs, ` +\n                `but it received ${inputs.length} input tensors. ` +\n                `Input received: ${inputs}`);\n        }\n        for (let inputIndex = 0; inputIndex < inputs.length; inputIndex++) {\n            const x = inputs[inputIndex];\n            const spec = inputSpec[inputIndex];\n            if (spec == null) {\n                continue;\n            }\n            // Check ndim.\n            const ndim = x.rank;\n            if (spec.ndim != null) {\n                if (ndim !== spec.ndim) {\n                    throw new ValueError(`Input ${inputIndex} is incompatible with layer ${this.name}: ` +\n                        `expected ndim=${spec.ndim}, found ndim=${ndim}`);\n                }\n            }\n            if (spec.maxNDim != null) {\n                if (ndim > spec.maxNDim) {\n                    throw new ValueError(`Input ${inputIndex} is incompatible with layer ${this.name}` +\n                        `: expected max_ndim=${spec.maxNDim}, found ndim=${ndim}`);\n                }\n            }\n            if (spec.minNDim != null) {\n                if (ndim < spec.minNDim) {\n                    throw new ValueError(`Input ${inputIndex} is incompatible with layer ${this.name}` +\n                        `: expected min_ndim=${spec.minNDim}, found ndim=${ndim}.`);\n                }\n            }\n            // Check dtype.\n            if (spec.dtype != null) {\n                if (x.dtype !== spec.dtype) {\n                    throw new ValueError(`Input ${inputIndex} is incompatible with layer ${this.name} ` +\n                        `: expected dtype=${spec.dtype}, found dtype=${x.dtype}.`);\n                }\n            }\n            // Check specific shape axes.\n            if (spec.axes) {\n                const xShape = x.shape;\n                for (const key in spec.axes) {\n                    const axis = Number(key);\n                    const value = spec.axes[key];\n                    // Perform Python-style slicing in case axis < 0;\n                    // TODO(cais): Use https://github.com/alvivi/typescript-underscore to\n                    // ensure type safety through Underscore calls.\n                    const xShapeAtAxis = axis >= 0 ? xShape[axis] : xShape[xShape.length + axis];\n                    if (value != null && [value, null].indexOf(xShapeAtAxis) === -1) {\n                        throw new ValueError(`Input ${inputIndex} is incompatible with layer ` +\n                            `${this.name}: expected axis ${axis} of input shape to ` +\n                            `have value ${value} but got shape ${xShape}.`);\n                    }\n                }\n            }\n            // Check shape.\n            if (spec.shape != null) {\n                for (let i = 0; i < spec.shape.length; ++i) {\n                    const specDim = spec.shape[i];\n                    const dim = x.shape[i];\n                    if (specDim != null && dim != null) {\n                        if (specDim !== dim) {\n                            throw new ValueError(`Input ${inputIndex} is incompatible with layer ` +\n                                `${this.name}: expected shape=${spec.shape}, ` +\n                                `found shape=${x.shape}.`);\n                        }\n                    }\n                }\n            }\n        }\n    }\n    /**\n     * This is where the layer's logic lives.\n     *\n     * @param inputs Input tensor, or list/tuple of input tensors.\n     * @param kwargs Additional keyword arguments.\n     *\n     * @return A tensor or list/tuple of tensors.\n     */\n    call(inputs, kwargs) {\n        return inputs;\n    }\n    invokeCallHook(inputs, kwargs) {\n        if (this._callHook != null) {\n            this._callHook(inputs, kwargs);\n        }\n    }\n    /**\n     * Set call hook.\n     * This is currently used for testing only.\n     * @param callHook\n     */\n    setCallHook(callHook) {\n        this._callHook = callHook;\n    }\n    /**\n     * Clear call hook.\n     * This is currently used for testing only.\n     */\n    clearCallHook() {\n        this._callHook = null;\n    }\n    /**\n     * Builds or executes a `Layer's logic.\n     *\n     * When called with `tf.Tensor`(s), execute the `Layer`s computation and\n     * return Tensor(s). For example:\n     *\n     * ```js\n     * const denseLayer = tf.layers.dense({\n     *   units: 1,\n     *   kernelInitializer: 'zeros',\n     *   useBias: false\n     * });\n     *\n     * // Invoke the layer's apply() method with a `tf.Tensor` (with concrete\n     * // numeric values).\n     * const input = tf.ones([2, 2]);\n     * const output = denseLayer.apply(input);\n     *\n     * // The output's value is expected to be [[0], [0]], due to the fact that\n     * // the dense layer has a kernel initialized to all-zeros and does not have\n     * // a bias.\n     * output.print();\n     * ```\n     *\n     * When called with `tf.SymbolicTensor`(s), this will prepare the layer for\n     * future execution.  This entails internal book-keeping on shapes of\n     * expected Tensors, wiring layers together, and initializing weights.\n     *\n     * Calling `apply` with `tf.SymbolicTensor`s are typically used during the\n     * building of non-`tf.Sequential` models. For example:\n     *\n     * ```js\n     * const flattenLayer = tf.layers.flatten();\n     * const denseLayer = tf.layers.dense({units: 1});\n     *\n     * // Use tf.layers.input() to obtain a SymbolicTensor as input to apply().\n     * const input = tf.input({shape: [2, 2]});\n     * const output1 = flattenLayer.apply(input);\n     *\n     * // output1.shape is [null, 4]. The first dimension is the undetermined\n     * // batch size. The second dimension comes from flattening the [2, 2]\n     * // shape.\n     * console.log(JSON.stringify(output1.shape));\n     *\n     * // The output SymbolicTensor of the flatten layer can be used to call\n     * // the apply() of the dense layer:\n     * const output2 = denseLayer.apply(output1);\n     *\n     * // output2.shape is [null, 1]. The first dimension is the undetermined\n     * // batch size. The second dimension matches the number of units of the\n     * // dense layer.\n     * console.log(JSON.stringify(output2.shape));\n     *\n     * // The input and output and be used to construct a model that consists\n     * // of the flatten and dense layers.\n     * const model = tf.model({inputs: input, outputs: output2});\n     * ```\n     *\n     * @param inputs a `tf.Tensor` or `tf.SymbolicTensor` or an Array of them.\n     * @param kwargs Additional keyword arguments to be passed to `call()`.\n     *\n     * @return Output of the layer's `call` method.\n     *\n     * @exception ValueError error in case the layer is missing shape information\n     *   for its `build` call.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    // Porting Note: This is a replacement for __call__() in Python.\n    apply(inputs, kwargs) {\n        kwargs = kwargs || {};\n        this.assertNotDisposed();\n        // Ensure inputs are all the same type.\n        const inputsList = generic_utils.toList(inputs);\n        let allAreSymbolic = true;\n        for (const input of inputsList) {\n            if (!(input instanceof SymbolicTensor)) {\n                allAreSymbolic = false;\n                break;\n            }\n        }\n        let noneAreSymbolic = true;\n        for (const input of inputsList) {\n            if (input instanceof SymbolicTensor) {\n                noneAreSymbolic = false;\n                break;\n            }\n        }\n        if (allAreSymbolic === noneAreSymbolic) {\n            throw new ValueError('Arguments to apply() must be all ' +\n                'SymbolicTensors or all Tensors');\n        }\n        // TODO(michaelterry): nameScope() may not be necessary.\n        return nameScope(this.name, () => {\n            // Handle laying building (weight creating, input spec locking).\n            if (!this.built) {\n                /*\n                  Throw exceptions in case the input is not compatible\n                  with the inputSpec specified in the layer constructor.\n                 */\n                this.assertInputCompatibility(inputs);\n                // Collect input shapes to build layer.\n                const inputShapes = [];\n                for (const xElem of generic_utils.toList(inputs)) {\n                    inputShapes.push(xElem.shape);\n                }\n                this.build(generic_utils.singletonOrArray(inputShapes));\n                this.built = true;\n                // Load weights that were specified at layer instantiation.\n                if (this.initialWeights) {\n                    this.setWeights(this.initialWeights);\n                }\n                if (this._refCount === null && noneAreSymbolic) {\n                    // The first use of this layer is a non-symbolic call, set ref count\n                    // to 1 so the Layer can be properly disposed if its dispose() method\n                    // is called.\n                    this._refCount = 1;\n                }\n            }\n            /*\n              Throw exceptions in case the input is not compatible\n              with the inputSpec set at build time.\n            */\n            this.assertInputCompatibility(inputs);\n            // Handle mask propagation.\n            // TODO(michaelterry): Mask propagation not currently implemented.\n            // Actually call the layer, collecting output(s), mask(s), and shape(s).\n            if (noneAreSymbolic) {\n                let output = this.call(inputs, kwargs);\n                // TODO(michaelterry): Compute the outputMask\n                // If the layer returns tensors from its inputs, unmodified,\n                // we copy them to avoid loss of tensor metadata.\n                const outputList = generic_utils.toList(output);\n                const outputListCopy = [];\n                // TODO(michaelterry): This copying may not be necessary given our eager\n                // backend.\n                for (let x of outputList) {\n                    if (inputsList.indexOf(x) !== -1) {\n                        x = x.clone();\n                    }\n                    outputListCopy.push(x);\n                }\n                output = generic_utils.singletonOrArray(outputListCopy);\n                if (this.activityRegularizer != null) {\n                    throw new NotImplementedError('Layer invocation in the presence of activity ' +\n                        'regularizer(s) is not supported yet.');\n                }\n                // TODO(michaelterry): Call addInboundNode()?\n                return output;\n            }\n            else {\n                const inputShape = collectInputShape(inputs);\n                const outputShape = this.computeOutputShape(inputShape);\n                let output;\n                const outputDType = guessOutputDType(inputs);\n                this.warnOnIncompatibleInputShape(Array.isArray(inputs) ? inputShape[0] :\n                    inputShape);\n                if (outputShape != null && outputShape.length > 0 &&\n                    Array.isArray(outputShape[0])) {\n                    // We have multiple output shapes. Create multiple output tensors.\n                    output = outputShape\n                        .map((shape, index) => new SymbolicTensor(outputDType, shape, this, generic_utils.toList(inputs), kwargs, this.name, index));\n                }\n                else {\n                    output = new SymbolicTensor(outputDType, outputShape, this, generic_utils.toList(inputs), kwargs, this.name);\n                }\n                /*\n                  Add an inbound node to the layer, so that it keeps track\n                  of the call and of all new variables created during the call.\n                  This also updates the layer history of the output tensor(s).\n                  If the input tensor(s) had no previous history,\n                  this does nothing.\n                */\n                this.addInboundNode(inputs, output, null, null, inputShape, outputShape, kwargs);\n                this._refCount++;\n                if (this.activityRegularizer != null) {\n                    throw new NotImplementedError('Layer invocation in the presence of activity ' +\n                        'regularizer(s) is not supported yet.');\n                }\n                return output;\n            }\n        });\n    }\n    /**\n     * Check compatibility between input shape and this layer's batchInputShape.\n     *\n     * Print warning if any incompatibility is found.\n     *\n     * @param inputShape Input shape to be checked.\n     */\n    warnOnIncompatibleInputShape(inputShape) {\n        if (this.batchInputShape == null) {\n            return;\n        }\n        else if (inputShape.length !== this.batchInputShape.length) {\n            console.warn(`The rank of the input tensor provided (shape: ` +\n                `${JSON.stringify(inputShape)}) does not match that of the ` +\n                `batchInputShape (${JSON.stringify(this.batchInputShape)}) ` +\n                `of the layer ${this.name}`);\n        }\n        else {\n            let dimMismatch = false;\n            this.batchInputShape.forEach((dimension, i) => {\n                if (dimension != null && inputShape[i] != null &&\n                    inputShape[i] !== dimension) {\n                    dimMismatch = true;\n                }\n            });\n            if (dimMismatch) {\n                console.warn(`The shape of the input tensor ` +\n                    `(${JSON.stringify(inputShape)}) does not ` +\n                    `match the expectation of layer ${this.name}: ` +\n                    `${JSON.stringify(this.batchInputShape)}`);\n            }\n        }\n    }\n    /**\n     * Retrieves the output shape(s) of a layer.\n     *\n     * Only applicable if the layer has only one inbound node, or if all inbound\n     * nodes have the same output shape.\n     *\n     * @returns Output shape or shapes.\n     * @throws AttributeError: if the layer is connected to more than one incoming\n     *   nodes.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    get outputShape() {\n        if (this.inboundNodes == null || this.inboundNodes.length === 0) {\n            throw new AttributeError(`The layer ${this.name} has never been called and thus has no ` +\n                `defined output shape.`);\n        }\n        const allOutputShapes = [];\n        for (const node of this.inboundNodes) {\n            const shapeString = JSON.stringify(node.outputShapes);\n            if (allOutputShapes.indexOf(shapeString) === -1) {\n                allOutputShapes.push(shapeString);\n            }\n        }\n        if (allOutputShapes.length === 1) {\n            const outputShapes = this.inboundNodes[0].outputShapes;\n            if (Array.isArray(outputShapes) && Array.isArray(outputShapes[0]) &&\n                outputShapes.length === 1) {\n                return outputShapes[0];\n            }\n            else {\n                return outputShapes;\n            }\n        }\n        else {\n            throw new AttributeError(`The layer ${this.name} has multiple inbound nodes with different ` +\n                `output shapes. Hence the notion of \"output shape\" is ill-defined ` +\n                `for the layer.`);\n            // TODO(cais): Implement getOutputShapeAt().\n        }\n    }\n    /**\n     * Counts the total number of numbers (e.g., float32, int32) in the\n     * weights.\n     *\n     * @returns An integer count.\n     * @throws RuntimeError: If the layer is not built yet (in which case its\n     *   weights are not defined yet.)\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    countParams() {\n        if (!this.built) {\n            throw new RuntimeError(`You tried to call countParams() on ${this.name}, ` +\n                `but the layer is not built yet. Build it first by calling ` +\n                `build(batchInputShape).`);\n        }\n        return variable_utils.countParamsInWeights(this.weights);\n    }\n    /**\n     * Creates the layer weights.\n     *\n     * Must be implemented on all layers that have weights.\n     *\n     * Called when apply() is called to construct the weights.\n     *\n     * @param inputShape A `Shape` or array of `Shape` (unused).\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    build(inputShape) {\n        this.built = true;\n    }\n    /**\n     * Returns the current values of the weights of the layer.\n     *\n     * @param trainableOnly Whether to get the values of only trainable weights.\n     * @returns Weight values as an `Array` of `tf.Tensor`s.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    getWeights(trainableOnly = false) {\n        return batchGetValue(trainableOnly ? this.trainableWeights : this.weights);\n    }\n    /**\n     * Sets the weights of the layer, from Tensors.\n     *\n     * @param weights a list of Tensors. The number of arrays and their shape\n     *   must match number of the dimensions of the weights of the layer (i.e.\n     *   it should match the output of `getWeights`).\n     *\n     * @exception ValueError If the provided weights list does not match the\n     *   layer's specifications.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    setWeights(weights) {\n        tidy(() => {\n            const params = this.weights;\n            if (params.length !== weights.length) {\n                // TODO(cais): Restore the following and use `providedWeights`, instead\n                // of `weights` in the error message, once the deeplearn.js bug is\n                // fixed: https://github.com/PAIR-code/deeplearnjs/issues/498 const\n                // providedWeights = JSON.stringify(weights).substr(0, 50);\n                throw new ValueError(`You called setWeights(weights) on layer \"${this.name}\" ` +\n                    `with a weight list of length ${weights.length}, ` +\n                    `but the layer was expecting ${params.length} weights. ` +\n                    `Provided weights: ${weights}...`);\n            }\n            if (params.length === 0) {\n                return;\n            }\n            const weightValueTuples = [];\n            const paramValues = batchGetValue(params);\n            for (let i = 0; i < paramValues.length; ++i) {\n                const pv = paramValues[i];\n                const p = params[i];\n                const w = weights[i];\n                if (!util.arraysEqual(pv.shape, w.shape)) {\n                    throw new ValueError(`Layer weight shape ${pv.shape} ` +\n                        `not compatible with provided weight shape ${w.shape}`);\n                }\n                weightValueTuples.push([p, w]);\n            }\n            batchSetValue(weightValueTuples);\n        });\n    }\n    /**\n     * Adds a weight variable to the layer.\n     *\n     * @param name Name of the new weight variable.\n     * @param shape The shape of the weight.\n     * @param dtype The dtype of the weight.\n     * @param initializer An initializer instance.\n     * @param regularizer A regularizer instance.\n     * @param trainable Whether the weight should be trained via backprop or not\n     *   (assuming that the layer itself is also trainable).\n     * @param constraint An optional trainable.\n     * @return The created weight variable.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    addWeight(name, shape, dtype, initializer, regularizer, trainable, constraint) {\n        // Reject duplicate weight names.\n        if (this._addedWeightNames.indexOf(name) !== -1) {\n            throw new ValueError(`Duplicate weight name ${name} for layer ${this.name}`);\n        }\n        this._addedWeightNames.push(name);\n        if (dtype == null) {\n            dtype = 'float32';\n        }\n        if (this.fastWeightInitDuringBuild) {\n            initializer = getInitializer('zeros');\n        }\n        const initValue = initializer.apply(shape, dtype);\n        const weight = new LayerVariable(initValue, dtype, name, trainable, constraint);\n        initValue.dispose();\n        // Request backend not to dispose the weights of the model on scope() exit.\n        if (regularizer != null) {\n            this.addLoss(() => regularizer.apply(weight.read()));\n        }\n        if (trainable == null) {\n            trainable = true;\n        }\n        if (trainable) {\n            this._trainableWeights.push(weight);\n        }\n        else {\n            this._nonTrainableWeights.push(weight);\n        }\n        return weight;\n    }\n    /**\n     * Set the fast-weight-initialization flag.\n     *\n     * In cases where the initialized weight values will be immediately\n     * overwritten by loaded weight values during model loading, setting\n     * the flag to `true` saves unnecessary calls to potentially expensive\n     * initializers and speeds up the loading process.\n     *\n     * @param value Target value of the flag.\n     */\n    setFastWeightInitDuringBuild(value) {\n        this.fastWeightInitDuringBuild = value;\n    }\n    /**\n     * Add losses to the layer.\n     *\n     * The loss may potentionally be conditional on some inputs tensors,\n     * for instance activity losses are conditional on the layer's inputs.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    addLoss(losses) {\n        if (losses == null || Array.isArray(losses) && losses.length === 0) {\n            return;\n        }\n        // Update this.losses\n        losses = generic_utils.toList(losses);\n        if (this._losses !== undefined && this._losses !== null) {\n            this.losses.push(...losses);\n        }\n    }\n    /**\n     * Computes the output shape of the layer.\n     *\n     * Assumes that the layer will be built to match that input shape provided.\n     *\n     * @param inputShape A shape (tuple of integers) or a list of shape tuples\n     *   (one per output tensor of the layer). Shape tuples can include null for\n     *   free dimensions, instead of an integer.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    /**\n     * Computes an output mask tensor.\n     *\n     * @param inputs Tensor or list of tensors.\n     * @param mask Tensor or list of tensors.\n     *\n     * @return null or a tensor (or list of tensors, one per output tensor of the\n     * layer).\n     */\n    computeMask(inputs, mask) {\n        if (!this.supportsMasking) {\n            if (mask != null) {\n                if (Array.isArray(mask)) {\n                    mask.forEach(maskElement => {\n                        if (maskElement != null) {\n                            throw new TypeError(`Layer ${this.name} does not support masking, ` +\n                                'but was passed an inputMask.');\n                        }\n                    });\n                }\n                else {\n                    throw new TypeError(`Layer ${this.name} does not support masking, ` +\n                        'but was passed an inputMask.');\n                }\n            }\n            // masking not explicitly supported: return null as mask\n            return null;\n        }\n        // if masking is explictly supported, by default\n        // carry over the input mask\n        return mask;\n    }\n    /**\n     * Internal method to create an inbound node for the layer.\n     *\n     * @param inputTensors List of input tensors.\n     * @param outputTensors List of output tensors.\n     * @param inputMasks List of input masks (a mask can be a tensor, or null).\n     * @param outputMasks List of output masks (a mask can be a tensor, or null).\n     * @param inputShapes List of input shape tuples.\n     * @param outputShapes List of output shape tuples.\n     * @param kwargs Dictionary of keyword arguments that were passed to the\n     *   `call` method of the layer at the call that created the node.\n     */\n    addInboundNode(inputTensors, outputTensors, inputMasks, outputMasks, inputShapes, outputShapes, kwargs = null) {\n        const inputTensorList = generic_utils.toList(inputTensors);\n        outputTensors = generic_utils.toList(outputTensors);\n        inputMasks = generic_utils.toList(inputMasks);\n        outputMasks = generic_utils.toList(outputMasks);\n        inputShapes = types_utils.normalizeShapeList(inputShapes);\n        outputShapes = types_utils.normalizeShapeList(outputShapes);\n        // Collect input tensor(s) coordinates.\n        const inboundLayers = [];\n        const nodeIndices = [];\n        const tensorIndices = [];\n        for (const x of inputTensorList) {\n            /*\n             * TODO(michaelterry): Keras adds this value to tensors; it's not\n             * clear whether we'll use this or not.\n             */\n            inboundLayers.push(x.sourceLayer);\n            nodeIndices.push(x.nodeIndex);\n            tensorIndices.push(x.tensorIndex);\n        }\n        // Create node, add it to inbound nodes.\n        // (This call has side effects.)\n        // tslint:disable-next-line:no-unused-expression\n        new Node({\n            outboundLayer: this,\n            inboundLayers,\n            nodeIndices,\n            tensorIndices,\n            inputTensors: inputTensorList,\n            outputTensors,\n            inputMasks,\n            outputMasks,\n            inputShapes,\n            outputShapes\n        }, kwargs);\n        // Update tensor history\n        for (let i = 0; i < outputTensors.length; i++) {\n            // TODO(michaelterry: _uses_learning_phase not tracked.\n            outputTensors[i].sourceLayer = this;\n            outputTensors[i].nodeIndex = this.inboundNodes.length - 1;\n            outputTensors[i].tensorIndex = i;\n        }\n    }\n    /**\n     * Returns the config of the layer.\n     *\n     * A layer config is a TS dictionary (serializable)\n     * containing the configuration of a layer.\n     * The same layer can be reinstantiated later\n     * (without its trained weights) from this configuration.\n     *\n     * The config of a layer does not include connectivity\n     * information, nor the layer class name.  These are handled\n     * by 'Container' (one layer of abstraction above).\n     *\n     * Porting Note: The TS dictionary follows TS naming standrds for\n     * keys, and uses tfjs-layers type-safe Enums.  Serialization methods\n     * should use a helper function to convert to the pythonic storage\n     * standard. (see serialization_utils.convertTsToPythonic)\n     *\n     * @returns TS dictionary of configuration.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    getConfig() {\n        const config = { name: this.name, trainable: this.trainable };\n        if (this.batchInputShape != null) {\n            config['batchInputShape'] = this.batchInputShape;\n        }\n        if (this.dtype != null) {\n            config['dtype'] = this.dtype;\n        }\n        return config;\n    }\n    /**\n     * Dispose the weight variables that this Layer instance holds.\n     *\n     * @returns {number} Number of disposed variables.\n     */\n    disposeWeights() {\n        this.weights.forEach(weight => weight.dispose());\n        return this.weights.length;\n    }\n    assertNotDisposed() {\n        if (this._refCount === 0) {\n            throw new Error(`Layer '${this.name}' is already disposed.`);\n        }\n    }\n    /**\n     * Attempt to dispose layer's weights.\n     *\n     * This method decrease the reference count of the Layer object by 1.\n     *\n     * A Layer is reference-counted. Its reference count is incremented by 1\n     * the first item its `apply()` method is called and when it becomes a part\n     * of a new `Node` (through calling the `apply()`) method on a\n     * `tf.SymbolicTensor`).\n     *\n     * If the reference count of a Layer becomes 0, all the weights will be\n     * disposed and the underlying memory (e.g., the textures allocated in WebGL)\n     * will be freed.\n     *\n     * Note: If the reference count is greater than 0 after the decrement, the\n     * weights of the Layer will *not* be disposed.\n     *\n     * After a Layer is disposed, it cannot be used in calls such as `apply()`,\n     * `getWeights()` or `setWeights()` anymore.\n     *\n     * @returns A DisposeResult Object with the following fields:\n     *   - refCountAfterDispose: The reference count of the Container after this\n     *     `dispose()` call.\n     *   - numDisposedVariables: Number of `tf.Variable`s (i.e., weights) disposed\n     *     during this `dispose()` call.\n     * @throws {Error} If the layer is not built yet, or if the layer has already\n     *   been disposed.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    dispose() {\n        if (!this.built) {\n            throw new Error(`Cannot dispose Layer ${this.name} because it has not been ` +\n                `built yet.`);\n        }\n        if (this._refCount === null) {\n            throw new Error(`Cannot dispose Layer ${this.name} because it has not been used ` +\n                `yet.`);\n        }\n        this.assertNotDisposed();\n        let numDisposedVariables = 0;\n        if (--this._refCount === 0) {\n            numDisposedVariables = this.disposeWeights();\n        }\n        return { refCountAfterDispose: this._refCount, numDisposedVariables };\n    }\n}\n/**\n * Collects the input shape(s) of a list of `tf.Tensor`s or\n * `tf.SymbolicTensor`s.\n *\n * TODO(michaelterry): Update PyKeras docs (backport).\n *\n * @param inputTensors List of input tensors (or single input tensor).\n *\n * @return List of shape tuples (or single tuple), one tuple per input.\n */\nfunction collectInputShape(inputTensors) {\n    inputTensors =\n        generic_utils.toList(inputTensors);\n    const shapes = [];\n    for (const x of inputTensors) {\n        shapes.push(x.shape);\n    }\n    return generic_utils.singletonOrArray(shapes);\n}\n/**\n * Guesses output dtype based on inputs.\n *\n * At present, just returns 'float32' for any input.\n *\n * @param inputTensors List of input tensors (or single input tensor).\n *\n * @return The guessed DType. At present, always returns 'float32'.\n */\nfunction guessOutputDType(inputTensors) {\n    return 'float32';\n}\n/**\n * Returns the list of input tensors necessary to compute `tensor`.\n *\n * Output will always be a list of tensors (potentially with 1 element).\n *\n * @param tensor The tensor to start from.\n * @param layer Origin layer of the tensor.\n * @param nodeIndex Origin node index of the tensor.\n *\n * @return Array of input tensors.\n */\nexport function getSourceInputs(tensor, layer, nodeIndex) {\n    if (layer == null || (nodeIndex != null && nodeIndex > 0)) {\n        layer = tensor.sourceLayer;\n        nodeIndex = tensor.nodeIndex;\n    }\n    if (layer.inboundNodes.length === 0) {\n        return [tensor];\n    }\n    else {\n        const node = layer.inboundNodes[nodeIndex];\n        if (node.inboundLayers.length === 0) {\n            return node.inputTensors;\n        }\n        else {\n            const sourceTensors = [];\n            for (let i = 0; i < node.inboundLayers.length; i++) {\n                const x = node.inputTensors[i];\n                const layer = node.inboundLayers[i];\n                const nodeIndex = node.nodeIndices[i];\n                const previousSources = getSourceInputs(x, layer, nodeIndex);\n                // Avoid input redundancy.\n                for (const x of previousSources) {\n                    if (sourceTensors.indexOf(x) === -1) {\n                        sourceTensors.push(x);\n                    }\n                }\n            }\n            return sourceTensors;\n        }\n    }\n}\n//# sourceMappingURL=topology.js.map"],"names":["FeedDict","constructor","feeds","this","id2Value","id2Mask","name2Id","id","feed","add","key","value","mask","name","val","dtype","cast","err","assertFeedCompatibility","addFeed","hasKey","names","Object","keys","getValue","getMask","disposeMasks","dispose","cachedSorted","cachedRecipientCounts","execute","fetches","feedDict","kwargs","probe","training","arrayFetches","Array","isArray","fetchArray","outputNames","map","t","finalOutputs","feedNames","outputName","indexOf","push","maxNumTensors","Infinity","minNumTensors","fetchAndFeedKey","join","sorted","recipientCounts","out","util","length","finalSorted","finalRecipientMap","getTopologicalSortAndRecipientCountsForOneFetch","recipientMap","visited","Set","fetch","symbolicTensor","has","forEach","recipient","recipientMap2Counts","getTopologicalSortAndRecipientCounts","assign","internalFeedDict","i","numTensors","memory","symbolic","srcLayer","sourceLayer","inputValues","inputMasks","tensorsToDispose","maskExists","input","inputs","isDisposed","stateful","outputTensors","apply","outputMask","supportsMasking","computeMask","layerOutputs","getNodeOutputs","outputSymbolicTensors","index","size","stack","marks","top","pop","topIsMarked","inboundNodes","output","nodeIndex","outputTensor","getOutputAt","standardizeSampleOrClassWeights","xWeight","weightType","numOutputs","Error","JSON","stringify","standardizeClassWeights","classWeight","async","standardizeWeights","y","sampleWeight","sampleWeightMode","yClasses","tidy","shape","clone","axis","argMax","reshape","rank","yClassIndices","from","data","classSampleWeight","classIndex","tensor1d","computeWeightedLoss","losses","sampleWeights","mul","standardizeDataIteratorOutput","model","iteratorOut","xs","ys","iteratorOutObj","flattenedXs","flattenTensorOrArrayOrMap","inputNames","flattenedYs","batchSize","outputs","xIndex","yIndex","inputOrOutput","values","result","fitDataset","dataset","args","hasBatchesPerEpoch","batchesPerEpoch","optimizer","epochs","Number","isInteger","isTraining","doValidation","validationData","valXs","valYs","isDatasetObject","validationBatches","standardizeTensorValidationData","trainFunction","makeTrainFunction","outLabels","getDedupedMetricsNames","callbackMetrics","slice","concat","n","callbacks","yieldEvery","verbose","callbackList","history","stepsPerEpoch","isFinite","getStepsPerEpoch","setModel","onTrainBegin","stopTraining_","epoch","initialEpoch","dataIterator","iterator","epochLogs","onEpochBegin","stepsDone","batchIndex","next","done","batchLogs","onBatchBegin","standardClassWeights","ins","outs","label","onBatchEnd","valOuts","evaluateDataset","batches","evaluate","validationBatchSize","metricsNames","onEpochEnd","onTrainEnd","syncData","checkBatchSize","sliceArrays","arrays","start","stop","array","sliceArraysByIndices","indices","toInt","makeBatches","batchStart","batchEnd","fitTensors","x","targets","inputValX","inputValY","valX","valY","checkBatchAxis","standardizedOuts","standardizeUserData","valIns","valStandardized","validationSplit","splitAt","Math","floor","originalBatchSize","validationSteps","checkTrainableWeightsConsistency","valFunction","makeTestFunction","testFunction","f","valF","shuffle","numTrainSamples","checkNumSamples","indexArray","epochIndexArray1D","batchIds","insBatch","testLoop","fitLoop","disposeNewTensors","ensureTensorsRank2OrHigher","tensors","Tensor","tensor","refTensors","oldTensorIds","oldTensor","isDataArray","isDataDict","isDataTensor","standardizeInputData","shapes","exceptionPrefix","gotUnexpectedData","hasOwnProperty","j","dim","refDim","checkInputData","LayersModel","super","summary","lineLength","positions","printFn","console","log","built","compile","loss","optimizer_","optimizers","isOptimizerOwned","Optimizer","lossFunctions","theLosses","l","lossFunction","_","feedOutputNames","feedOutputShapes","feedLossFns","internalOutputShapes","skipTargetIndices","metrics","metricsTensors","weightedLoss","nestedMetrics","wrappedMetrics","TypeError","outputMetrics","collectMetrics","appendMetric","outputIndex","metricName","metricTensor","accFn","weightedMetricFn","metric","outputShape","suffix","metricFn","metricResult","handleMetrics","collectedTrainableWeights","trainableWeights","standardizeUserDataXY","testOuts","steps","hasBatches","numExamples","batch","xsAndYs","batchOuts","scalar","batchOut","oldScalar","stepsName","numSamples","outputsIsArray","retrieveSymbolicTensors","tensorValue","executeOutputs","symbolicTensorNames","outputsRemaining","layer","layers","layerOutputNames","remainingNames","predictLoop","outsBatches","predict","xsRank2OrHigher","feedInputShapes","predictOnBatch","outputShapes","setX","sort","setY","target","checkArrayLengths","feedInputNames","lossFns","keyLosses","slicedYShape","slicedShape","targetDim","outDim","checkLossAndTargetCompatibility","standardXs","standardYs","standardSampleWeights","classWeights","dedupedOutLabels","newLabel","lossValues","metricsValues","variables","param","read","minimize","totalLoss","meanLoss","weightedMetric","calculateLosses","regularizerLoss","valOutputs","meanMetric","fit","trainOnBatch","standardizeOut","v","getNamedWeights","config","namedWeights","trainableOnly","weights","weightValues","getWeights","trainable","originalName","stopTraining","refCountAfterDispose","numTensorsBeforeOptmizerDisposal","numDisposedVariables","getLossIdentifiers","lossNames","getMetricIdentifiers","metricsIdentifiers","getTrainingConfig","optimizer_config","class_name","getClassName","getConfig","loadTrainingConfig","trainingConfig","weighted_metrics","loss_weights","sample_weight_mode","tsConfig","lossEntry","save","handlerOrURL","handlers","io","weightDataAndSpecs","modelArtifacts","modelTopology","toJSON","format","generatedBy","version","convertedBy","includeOptimizer","optimizerWeightData","specs","optimizerWeightSpecs","userDefinedMetadata","checkSize","weightData","weightSpecs","setUserDefinedMetadata","getUserDefinedMetadata","className","serialization","Functional","InputLayer","toString","sparse","inputShape","batchInputShape","inputSpec","inputTensor","tensorIndex","outboundLayer","inboundLayers","nodeIndices","tensorIndices","inputTensors","outputMasks","inputShapes","_refCount","Input","batchShape","InputSpec","ndim","maxNDim","minNDim","axes","SymbolicTensor","callArgs","outputTensorIndex","_nextNodeID","Node","outboundNodes","inboundNames","_nextLayerID","Layer","_callHook","_addedWeightNames","_stateful","activityRegularizer","_trainableWeights","_nonTrainableWeights","_losses","_updates","_built","prefix","trainable_","inputDType","initialWeights","fastWeightInitDuringBuild","nodeKey","getNodeAtIndex","attrName","getInputAt","lossFn","updates","w","filter","nonTrainableWeights","resetStates","assertInputCompatibility","inputIndex","spec","xShape","xShapeAtAxis","specDim","call","invokeCallHook","setCallHook","callHook","clearCallHook","assertNotDisposed","inputsList","allAreSymbolic","noneAreSymbolic","xElem","build","setWeights","outputList","outputListCopy","collectInputShape","computeOutputShape","outputDType","warnOnIncompatibleInputShape","addInboundNode","dimMismatch","dimension","allOutputShapes","node","shapeString","countParams","params","weightValueTuples","paramValues","pv","p","addWeight","initializer","regularizer","constraint","initValue","weight","addLoss","setFastWeightInitDuringBuild","undefined","maskElement","inputTensorList","disposeWeights","getSourceInputs","sourceTensors","previousSources"],"sourceRoot":""}