{"version":3,"file":"stylist-vendors-6e5319ad.e9cc3240e5750b720ac7.js","mappings":"2NAuBO,MAAMA,UAAkB,KAC3B,WAAAC,CAAYC,GAIR,GAHAC,MAAMD,GACNE,KAAKC,WAAa,KAClBD,KAAKE,+BAAiC,gBACV,MAAxBJ,EAAKK,iBAA8C,MAAnBL,EAAKM,WAAoB,CAKzD,IAAIC,EAAY,KACM,MAAlBP,EAAKO,YACLA,EAAYP,EAAKO,WAEG,MAApBP,EAAKQ,YAGLN,KAAKG,gBAAkB,CAACE,EAAW,MAKnCL,KAAKG,gBACD,CAACE,GAAWE,OAAO,KAAqBT,EAAKQ,aAEzD,CACAN,KAAKQ,SAAWV,EAAKU,SACrB,KAAoCR,KAAKQ,SAAU,YACnDR,KAAKS,UAAYX,EAAKW,UACtB,KAAoCT,KAAKS,UAAW,aACpDT,KAAKU,uBAAwB,QAAeZ,EAAKY,uBAAyBV,KAAKE,gCAC/EF,KAAKW,uBAAwB,QAAeb,EAAKa,uBACjDX,KAAKY,qBAAsB,QAAed,EAAKc,qBAC/CZ,KAAKa,sBAAuB,QAAcf,EAAKe,sBAC/Cb,KAAKc,SAAWhB,EAAKgB,SACrBd,KAAKe,gBAAkBjB,EAAKgB,SAC5Bd,KAAKM,YAAcR,EAAKQ,WAC5B,CACA,KAAAU,CAAMZ,GACFJ,KAAKC,WAAaD,KAAKiB,UAAU,aAAc,CAACjB,KAAKQ,SAAUR,KAAKS,WAAYT,KAAKkB,MAAOlB,KAAKU,sBAAuBV,KAAKW,uBAAuB,EAAMX,KAAKa,sBAC/Jb,KAAKmB,OAAQ,CACjB,CAGA,4BAAAC,CAA6BhB,GAAc,CAC3C,WAAAiB,CAAYC,EAAQC,GAChB,OAAO,IAAAC,OAAK,IACHxB,KAAKc,UAINQ,GAAS,QAAoBA,IACtB,IAAAG,UAASH,GAAQ,IAAAI,WAAUJ,KAJ3B,MAOnB,CACA,kBAAAK,CAAmBvB,GAEf,GADAA,GAAa,QAAmBA,GACR,MAApBJ,KAAKM,YACL,MAAO,IAAIF,EAAYJ,KAAKS,WAGhC,MAAMmB,EAAS,KAAqB5B,KAAKM,aACzC,GAAIsB,EAAOC,SAAWzB,EAAWyB,OAAS,EACtC,MAAM,IAAI,KAAW,oBAAoB7B,KAAKM,mDACjBF,KAE5B,CACD,IAAI0B,EAAI,EACR,IAAK,IAAIC,EAAI,EAAGA,EAAIH,EAAOC,SAAUE,EAAG,CACpC,MAAMC,EAAKJ,EAAOG,GACZE,EAAK7B,EAAW2B,EAAI,GAC1B,GAAW,MAANC,GAAsB,MAANC,GAAgBD,IAAOC,EACxC,MAAM,IAAI,KAAW,oBAAoBjC,KAAKM,mDACjBF,KAElB,MAAN4B,IACLJ,EAAOE,GAAKG,GAEhBH,GACJ,CACJ,CACA,MAAO,CAAC1B,EAAW,MAAOwB,EAAQ5B,KAAKS,UAC3C,CACA,IAAAyB,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACRxB,KAAKoC,eAAed,EAAQa,GAE5B,IAAIE,GAAQ,QAAoBf,GACZ,UAAhBe,EAAMnB,QACNmB,EAAQ,KAAOA,EAAO,UAG1B,OADe,KAASrC,KAAKC,WAAWqC,OAAQD,EAAME,QACxCC,SAAQ,QAAmBxC,KAAK2B,mBAAmBU,EAAMI,QAAQ,GAEvF,CACA,SAAAC,GACI,MAAMC,EAAS,CACXnC,SAAUR,KAAKQ,SACfC,UAAWT,KAAKS,UAChBC,uBAAuB,QAAqBV,KAAKU,uBACjDC,uBAAuB,QAAqBX,KAAKW,uBACjDC,qBAAqB,QAAqBZ,KAAKY,qBAC/CC,sBAAsB,QAAoBb,KAAKa,sBAC/CC,SAAUd,KAAKc,SACfR,YAAaN,KAAKM,aAEhBsC,EAAa7C,MAAM2C,YAEzB,OADAG,OAAOC,OAAOH,EAAQC,GACfD,CACX,EAGJ/C,EAAUmD,UAAY,YACtB,EAAAC,cAAA,cAA4BpD,E,8HCrGrB,SAASqD,EAAmBC,EAAGC,EAAMC,EAAUC,EAAMC,EAAOC,EAAU,MACzE,IAAIC,EACJ,GAAe,IAAXN,EAAEO,KACFD,EAAM,cAAgBN,EAAGC,EAAMC,EAAUC,EAAMC,EAAOC,QAErD,GAAe,IAAXL,EAAEO,KAEPD,EAAM,cAAgBN,EAAGC,EAAMC,EAAUC,EAAMC,EAAOC,OAErD,IAAe,IAAXL,EAAEO,KAIP,MAAM,IAAI,KAAoB,2DAA2DP,EAAEO,YAH3FD,EAAM,cAAgBN,EAAGC,EAAMC,EAAUC,EAAMC,EAAOC,EAK1D,CACA,OAAOC,CACX,CA6EO,SAASE,EAAyBR,EAAGI,EAAOD,EAAMM,EAAeJ,EAAU,MAC9E,OAAI,EAAAK,KAAA,YAAiBD,EAAcE,QAAQC,OAAQ,KAAiB,EAAGZ,EAAEO,KAAO,IA5DpF,SAAyCP,EAAGI,EAAOD,EAAMM,EAAeJ,EAAU,MAC9E,OAAO,IAAA/B,OAAK,KACR,MAAMuC,EAAkB,UAAYb,EAAGS,GACjCR,EAAOY,EAAgBZ,KACvBC,EAAWW,EAAgBX,SAEjC,MAAO,CADQH,EAAmBC,EAAGC,EAAMC,EAAUC,EAAMC,EAAOC,GAClDJ,EAAMC,EAAS,GAEvC,CAqDeY,CAAgCd,EAAGI,EAAOD,EAAMM,EAAeJ,GAnC9E,SAA2CL,EAAGI,EAAOD,EAAMM,EAAeJ,EAAU,MAChF,OAAO,IAAA/B,OAAK,KACR,MAAMuC,EAAkB,UAAYb,EAAGS,GACjCR,EAAOY,EAAgBZ,KACvBC,EAAWW,EAAgBX,SAC3Ba,EAAc,GACpB,IAAK,MAAMC,KAAQ,KAAiB,EAAGhB,EAAEO,OACA,IAAjCE,EAAcQ,QAAQD,GACtBD,EAAYG,KAAK,GAGjBH,EAAYG,KAAKlB,EAAET,MAAMyB,IAGjC,MAAMG,EAAgBlB,EAAKX,QAAQyB,GAC7BK,EAAoBlB,EAASZ,QAAQyB,GACrCM,EAA0B,MAATjB,EAAgB,KAAOA,EAAMd,QAAQyB,GACtDO,EAAwB,MAARnB,EAAe,KAAOA,EAAKb,QAAQyB,GAEzD,MAAO,CADQhB,EAAmBC,EAAGmB,EAAeC,EAAmBE,EAAeD,EAAgBhB,GACtFJ,EAAMC,EAAS,GAEvC,CAiBeqB,CAAkCvB,EAAGI,EAAOD,EAAMM,EAAeJ,EAEhF,CACO,MAAMmB,UAA2B,KACpC,WAAA7E,CAAYC,GACI,MAARA,IACAA,EAAO,CAAC,GAEZC,MAAMD,GACNE,KAAKe,iBAAkB,EACvBf,KAAKkE,KAAoB,MAAbpE,EAAKoE,MAAgB,EAAIpE,EAAKoE,KAC1ClE,KAAK2E,SAA4B,MAAjB7E,EAAK6E,SAAmB,IAAO7E,EAAK6E,SACpD3E,KAAKuD,QAA0B,MAAhBzD,EAAKyD,QAAkB,KAAOzD,EAAKyD,QAClDvD,KAAK4E,OAAwB,MAAf9E,EAAK8E,QAAwB9E,EAAK8E,OAChD5E,KAAK6E,MAAsB,MAAd/E,EAAK+E,OAAuB/E,EAAK+E,MAC9C7E,KAAK8E,iBAAkB,QAAehF,EAAKgF,iBAAmB,SAC9D9E,KAAK+E,kBAAmB,QAAejF,EAAKiF,kBAAoB,QAChE/E,KAAKgF,uBACD,QAAelF,EAAKkF,uBAAyB,SACjDhF,KAAKiF,2BACD,QAAenF,EAAKmF,2BAA6B,QACrDjF,KAAKkF,gBAAiB,QAAcpF,EAAKoF,gBACzClF,KAAKmF,iBAAkB,QAAcrF,EAAKqF,iBAC1CnF,KAAKoF,iBAAkB,QAAetF,EAAKsF,iBAC3CpF,KAAKqF,kBAAmB,QAAevF,EAAKuF,iBAChD,CACA,KAAArE,CAAMZ,GACFA,GAAa,QAAmBA,GAChC,MAAM8D,EAAOlE,KAAKkE,MAAQ,EAAIlE,KAAKkE,KAAQlE,KAAKkE,KAAO9D,EAAWyB,OAC5DyD,EAAMlF,EAAW8D,GACvB,GAAW,MAAPoB,EACA,MAAM,IAAI,KAAW,QAAQpB,gGAEtBqB,KAAKC,UAAUpF,OAE1BJ,KAAKyF,UACD,CAAC,IAAI,KAAU,CAAEC,KAAMtF,EAAWyB,OAAQ8D,KAAM,CAAE,CAACzB,GAAOoB,MAC9D,MAAM7C,EAAQ,CAAC6C,GACXtF,KAAK6E,QACL7E,KAAKsD,MAAQtD,KAAKiB,UAAU,QAASwB,EAAO,KAAMzC,KAAK+E,iBAAkB/E,KAAKqF,kBAAkB,EAAMrF,KAAKmF,kBAE3GnF,KAAK4E,SACL5E,KAAKqD,KAAOrD,KAAKiB,UAAU,OAAQwB,EAAO,KAAMzC,KAAK8E,gBAAiB9E,KAAKoF,iBAAiB,EAAMpF,KAAKkF,iBAE3GlF,KAAK4F,WAAa5F,KAAKiB,UAAU,cAAewB,EAAO,KAAMzC,KAAKgF,sBAAuB,MAAM,GAC/FhF,KAAK6F,eAAiB7F,KAAKiB,UAAU,kBAAmBwB,EAAO,KAAMzC,KAAKiF,0BAA2B,MAAM,GAC3GjF,KAAKmB,OAAQ,CACjB,CACA,IAAAe,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACR,MAAMsE,EAAiC,MAAtB3D,EAAiB,UAAoBA,EAAiB,SACjEE,GAAQ,QAAoBf,GAC5BlB,EAAaiC,EAAMI,MACnBiD,EAAOtF,EAAWyB,OAClB8B,EAAgB,KAAiB,EAAG+B,GACpCxB,EAAOlE,KAAKkE,MAAQ,EAAIlE,KAAKkE,KAAQlE,KAAKkE,KAAOwB,EACvD/B,EAAcoC,OAAO7B,EAAM,GAC3B,MAAM8B,EAAiB,KAA2B,EAAGN,GACrDM,EAAe9B,GAAQ9D,EAAW8D,GAClC,MAAM+B,EAAsBtC,EAAcE,QAC1CoC,EAAoBnC,OACpB,MAAMoC,GAAqB,EAAAtC,KAAA,YAAiBqC,EAAqB,KAAiB,EAAGP,GAAM7B,MAAM,EAAG6B,EAAO,IAa3G,IAAKI,EACD,MAbuB,MACvB,GAAII,EAAmB,CACnB,MAAMC,EAAsBnG,KAAK4F,WAAWtD,OAAOE,QAAQwD,GACrDI,EAA0BpG,KAAK6F,eAAevD,OAAOE,QAAQwD,GAC7DxB,EAAgBxE,KAAK4E,OAAS5E,KAAKqD,KAAKf,OAAOE,QAAQwD,GAAkB,KACzEzB,EAAiBvE,KAAK6E,MAAQ7E,KAAKsD,MAAMhB,OAAOE,QAAQwD,GAAkB,KAChF,OAAO/C,EAAmBZ,EAAO8D,EAAqBC,EAAyB5B,EAAeD,EAAgBvE,KAAKuD,QACvH,CAEI,OAAON,EAAmBZ,EAAOrC,KAAK4F,WAAWtD,OAAQtC,KAAK6F,eAAevD,OAAqB,MAAbtC,KAAKqD,KAAe,KAAOrD,KAAKqD,KAAKf,OAAsB,MAAdtC,KAAKsD,MAAgB,KAAOtD,KAAKsD,MAAMhB,OAAQtC,KAAKuD,QAC1L,EAGO8C,GAEX,MAAOC,EAAgBnD,EAAMC,GAAYM,EAAyBrB,EAAOrC,KAAKsD,MAAMhB,OAAQtC,KAAKqD,KAAKf,OAAQqB,EAAe3D,KAAKuD,SAC5HgD,EAAkB,CAACC,EAAUC,EAAO9B,KACtC,QAAS,KACL,MAAM+B,EAAQ,EAAI/B,EACZgC,EAAYH,EAASlE,OACrBsE,EAAcD,EAAUE,IAAIJ,GAAOK,IAAIJ,GAC7CF,EAASO,MAAMJ,EAAUE,IAAID,GAAa,GAC5C,EAaN,MALoC,MAChCL,EAAgBvG,KAAK4F,WAAYzC,EAAMnD,KAAK2E,UAC5C4B,EAAgBvG,KAAK6F,eAAgBzC,EAAUpD,KAAK2E,SAAS,EAEjEqC,GACOV,CAAc,GAE7B,CACA,SAAA5D,GACI,MAAMC,EAAS,CACXuB,KAAMlE,KAAKkE,KACXS,SAAU3E,KAAK2E,SACfpB,QAASvD,KAAKuD,QACdqB,OAAQ5E,KAAK4E,OACbC,MAAO7E,KAAK6E,MACZC,iBAAiB,QAAqB9E,KAAK8E,iBAC3CC,kBAAkB,QAAqB/E,KAAK+E,kBAC5CC,uBAAuB,QAAqBhF,KAAKgF,uBACjDC,2BAA2B,QAAqBjF,KAAKiF,2BACrDG,iBAAiB,QAAqBpF,KAAKoF,iBAC3CC,kBAAkB,QAAqBrF,KAAKqF,kBAC5CH,gBAAgB,QAAoBlF,KAAKkF,gBACzCC,iBAAiB,QAAoBnF,KAAKmF,kBAExCvC,EAAa7C,MAAM2C,YAEzB,OADAG,OAAOC,OAAOH,EAAQC,GACfD,CACX,EAGJ+B,EAAmB3B,UAAY,qBAC/B,EAAAC,cAAA,cAA4B0B,GACrB,MAAMuC,UAA2B,KACpC,WAAApH,CAAYC,GAMR,GALY,MAARA,IACAA,EAAO,CAAC,GAEZC,MAAMD,GACNE,KAAKkE,KAAoB,MAAbpE,EAAKoE,MAAgB,EAAIpE,EAAKoE,KACjB,kBAAdlE,KAAKkE,MACZ,IAAKgD,OAAOC,UAAUnH,KAAKkE,MACvB,MAAM,IAAIkD,MAAM,gDAAgDpH,KAAKkE,YAGxE,KAAImD,MAAMC,QAAQtH,KAAKkE,MASxB,MAAM,IAAIkD,MACN,wEAAgB7B,KAAKC,UAAUxF,KAAKkE,SATxC,IAAK,MAAMA,KAAQlE,KAAKkE,KACpB,IAAKgD,OAAOC,UAAUjD,GAClB,MAAM,IAAIkD,MACN,0DAAgB7B,KAAKC,UAAUxF,KAAKkE,QAOpD,CACAlE,KAAKuD,QAA0B,MAAhBzD,EAAKyD,QAAkB,KAAOzD,EAAKyD,QAClDvD,KAAK4E,OAAwB,MAAf9E,EAAK8E,QAAwB9E,EAAK8E,OAChD5E,KAAK6E,MAAsB,MAAd/E,EAAK+E,OAAuB/E,EAAK+E,MAC9C7E,KAAK8E,iBAAkB,QAAehF,EAAKgF,iBAAmB,SAC9D9E,KAAK+E,kBAAmB,QAAejF,EAAKiF,kBAAoB,QAChE/E,KAAKoF,iBAAkB,QAAetF,EAAKsF,iBAC3CpF,KAAKqF,kBAAmB,QAAevF,EAAKuF,kBAC5CrF,KAAKe,iBAAkB,CAC3B,CACA,KAAAC,CAAMZ,GAEF,MAAMmH,GADNnH,GAAa,QAAmBA,IACPyB,OAEA,kBAAd7B,KAAKkE,OACZlE,KAAKkE,KAAO,CAAClE,KAAKkE,OAEtB,IAAK,IAAIpC,EAAI,EAAGA,EAAI9B,KAAKkE,KAAKrC,SAAUC,EAChC9B,KAAKkE,KAAKpC,GAAK,IACf9B,KAAKkE,KAAKpC,IAAMyF,GAIxB,IAAK,MAAMrD,KAAQlE,KAAKkE,KACpB,GAAIA,EAAO,GAAKA,GAAQqD,EACpB,MAAM,IAAIH,MAAM,iBAAiBlD,KAGzC,GAAIlE,KAAKkE,KAAKrC,SAAW,KAAqB7B,KAAKkE,MAAMrC,OACrD,MAAM,IAAIuF,MAAM,4BAA4BpH,KAAKkE,QAErD,MAAMsD,EAAaxH,KAAKkE,KAAKuD,KAAIvD,GAAQ9D,EAAW8D,KAC9CwD,GAAY,EACd1H,KAAK6E,MACL7E,KAAKsD,MAAQtD,KAAKiB,UAAU,QAASuG,EAAY,UAAWxH,KAAK+E,iBAAkB/E,KAAKqF,iBAAkBqC,GAG1G1H,KAAKsD,MAAQ,KAEbtD,KAAK4E,OACL5E,KAAKqD,KAAOrD,KAAKiB,UAAU,OAAQuG,EAAY,UAAWxH,KAAK8E,gBAAiB9E,KAAKoF,gBAAiBsC,GAGtG1H,KAAKqD,KAAO,KAEhBrD,KAAKmB,OAAQ,CACjB,CACA,IAAAe,CAAKZ,EAAQa,GACT,MAAME,GAAQ,QAAoBf,GAC5BlB,EAAaiC,EAAMI,MACnB8E,EAAQnH,EAAWyB,OACzB,OAAO,IAAAL,OAAK,KAER,IAAI,KAAE2B,EAAI,SAAEC,IAAa,IAAAuE,SAAQtF,EAAOrC,KAAKkE,MAD5B,GAEjB,MAAM8B,EAAiB,KAA2B,EAAGuB,GACrD,IAAK,MAAMjC,KAAOtF,KAAKkE,KACnB8B,EAAeV,GAAOlF,EAAWkF,GAErC,MAAMsC,EAAaC,GACN,MAALA,GAAaA,EAAEpF,MAAMZ,SAAW0F,GAChCvH,KAAKkE,OAAS,CAACqD,EAAQ,GAChBM,EAAErF,QAAQwD,GAGV6B,EAGf,IAAIhD,EAAQ+C,EAAU5H,KAAKsD,MAAMhB,QAC7BwF,EAASF,EAAU5H,KAAKqD,KAAKf,QAOjC,MAAMyF,EAAgB,GAChBC,EAAoB,GAC1B,IAAK,IAAIlG,EAAI,EAAGA,EAAIyF,IAASzF,GACK,IAA1B9B,KAAKkE,KAAKC,QAAQrC,IAClBiG,EAAc3D,KAAKhE,EAAW0B,IAC9BkG,EAAkB5D,KAAK,KAGvB2D,EAAc3D,KAAK,GACnB4D,EAAkB5D,KAAKhE,EAAW0B,KAO1C,OAJAqB,EAAOA,EAAK8E,KAAKF,GACjB3E,EAAWA,EAAS6E,KAAKF,GACzBlD,EAAQA,EAAMoD,KAAKD,GACnBF,EAASA,EAAOG,KAAKD,GACd/E,EAAmBZ,EAAOc,EAAMC,EAAU0E,EAAQjD,EAAO7E,KAAKuD,QAAQ,GAErF,CACA,SAAAb,GACI,MAAMC,EAAS,CACXuB,KAAMlE,KAAKkE,KACXX,QAASvD,KAAKuD,QACdqB,OAAQ5E,KAAK4E,OACbC,MAAO7E,KAAK6E,MACZC,iBAAiB,QAAqB9E,KAAK8E,iBAC3CC,kBAAkB,QAAqB/E,KAAK+E,kBAC5CK,iBAAiB,QAAqBpF,KAAKoF,iBAC3CC,kBAAkB,QAAqBrF,KAAKqF,mBAE1CzC,EAAa7C,MAAM2C,YAEzB,OADAG,OAAOC,OAAOH,EAAQC,GACfD,CACX,EAGJsE,EAAmBlE,UAAY,qBAC/B,EAAAC,cAAA,cAA4BiE,E,uEC1XrB,MAAMiB,UAAsB,KAC/B,WAAArI,CAAYC,GACRC,MAAMD,GACNE,KAAKe,iBAAkB,EACvBf,KAAKmI,OAASrI,EAAKqI,MACvB,CACA,kBAAAxG,CAAmBvB,GACf,OAAOA,CACX,CACA,SAAAsC,GACI,MAAME,EAAa7C,MAAM2C,YACnBC,EAAS,CAAEwF,OAAQnI,KAAKmI,QAE9B,OADAtF,OAAOC,OAAOH,EAAQC,GACfD,CACX,CACA,IAAAT,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACRxB,KAAKoC,eAAed,EAAQa,GAC5B,MAAME,GAAQ,QAAoBf,GAGlC,OADe,MADA,IAAM,KAAee,EAAMI,MAAO,EAAGzC,KAAKmI,QAAQC,IAAI/F,KAC/B,IAAMA,GAAOF,EAAiB,WAAK,EAC5D,GAErB,EAGJ+F,EAAcnF,UAAY,gBAC1B,EAAAC,cAAA,cAA4BkF,GACrB,MAAMG,UAAwB,KACjC,WAAAxI,CAAYC,GACRC,MAAMD,GACNE,KAAKe,iBAAkB,EACvBf,KAAKsI,KAAOxI,EAAKwI,IACrB,CACA,kBAAA3G,CAAmBvB,GACf,OAAOA,CACX,CACA,SAAAsC,GACI,MAAME,EAAa7C,MAAM2C,YACnBC,EAAS,CAAE2F,KAAMtI,KAAKsI,MAE5B,OADAzF,OAAOC,OAAOH,EAAQC,GACfD,CACX,CACA,IAAAT,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACRxB,KAAKoC,eAAed,EAAQa,GAC5B,MAAME,GAAQ,QAAoBf,GAClC,GAAItB,KAAKsI,KAAO,GAAKtI,KAAKsI,KAAO,EAAG,CAChC,MAAMC,EAAS,KACX,MAAMJ,EAASK,KAAKC,KAAKzI,KAAKsI,MAAQ,EAAItI,KAAKsI,OAC/C,OAAOjG,EAAMyE,IAAI,KAAezE,EAAMI,MAAO,EAAG0F,GAAQ,EAE5D,OAAO,KAAeI,GAAQ,IAAMlG,GAAOF,EAAiB,WAAK,EACrE,CACA,OAAOE,CAAK,GAEpB,EAGJgG,EAAgBtF,UAAY,kBAC5B,EAAAC,cAAA,cAA4BqF,GA8BrB,MAAMK,UAAqB,KAC9B,WAAA7I,CAAYC,GACRC,MAAMD,GACNE,KAAKe,iBAAkB,EACvBf,KAAKsI,KAAOxI,EAAKwI,KACjBtI,KAAK2I,WAAa7I,EAAK6I,UAC3B,CACA,cAAAC,CAAetH,GACX,OAAOtB,KAAK2I,aAAc,QAAoBrH,GAAQmB,KAC1D,CACA,kBAAAd,CAAmBvB,GACf,OAAOA,CACX,CACA,SAAAsC,GACI,MAAME,EAAa7C,MAAM2C,YACnBC,EAAS,CAAE2F,KAAMtI,KAAKsI,MAE5B,OADAzF,OAAOC,OAAOH,EAAQC,GACfD,CACX,CACA,IAAAT,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACR,GAAIxB,KAAKsI,KAAO,GAAKtI,KAAKsI,KAAO,EAAG,CAChC,MAAMK,EAAa3I,KAAK4I,eAAetH,GACjCuH,EAAgB,KAClB,MAAMxG,GAAQ,QAAoBf,GAG5BwH,GAAS,mBACf,IAAIC,GAAU,IAAAC,eAAa,IAAAC,eAAcN,GAAa3I,KAAKsI,MAC3DS,EAAU,KAAOA,EAAS,WAE1B,MAAMG,IAAM,EAAIlJ,KAAKsI,OAAS,EAAItI,KAAKsI,KAAOQ,GAAU,MAAQ,GAC1DK,GAAKD,EAAIJ,EAAS9I,KAAKsI,KAG7B,OADUjG,EAAMyE,IAAIiC,GAASX,IAAIW,EAAQX,KAAK,GAAGtB,IAAIgC,IAC5ChC,IAAIoC,GAAGd,IAAIe,EAAE,EAE1B,OAAO,KAAeN,GAAe,KAAM,QAAoBvH,IAASa,EAAiB,WAAK,EAClG,CACA,OAAOb,CAAM,GAErB,EAGJoH,EAAa3F,UAAY,eACzB,EAAAC,cAAA,cAA4B0F,E,uICrHrB,SAASU,EAAOlG,EAAGmG,EAAUC,EAASC,EAASC,EAAYC,GAC9D,OAAO,IAAAjI,OAAK,KAmBR,IAAIkI,GAlBJ,QAAgBF,IAChB,QAAcC,IACd,QAAiBF,GACF,MAAXD,IACAA,EAAU,CAAC,EAAG,IAEH,MAAXC,IACAA,EAAU,SAEI,MAAdC,IACAA,GAAa,WAED,MAAZC,IACAA,EAAW,OAIfvG,GAAI,QAAsBA,EAAGsG,GAE7B,MAAMG,EAA6B,SAAZJ,EAAsB,OAAS,QAetD,OAZIG,EAFa,QAAbD,EAEI,UAAYvG,EAAGmG,EAAUC,EAASK,GAKlC,UAEJzG,EAAGmG,EAAUC,EAASK,GAEP,kBAAfH,IACAE,EAAI,YAAcA,EAAG,CAAC,EAAG,EAAG,EAAG,KAE5BA,CAAC,GAEhB,CAWO,SAASE,EAAO1G,EAAGmG,EAAUC,EAASC,EAASC,EAAYC,GAC9D,OAAO,IAAAjI,OAAK,KAkBR,IAAIkI,GAjBJ,QAAgBF,IAChB,QAAcC,IACd,QAAiBF,GACF,MAAXD,IACAA,EAAU,CAAC,EAAG,EAAG,IAEN,MAAXC,IACAA,EAAU,SAEI,MAAdC,IACAA,GAAa,WAED,MAAZC,IACAA,EAAW,OAGfvG,GAAI,QAAsBA,EAAGsG,GAE7B,MAAMG,EAA6B,SAAZJ,EAAsB,OAAS,QAUtD,OARIG,EADa,QAAbD,EACI,YAAcvG,EAAGmG,EAAUC,EAASK,GAGpC,YAAczG,EAAGmG,EAAUC,EAASK,GAEzB,kBAAfH,IACAE,EAAI,YAAcA,EAAG,CAAC,EAAG,EAAG,EAAG,EAAG,KAE/BA,CAAC,GAEhB,CAIO,MAAMG,UAAkB,KAO3B,WAAAhK,CAAYC,GAKR,GAJqB,MAAjBA,EAAKuJ,WACLvJ,EAAKuJ,SAAW,GAEpBtJ,MAAMD,GACuB,kBAAlBA,EAAKuJ,SACZrJ,KAAKqJ,SAAW,CAACvJ,EAAKuJ,cAErB,KAAIhC,MAAMC,QAAQxH,EAAKuJ,WACC,IAAzBvJ,EAAKuJ,SAASxH,QACc,kBAArB/B,EAAKuJ,SAAS,GAIrB,MAAM,IAAI,KAEN,qGAAG9D,KAAKC,UAAU1F,EAAKuJ,aAL3BrJ,KAAKqJ,SAAWvJ,EAAKuJ,QAMzB,CAEA,IADA,QAAsBrJ,KAAKqJ,SAAU,YACjB,MAAhBvJ,EAAKwJ,QACLtJ,KAAKsJ,QAAUtJ,KAAKqJ,cAGpB,GAA4B,kBAAjBvJ,EAAKwJ,QACZtJ,KAAKsJ,QAAU,CAACxJ,EAAKwJ,aAEpB,KAAIjC,MAAMC,QAAQxH,EAAKwJ,UACA,IAAxBxJ,EAAKwJ,QAAQzH,QACc,kBAApB/B,EAAKwJ,QAAQ,GAIpB,MAAM,IAAI,KAEN,oGAAG/D,KAAKC,UAAU1F,EAAKwJ,YAL3BtJ,KAAKsJ,QAAUxJ,EAAKwJ,OAMxB,EAEJ,QAAsBtJ,KAAKsJ,QAAS,WACpCtJ,KAAKuJ,QAA0B,MAAhBzJ,EAAKyJ,QAAkB,QAAUzJ,EAAKyJ,SACrD,QAAiBvJ,KAAKuJ,SACtBvJ,KAAKyF,UAAY,CAAC,IAAI,KAAU,CAAEC,KAAM,IAC5C,CACA,kBAAA/D,CAAmBvB,GACfA,GAAa,QAAmBA,GAChC,MAAMyB,GAAS,QAAiBzB,EAAW,GAAIJ,KAAKqJ,SAAS,GAAIrJ,KAAKuJ,QAASvJ,KAAKsJ,QAAQ,IAC5F,MAAO,CAAClJ,EAAW,GAAIyB,EAAQzB,EAAW,GAC9C,CACA,IAAA8B,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACRxB,KAAKoC,eAAed,EAAQa,GAE5Bb,EAAS,MAAa,QAAoBA,GAAS,GACnD,MAAMwI,EAAS9J,KAAK+J,iBAAgB,QAAoBzI,GAAS,CAACtB,KAAKqJ,SAAS,GAAI,GAAI,CAACrJ,KAAKsJ,QAAQ,GAAI,GAAItJ,KAAKuJ,QAAS,gBAE5H,OAAO,UAAYO,EAAQ,CAAC,GAAG,GAEvC,CACA,SAAApH,GACI,MAAMC,EAAS,CACX0G,SAAUrJ,KAAKqJ,SACfE,QAASvJ,KAAKuJ,QACdD,QAAStJ,KAAKsJ,SAEZ1G,EAAa7C,MAAM2C,YAEzB,OADAG,OAAOC,OAAOH,EAAQC,GACfD,CACX,EAEG,MAAMqH,UAAqBH,EAC9B,WAAAhK,CAAYC,GACRC,MAAMD,EACV,CACA,eAAAiK,CAAgBzI,EAAQ+H,EAAUC,EAASC,EAASC,GAGhD,OAFA,QAAgBA,IAChB,QAAiBD,GACVH,EAAO9H,EAAQ+H,EAAUC,EAASC,EAASC,EAAY,MAClE,EAGJQ,EAAajH,UAAY,eACzB,EAAAC,cAAA,cAA4BgH,GACrB,MAAMC,UAAyBJ,EAClC,WAAAhK,CAAYC,GACRC,MAAMD,EACV,CACA,eAAAiK,CAAgBzI,EAAQ+H,EAAUC,EAASC,EAASC,GAGhD,OAFA,QAAgBA,IAChB,QAAiBD,GACVH,EAAO9H,EAAQ+H,EAAUC,EAASC,EAASC,EAAY,MAClE,EAGJS,EAAiBlH,UAAY,mBAC7B,EAAAC,cAAA,cAA4BiH,GAIrB,MAAMC,UAAkB,KAC3B,WAAArK,CAAYC,GAQR,GAPqB,MAAjBA,EAAKuJ,WACLvJ,EAAKuJ,SAAW,CAAC,EAAG,IAExBtJ,MAAMD,GACNE,KAAKqJ,SAAWhC,MAAMC,QAAQxH,EAAKuJ,UAC/BvJ,EAAKuJ,SACL,CAACvJ,EAAKuJ,SAAUvJ,EAAKuJ,UACL,MAAhBvJ,EAAKwJ,QACLtJ,KAAKsJ,QAAUtJ,KAAKqJ,cAEnB,GAAIhC,MAAMC,QAAQxH,EAAKwJ,SAAU,CAClC,GAA4B,IAAxBxJ,EAAKwJ,QAAQzH,OACb,MAAM,IAAI,KAEN,wHAAG/B,EAAKwJ,QAAQzH,WAExB7B,KAAKsJ,QAAUxJ,EAAKwJ,OACxB,MAGItJ,KAAKsJ,QAAU,CAACxJ,EAAKwJ,QAASxJ,EAAKwJ,UAEvC,QAAsBtJ,KAAKqJ,SAAU,aACrC,QAAsBrJ,KAAKsJ,QAAS,WACpCtJ,KAAKuJ,QAA0B,MAAhBzJ,EAAKyJ,QAAkB,QAAUzJ,EAAKyJ,QACrDvJ,KAAKwJ,WACkB,MAAnB1J,EAAK0J,WAAqB,eAAiB1J,EAAK0J,YACpD,QAAgBxJ,KAAKwJ,aACrB,QAAiBxJ,KAAKuJ,SACtBvJ,KAAKyF,UAAY,CAAC,IAAI,KAAU,CAAEC,KAAM,IAC5C,CACA,kBAAA/D,CAAmBvB,GACfA,GAAa,QAAmBA,GAChC,IAAI+J,EAA2B,kBAApBnK,KAAKwJ,WAAiCpJ,EAAW,GAAKA,EAAW,GACxEgK,EAA2B,kBAApBpK,KAAKwJ,WAAiCpJ,EAAW,GAAKA,EAAW,GAK5E,OAJA+J,GACI,QAAiBA,EAAMnK,KAAKqJ,SAAS,GAAIrJ,KAAKuJ,QAASvJ,KAAKsJ,QAAQ,IACxEc,GACI,QAAiBA,EAAMpK,KAAKqJ,SAAS,GAAIrJ,KAAKuJ,QAASvJ,KAAKsJ,QAAQ,IAChD,kBAApBtJ,KAAKwJ,WACE,CAACpJ,EAAW,GAAIA,EAAW,GAAI+J,EAAMC,GAGrC,CAAChK,EAAW,GAAI+J,EAAMC,EAAMhK,EAAW,GAEtD,CACA,IAAA8B,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACRxB,KAAKoC,eAAed,EAAQa,GACrBnC,KAAK+J,iBAAgB,QAAoBzI,GAAStB,KAAKqJ,SAAUrJ,KAAKsJ,QAAStJ,KAAKuJ,QAASvJ,KAAKwJ,cAEjH,CACA,SAAA9G,GACI,MAAMC,EAAS,CACX0G,SAAUrJ,KAAKqJ,SACfE,QAASvJ,KAAKuJ,QACdD,QAAStJ,KAAKsJ,QACdE,WAAYxJ,KAAKwJ,YAEf5G,EAAa7C,MAAM2C,YAEzB,OADAG,OAAOC,OAAOH,EAAQC,GACfD,CACX,EAEG,MAAM0H,UAAqBH,EAC9B,WAAArK,CAAYC,GACRC,MAAMD,EACV,CACA,eAAAiK,CAAgBzI,EAAQ+H,EAAUC,EAASC,EAASC,GAGhD,OAFA,QAAgBA,IAChB,QAAiBD,GACVH,EAAO9H,EAAQ+H,EAAUC,EAASC,EAASC,EAAY,MAClE,EAGJa,EAAatH,UAAY,eACzB,EAAAC,cAAA,cAA4BqH,GACrB,MAAMC,UAAyBJ,EAClC,WAAArK,CAAYC,GACRC,MAAMD,EACV,CACA,eAAAiK,CAAgBzI,EAAQ+H,EAAUC,EAASC,EAASC,GAGhD,OAFA,QAAgBA,IAChB,QAAiBD,GACVH,EAAO9H,EAAQ+H,EAAUC,EAASC,EAASC,EAAY,MAClE,EAGJc,EAAiBvH,UAAY,mBAC7B,EAAAC,cAAA,cAA4BsH,GAIrB,MAAMC,UAAkB,KAC3B,WAAA1K,CAAYC,GAQR,GAPqB,MAAjBA,EAAKuJ,WACLvJ,EAAKuJ,SAAW,CAAC,EAAG,EAAG,IAE3BtJ,MAAMD,GACNE,KAAKqJ,SAAWhC,MAAMC,QAAQxH,EAAKuJ,UAC/BvJ,EAAKuJ,SACL,CAACvJ,EAAKuJ,SAAUvJ,EAAKuJ,SAAUvJ,EAAKuJ,UACpB,MAAhBvJ,EAAKwJ,QACLtJ,KAAKsJ,QAAUtJ,KAAKqJ,cAEnB,GAAIhC,MAAMC,QAAQxH,EAAKwJ,SAAU,CAClC,GAA4B,IAAxBxJ,EAAKwJ,QAAQzH,OACb,MAAM,IAAI,KAEN,wHAAG/B,EAAKwJ,QAAQzH,WAExB7B,KAAKsJ,QAAUxJ,EAAKwJ,OACxB,MAGItJ,KAAKsJ,QAAU,CAACxJ,EAAKwJ,QAASxJ,EAAKwJ,QAASxJ,EAAKwJ,UAErD,QAAsBtJ,KAAKqJ,SAAU,aACrC,QAAsBrJ,KAAKsJ,QAAS,WACpCtJ,KAAKuJ,QAA0B,MAAhBzJ,EAAKyJ,QAAkB,QAAUzJ,EAAKyJ,QACrDvJ,KAAKwJ,WACkB,MAAnB1J,EAAK0J,WAAqB,eAAiB1J,EAAK0J,YACpD,QAAgBxJ,KAAKwJ,aACrB,QAAiBxJ,KAAKuJ,SACtBvJ,KAAKyF,UAAY,CAAC,IAAI,KAAU,CAAEC,KAAM,IAC5C,CACA,kBAAA/D,CAAmBvB,GACfA,GAAa,QAAmBA,GAChC,IAAIoK,EAA6B,kBAApBxK,KAAKwJ,WAAiCpJ,EAAW,GAAKA,EAAW,GAC1E+J,EAA2B,kBAApBnK,KAAKwJ,WAAiCpJ,EAAW,GAAKA,EAAW,GACxEgK,EAA2B,kBAApBpK,KAAKwJ,WAAiCpJ,EAAW,GAAKA,EAAW,GAM5E,OALAoK,GAAS,QAAiBA,EAAQxK,KAAKqJ,SAAS,GAAIrJ,KAAKuJ,QAASvJ,KAAKsJ,QAAQ,IAC/Ea,GACI,QAAiBA,EAAMnK,KAAKqJ,SAAS,GAAIrJ,KAAKuJ,QAASvJ,KAAKsJ,QAAQ,IACxEc,GACI,QAAiBA,EAAMpK,KAAKqJ,SAAS,GAAIrJ,KAAKuJ,QAASvJ,KAAKsJ,QAAQ,IAChD,kBAApBtJ,KAAKwJ,WACE,CAACpJ,EAAW,GAAIA,EAAW,GAAIoK,EAAQL,EAAMC,GAG7C,CAAChK,EAAW,GAAIoK,EAAQL,EAAMC,EAAMhK,EAAW,GAE9D,CACA,IAAA8B,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACRxB,KAAKoC,eAAed,EAAQa,GACrBnC,KAAK+J,iBAAgB,QAAoBzI,GAAStB,KAAKqJ,SAAUrJ,KAAKsJ,QAAStJ,KAAKuJ,QAASvJ,KAAKwJ,cAEjH,CACA,SAAA9G,GACI,MAAMC,EAAS,CACX0G,SAAUrJ,KAAKqJ,SACfE,QAASvJ,KAAKuJ,QACdD,QAAStJ,KAAKsJ,QACdE,WAAYxJ,KAAKwJ,YAEf5G,EAAa7C,MAAM2C,YAEzB,OADAG,OAAOC,OAAOH,EAAQC,GACfD,CACX,EAEG,MAAM8H,UAAqBF,EAC9B,WAAA1K,CAAYC,GACRC,MAAMD,EACV,CACA,eAAAiK,CAAgBzI,EAAQ+H,EAAUC,EAASC,EAASC,GAGhD,OAFA,QAAgBA,IAChB,QAAiBD,GACVK,EAAOtI,EAAQ+H,EAAUC,EAASC,EAASC,EAAY,MAClE,EAGJiB,EAAa1H,UAAY,eACzB,EAAAC,cAAA,cAA4ByH,GACrB,MAAMC,UAAyBH,EAClC,WAAA1K,CAAYC,GACRC,MAAMD,EACV,CACA,eAAAiK,CAAgBzI,EAAQ+H,EAAUC,EAASC,EAASC,GAGhD,OAFA,QAAgBA,IAChB,QAAiBD,GACVK,EAAOtI,EAAQ+H,EAAUC,EAASC,EAASC,EAAY,MAClE,EAGJkB,EAAiB3H,UAAY,mBAC7B,EAAAC,cAAA,cAA4B0H,GAIrB,MAAMC,UAAwB,KACjC,WAAA9K,CAAYC,GACRC,MAAMD,GACNE,KAAKyF,UAAY,CAAC,IAAI,KAAU,CAAEC,KAAM,IAC5C,CACA,kBAAA/D,CAAmBvB,GACf,MAAO,CAACA,EAAW,GAAIA,EAAW,GACtC,CACA,IAAA8B,CAAKZ,EAAQa,GACT,MAAM,IAAI,IACd,EAEG,MAAMyI,UAA+BD,EACxC,WAAA9K,CAAYC,GACRC,MAAMD,GAAQ,CAAC,EACnB,CACA,IAAAoC,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACR,MAAMa,GAAQ,QAAoBf,GAClC,OAAO,OAASe,EAAO,EAAE,GAEjC,EAGJuI,EAAuB7H,UAAY,yBACnC,EAAAC,cAAA,cAA4B4H,GACrB,MAAMC,UAA2BF,EACpC,WAAA9K,CAAYC,GACRC,MAAMD,GAAQ,CAAC,EACnB,CACA,IAAAoC,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACR,MAAMa,GAAQ,QAAoBf,GAClC,OAAO,MAAQe,EAAO,EAAE,GAEhC,EAGJwI,EAAmB9H,UAAY,qBAC/B,EAAAC,cAAA,cAA4B6H,GAIrB,MAAMC,UAAwB,KACjC,WAAAjL,CAAYC,GACRC,MAAMD,GACNE,KAAKwJ,WACkB,MAAnB1J,EAAK0J,WAAqB,eAAiB1J,EAAK0J,YACpD,QAAgBxJ,KAAKwJ,YACrBxJ,KAAKyF,UAAY,CAAC,IAAI,KAAU,CAAEC,KAAM,IAC5C,CACA,kBAAA/D,CAAmBvB,GAEf,MAAwB,iBAApBJ,KAAKwJ,WACE,CAACpJ,EAAW,GAAIA,EAAW,IAG3B,CAACA,EAAW,GAAIA,EAAW,GAE1C,CACA,IAAA8B,CAAKZ,EAAQa,GACT,MAAM,IAAI,IACd,CACA,SAAAO,GACI,MAAMC,EAAS,CAAE6G,WAAYxJ,KAAKwJ,YAC5B5G,EAAa7C,MAAM2C,YAEzB,OADAG,OAAOC,OAAOH,EAAQC,GACfD,CACX,EAEG,MAAMoI,UAA+BD,EACxC,IAAA5I,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACR,MAAMa,GAAQ,QAAoBf,GAClC,MAAwB,iBAApBtB,KAAKwJ,WACE,OAASnH,EAAO,CAAC,EAAG,IAGpB,OAASA,EAAO,CAAC,EAAG,GAC/B,GAER,EAGJ0I,EAAuBhI,UAAY,yBACnC,EAAAC,cAAA,cAA4B+H,GACrB,MAAMC,UAA2BF,EACpC,IAAA5I,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACR,MAAMa,GAAQ,QAAoBf,GAClC,MAAwB,iBAApBtB,KAAKwJ,WACE,MAAQnH,EAAO,CAAC,EAAG,IAGnB,MAAQA,EAAO,CAAC,EAAG,GAC9B,GAER,EAGJ2I,EAAmBjI,UAAY,qBAC/B,EAAAC,cAAA,cAA4BgI,E,mHC1erB,MAAMC,UAAc,KACvB,WAAApL,CAAYC,GACRC,MAAMD,GAAQ,CAAC,GACfE,KAAKe,iBAAkB,CAC3B,CAKA,aAAAmK,CAAc5J,GACV,MAAM,IAAI,IACd,CAWA,+BAAA6J,CAAgCC,EAAQC,GACpC,GAAc,MAAVD,GAA4B,MAAVC,EAClB,OAAO,KAEN,GAAID,EAAOvJ,OAASwJ,EAAOxJ,OAC5B,OAAO7B,KAAKmL,gCAAgCE,EAAQD,GAEnD,GAAsB,IAAlBC,EAAOxJ,OACZ,OAAOuJ,EAEX,MAAME,EAAcF,EAAOvH,MAAM,EAAGuH,EAAOvJ,OAASwJ,EAAOxJ,QAC3D,IAAK,IAAIE,EAAI,EAAGA,EAAIsJ,EAAOxJ,SAAUE,EAAG,CACpC,MAAMD,EAAIsJ,EAAOA,EAAOvJ,OAASwJ,EAAOxJ,OAASE,GAC3CwJ,EAAIF,EAAOtJ,GACjB,GAAS,MAALD,GAAkB,MAALyJ,GAAazJ,EAAI,GAAKyJ,EAAI,EACvCD,EAAYlH,KAAK,WAEhB,GAAU,IAANtC,EACLwJ,EAAYlH,KAAKmH,QAEhB,GAAU,IAANA,EACLD,EAAYlH,KAAKtC,OAEhB,CACD,GAAIA,IAAMyJ,EACN,MAAM,IAAI,KAAW,wDACjBhG,KAAKC,UAAU4F,GAAU,IAAM7F,KAAKC,UAAU6F,IAEtDC,EAAYlH,KAAKtC,EACrB,CACJ,CACA,OAAOwJ,CACX,CACA,KAAAtK,CAAMZ,GAOF,GALIiH,MAAMC,QAAQlH,KAAgBiH,MAAMC,QAAQlH,EAAW,MAEvDA,EAAa,EAAC,QAAmBA,KAGjCA,EAAWyB,OAAS,EACpB,MAAM,IAAI,KACN,wEAAQzB,EAAWyB,oBAI3B,IAAI2J,EAAa,GACjB,IAAK,MAAM/I,KAASrC,EACH,MAATqC,GAA8B,OAAbA,EAAM,IACvB+I,EAAWpH,KAAK3B,EAAM,IAI9B,GADA+I,EAAa,KAAqBA,GAC9BA,EAAW3J,OAAS,EACpB,MAAM,IAAI,KACN,8EAA4B0D,KAAKC,UAAUpF,OAEnD,IAAIkL,EAA+B,MAAjBlL,EAAW,GAAa,KAAOA,EAAW,GAAGyD,MAAM,GACrE,IAAK,IAAI/B,EAAI,EAAGA,EAAI1B,EAAWyB,SAAUC,EAAG,CACxC,MAAMW,EAAyB,MAAjBrC,EAAW0B,GAAa,KAAO1B,EAAW0B,GAAG+B,MAAM,GACjEyH,EAActL,KAAKmL,gCAAgCG,EAAa7I,EACpE,CAGA,MAAMgJ,EAAWrL,EAAWqH,KAAIhF,GAASA,EAAMZ,UACb,IAA9BzB,EAAW+D,QAAQ,OACuB,IAA1C,KAAqBsH,GAAU5J,OAC/B7B,KAAK0L,iBAAkB,EAGvB1L,KAAK0L,iBAAkB,CAE/B,CACA,IAAAxJ,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KAER,GAAIxB,KAAK0L,gBAAiB,CACtB,MAAMC,EAAiB,GACjBC,EAAYtK,EAAOmG,KAAIpF,GAASA,EAAMoB,OAC5C,IAAiC,IAA7BmI,EAAUzH,QAAQ,MAAc,CAGhC,MAAM0H,EAAU,KAAcD,GAC9B,IAAK,IAAI1I,KAAK5B,EAAQ,CAClB,MAAMwK,EAAQ5I,EAAEO,KAChB,IAAK,IAAI1B,EAAI,EAAGA,EAAI8J,EAAUC,IAAS/J,EACnCmB,EAAI,KAAaA,EAAG,GAExByI,EAAevH,KAAKlB,EACxB,CACA,OAAOlD,KAAKkL,cAAcS,EAC9B,CACK,CAGD,IAAII,GAAa,EACjB,IAAK,MAAM7I,KAAK5B,EAAQ,CACpB,MAAMwK,EAAQ5I,EAAEO,KAChB,GAAa,MAATqI,EAAe,CACf,MAAME,EAAS9I,EAAET,MACXpC,EAAY2L,EAAO,GACnBC,EAAWD,EAAOnI,MAAM,GAAGtD,OAAO,CAACF,IACzC,IAAI6L,EAAchJ,EAAEV,QAAQ,CAACnC,GAAWE,OAAO,KAAoByL,EAAOnI,MAAM,MAChFqI,EAAc,YAAcA,EAAa,CAAC,EAAG,IAC7CA,EAAcA,EAAY1J,QAAQyJ,GAClCN,EAAevH,KAAK8H,GACpBH,GAAa,CACjB,MACK,GAAID,EAAQ,EAAG,CAChB,MAAMK,EAAO,KAAgB,EAAGL,GAAOvL,OAAO,CAAC,IAC/CoL,EAAevH,KAAK,YAAclB,EAAGiJ,IACrCJ,GAAa,CACjB,MAGIJ,EAAevH,KAAKlB,EAE5B,CACA,IAAIwG,EAAI1J,KAAKkL,cAAcS,GAC3B,MAAMS,EAAQ1C,EAAEjG,KAChB,GAAIsI,EAGA,GAAa,MAATK,EAAe,CACf,MAAMC,EAAS3C,EAAEjH,MAEXpC,EAAYgM,EADJA,EAAOxK,OACY,GAC3BoK,EAAW,CAAC5L,GAAWE,OAAO8L,EAAOxI,MAAM,EAAGwI,EAAOxK,OAAS,IACpE6H,EAAI,YAAcA,EAAElH,QAAQ,EAAE,EAAGnC,IAAa,CAAC,EAAG,IAC7CmC,QAAQyJ,EACjB,MACK,GAAIG,EAAQ,EAAG,CAChB,MAAMD,EAAO,CAACC,EAAQ,GAAG7L,OAAO,KAAgB,EAAG6L,EAAQ,IAC3D1C,EAAI,YAAcA,EAAGyC,EACzB,CAEJ,OAAOzC,CACX,CACJ,CAEI,OAAO1J,KAAKkL,cAAc5J,EAC9B,GAER,CACA,kBAAAK,CAAmBvB,GAEf,IAAIkL,EAEAA,EADiB,MAAjBlL,EAAW,GACG,KAGAA,EAAW,GAAGyD,MAAM,GAEtC,IAAK,IAAI/B,EAAI,EAAGA,EAAI1B,EAAWyB,SAAUC,EAAG,CACxC,MAAMW,EAAyB,MAAjBrC,EAAW0B,GAAa,KAAO1B,EAAW0B,GAAG+B,MAAM,GACjEyH,EAActL,KAAKmL,gCAAgCG,EAAa7I,EACpE,CACA,IAAI+I,EAAa,GACjB,IAAK,MAAM/I,KAASrC,EACH,MAATqC,GAA8B,OAAbA,EAAM,IACvB+I,EAAWpH,KAAK3B,EAAM,IAU9B,OAPA+I,EAAa,KAAqBA,GAE9BF,EADsB,IAAtBE,EAAW3J,OACG2J,EAAWjL,OAAO+K,GAGlB,CAAC,MAAM/K,OAAO+K,GAEzBA,CACX,CACA,WAAAjK,CAAYC,EAAQC,GAChB,OAAO,QAAS,KACZ,GAAY,MAARA,EACA,OAAO,KAEX,IAAK8F,MAAMC,QAAQ/F,GACf,MAAM,IAAI,KAAW,6BAEzB,IAAK8F,MAAMC,QAAQhG,GACf,MAAM,IAAI,KAAW,+BAEzB,GAAIC,EAAKM,SAAWP,EAAOO,OACvB,MAAM,IAAI,KAEN,mGAAIP,EAAOO,aAAaN,EAAKM,WAErC,GAAIN,EAAK+K,OAAMC,GAAU,MAALA,IAChB,OAAO,KAGX,IAAIzC,GADJvI,EAAOA,EAAKkG,KAAI8E,GAAU,MAALA,EAAYA,EAAI,aAAeA,EAAG,MACrC,GAClB,IAAK,IAAIzK,EAAI,EAAGA,EAAIP,EAAKM,OAAS,IAAKC,EACnCgI,EAAS,aAAeA,EAAQvI,EAAKO,IAEzC,OAAOgI,CAAM,GAErB,EAEG,MAAM0C,UAAYvB,EACrB,WAAApL,CAAYC,GACRC,MAAMD,EACV,CACA,aAAAoL,CAAc5J,GACV,OAAO,IAAAE,OAAK,KACR,IAAIsI,EAASxI,EAAO,GAAGmL,QACvB,IAAK,IAAI3K,EAAI,EAAGA,EAAIR,EAAOO,SAAUC,EACjCgI,EAAS,MAAQA,EAAQxI,EAAOQ,IAEpC,OAAOgI,CAAM,GAErB,EAGJ0C,EAAIzJ,UAAY,MAChB,EAAAC,cAAA,cAA4BwJ,GAwDrB,MAAME,UAAiBzB,EAC1B,WAAApL,CAAYC,GACRC,MAAMD,EACV,CACA,aAAAoL,CAAc5J,GACV,OAAO,IAAAE,OAAK,KACR,IAAIsI,EAASxI,EAAO,GAAGmL,QACvB,IAAK,IAAI3K,EAAI,EAAGA,EAAIR,EAAOO,SAAUC,EACjCgI,EAAS,MAAQA,EAAQxI,EAAOQ,IAEpC,OAAOgI,CAAM,GAErB,EAGJ4C,EAAS3J,UAAY,WACrB,EAAAC,cAAA,cAA4B0J,GAwDrB,MAAMC,UAAgB1B,EACzB,WAAApL,CAAYC,GACRC,MAAMD,EACV,CACA,aAAAoL,CAAc5J,GACV,OAAO,IAAAE,OAAK,KACR,IAAIsI,EAASxI,EAAO,GAAGmL,QACvB,IAAK,IAAI3K,EAAI,EAAGA,EAAIR,EAAOO,SAAUC,EACjCgI,EAAS,MAAQA,EAAQxI,EAAOQ,IAEpC,OAAO,MAAQ,EAAIR,EAAOO,OAAQiI,EAAO,GAEjD,EAGJ6C,EAAQ5J,UAAY,UACpB,EAAAC,cAAA,cAA4B2J,GAyDrB,MAAMC,UAAgB3B,EACzB,WAAApL,CAAYC,GACRC,MAAMD,EACV,CACA,aAAAoL,CAAc5J,GACV,OAAO,IAAAE,OAAK,KACR,IAAIsI,EAASxI,EAAO,GACpB,IAAK,IAAIQ,EAAI,EAAGA,EAAIR,EAAOO,SAAUC,EACjCgI,EAAS,UAAYA,EAAQxI,EAAOQ,IAExC,OAAOgI,CAAM,GAErB,EAGJ8C,EAAQ7J,UAAY,UACpB,EAAAC,cAAA,cAA4B4J,GAwDrB,MAAMC,UAAgB5B,EACzB,WAAApL,CAAYC,GACRC,MAAMD,EACV,CACA,aAAAoL,CAAc5J,GACV,OAAO,IAAAE,OAAK,KACR,IAAIsI,EAASxI,EAAO,GACpB,IAAK,IAAIQ,EAAI,EAAGA,EAAIR,EAAOO,SAAUC,EACjCgI,EAAS,UAAYA,EAAQxI,EAAOQ,IAExC,OAAOgI,CAAM,GAErB,EAGJ+C,EAAQ9J,UAAY,UACpB,EAAAC,cAAA,cAA4B6J,GAwDrB,MAAMC,UAAoB7B,EAC7B,WAAApL,CAAYC,GACRC,MAAMD,GACNE,KAAK+M,cAAgB,EACT,MAARjN,IACAA,EAAO,CAAC,GAEZE,KAAKkE,KAAoB,MAAbpE,EAAKoE,KAAelE,KAAK+M,aAAejN,EAAKoE,KACzDlE,KAAKe,iBAAkB,EACvBf,KAAK0L,iBAAkB,CAC3B,CACA,KAAA1K,CAAMZ,GAEF,IAAMiH,MAAMC,QAAQlH,KAAeiH,MAAMC,QAAQlH,EAAW,KAClC,IAAtBA,EAAWyB,OACX,MAAM,IAAI,KAAW,yEAIzB,IAAImL,GAAe,EACnB,IAAK,MAAMvK,KAASrC,EAChB,GAAa,MAATqC,EAAe,CACfuK,GAAe,EACf,KACJ,CAEJ,GAAIA,EACA,OAEJ,MAAMC,EAAW,GACjB,IAAK,IAAInL,EAAI,EAAGA,EAAI1B,EAAWyB,SAAUC,EAAG,CACxC,MAAMoL,EAAyB9M,EAAW0B,GAAG+B,QAC7CqJ,EAAuBnH,OAAO/F,KAAKkE,KAAM,GACzC,IAAIiJ,GAAS,EACb,IAAK,MAAM1K,KAASwK,EAChB,GAAI,EAAArJ,KAAA,YAAiBnB,EAAOyK,GAAyB,CACjDC,GAAS,EACT,KACJ,CAECA,GACDF,EAAS7I,KAAK8I,EAEtB,CACA,GAAID,EAASpL,OAAS,EAClB,MAAM,IAAI,KAAW,4GAEjB0D,KAAKC,UAAUpF,GAE3B,CACA,aAAA8K,CAAc5J,GACV,OAAO,IAAAE,OAAK,IACD,KAAcF,EAAQtB,KAAKkE,OAE1C,CACA,kBAAAvC,CAAmBvB,GACf,IAAMiH,MAAMC,QAAQlH,KAAeiH,MAAMC,QAAQlH,EAAW,IACxD,MAAM,IAAI,KAAW,+DAEzB,MAAMgN,EAAchN,EACdkL,EAAc8B,EAAY,GAAGvJ,QAC7BK,EAAOlE,KAAKkE,KAAO,EAAIoH,EAAYzJ,OAAS7B,KAAKkE,KAAOlE,KAAKkE,KAGnE,IAAK,MAAMzB,KAAS2K,EAAYvJ,MAAM,GAAI,CACtC,GAAyB,MAArByH,EAAYpH,IAAgC,MAAfzB,EAAMyB,GAAe,CAClDoH,EAAYpH,GAAQ,KACpB,KACJ,CACAoH,EAAYpH,IAASzB,EAAMyB,EAC/B,CACA,OAAOoH,CACX,CACA,WAAAjK,CAAYC,EAAQC,GAChB,GAAY,MAARA,EACA,OAAO,KAEX,IAAK8F,MAAMC,QAAQ/F,GACf,MAAM,IAAI,KAAW,6CAEzB,IAAK8F,MAAMC,QAAQhG,GACf,MAAM,IAAI,KAAW,+CAEzB,GAAIC,EAAKM,SAAWP,EAAOO,OACvB,MAAM,IAAI,KAAW,mCAAmCN,EAAKM,qCAC5BP,EAAOO,WAE5C,OAAO,QAAS,KACZ,IAAIwL,GAAe,EAOnB,GANA9L,EAAK+L,SAAQf,IACA,MAALA,IACAc,GAAe,EAEnB,IAEAA,EACA,OAAO,KAEX,MAAME,EAAc,GACpB,IAAK,IAAIzL,EAAI,EAAGA,EAAIR,EAAOO,SAAUC,EAClB,MAAXP,EAAKO,GAELyL,EAAYnJ,KAAK,WAAa9C,EAAOQ,IAAI0L,OAAO,SAE3CjM,EAAKO,GAAG2B,KAAOnC,EAAOQ,GAAG2B,KAE9B8J,EAAYnJ,KAAK,aAAe7C,EAAKO,IAAK,IAG1CyL,EAAYnJ,KAAK7C,EAAKO,IAG9B,MAAM2L,EAAoB,SAAWF,EAAavN,KAAKkE,MACvD,OAAO,MAAQuJ,GAAoB,GAAG,EAAM,GAEpD,CACA,SAAA/K,GACI,MAAMC,EAAS,CACX,KAAQ3C,KAAKkE,MAEXtB,EAAa7C,MAAM2C,YAEzB,OADAG,OAAOC,OAAOH,EAAQC,GACfD,CACX,EAuEJ,SAAS+K,EAAcxJ,EAAMoB,GACzB,KAAOpB,EAAO,GACVA,GAAQoB,EAEZ,OAAOpB,CACX,CAzEA4I,EAAY/J,UAAY,cACxB,EAAAC,cAAA,cAA4B8J,GAqJrB,MAAMa,UAAY1C,EACrB,WAAApL,CAAYC,GACRC,MAAMD,GACNE,KAAK2F,KAAO7F,EAAK6F,KACjB3F,KAAK4N,UAA8B,MAAlB9N,EAAK8N,WAA4B9N,EAAK8N,UACvD5N,KAAKe,iBAAkB,EACvBf,KAAK0L,iBAAkB,CAC3B,CACA,KAAA1K,CAAMZ,GACF,cAAgBiH,MAAMC,QAAQlH,IAAqC,IAAtBA,EAAWyB,QACpDwF,MAAMC,QAAQlH,EAAW,KAAOiH,MAAMC,QAAQlH,EAAW,KAAK,IAAM,kEACxE,MAAMgL,EAAShL,EAAW,GACpBiL,EAASjL,EAAW,GAC1B,GAAIgL,EAAOvJ,OAAS,GAAKwJ,EAAOxJ,OAAS,EACrC,MAAM,IAAI,KAAoB,gEAElC,MAAM8D,EAAO3F,KAAK6N,cAAczC,EAAQC,GACxC,GAAID,EAAOzF,EAAK,MAAQ0F,EAAO1F,EAAK,IAChC,MAAM,IAAI,KACN,8BAAGyF,EAAOzF,EAAK,WAAW0F,EAAO1F,EAAK,MAElD,CACA,aAAAuF,CAAc5J,GACV,GAAsB,IAAlBA,EAAOO,OACP,MAAM,IAAI,KACN,oEAAgBP,EAAOO,oBAE/B,IAEI8D,EAFAmI,EAAKxM,EAAO,GACZyM,EAAKzM,EAAO,GAehB,OANIqE,EAPC0B,MAAMC,QAAQtH,KAAK2F,MAOb3F,KAAK2F,KAAK8B,KAAI,CAACvD,EAAMpC,IAAM4L,EAAcxJ,EAAM5C,EAAOQ,GAAGW,MAAMZ,UAN/D,CACH6L,EAAc1N,KAAK2F,KAAMmI,EAAGrL,MAAMZ,QAClC6L,EAAc1N,KAAK2F,KAAMoI,EAAGtL,MAAMZ,SAMtC7B,KAAK4N,YACLE,GAAK,QAAYA,EAAInI,EAAK,IAC1BoI,GAAK,QAAYA,EAAIpI,EAAK,KArHtC,SAAkBzC,EAAGwG,EAAG/D,GACpB,GAAIzC,EAAET,MAAMZ,OAAS,GAAK6H,EAAEjH,MAAMZ,OAAS,EACvC,MAAM,IAAI,KAAoB,oEASlC,GAPA,cAAgBqB,EAAET,MAAMZ,QAAU,GAAG,IACjC,uDAAWqB,EAAET,MAAMZ,WACvB,cAAgBqB,EAAET,MAAMZ,QAAU,GAAG,IACjC,uDAAW6H,EAAEjH,MAAMZ,WACH,kBAAT8D,IACPA,EAAO,CAACA,EAAMA,IAEF,cAAZzC,EAAEhC,OAAqC,cAAZwI,EAAExI,MAC7B,MAAM,IAAI,KAAoB,+DAElC,MAAM4K,EAAQ5I,EAAET,MAAMZ,OAChBuK,EAAQ1C,EAAEjH,MAAMZ,OACV,MAAR8D,IAEAA,EAAO,CAACmG,EAAQ,EAAGM,EAAQ,IAE/B,MAAM4B,EAAYrI,EAClB,OAAO,QAAS,KACZ,IAAIsI,EAoBAzK,EAnBJ,GAAIsI,EAAQM,EAAO,CACf6B,EAAOnC,EAAQM,EACf,MAAM8B,EAAY,GAClB,IAAK,IAAIpM,EAAI,EAAGA,EAAImM,IAAQnM,EACxBoM,EAAU9J,KAAK,GAEnBsF,EAAIA,EAAElH,QAAQkH,EAAEjH,MAAMlC,OAAO2N,GACjC,MACK,GAAI9B,EAAQN,EAAO,CACpBmC,EAAO7B,EAAQN,EACf,MAAMoC,EAAY,GAClB,IAAK,IAAIpM,EAAI,EAAGA,EAAImM,IAAQnM,EACxBoM,EAAU9J,KAAK,GAEnBlB,EAAIA,EAAEV,QAAQU,EAAET,MAAMlC,OAAO2N,GACjC,MAEID,EAAO,EAGX,GAAuB,IAAnB/K,EAAET,MAAMZ,QAAmC,IAAnB6H,EAAEjH,MAAMZ,OAE5B2B,EADAwK,EAAU,KAAOA,EAAU,GACrB9K,EAAE4D,IAAI4C,GAAGyE,IAAIH,EAAU,IAGvB9K,EAAEkL,UAAU,CAAC,EAAG,IAAItH,IAAI4C,GAAGyE,IAAIH,EAAU,QAGlD,CACD,MAAMK,EAAOL,EAAU,KAAO9K,EAAET,MAAMZ,OAAS,EACzCyM,EAAON,EAAU,KAAOtE,EAAEjH,MAAMZ,OAAS,EAC/C2B,EAAMN,EAAEqL,OAAO7E,EAAG2E,EAAMC,EAC5B,CACA,GAAIL,EAAO,EAAG,CACV,IAAIO,EAEAA,EADA1C,EAAQM,EACFN,EAAQM,EAAQ,EAGhBN,EAAQ,EAElB,MAAM2C,EAAc,GACpB,IAAK,IAAI3M,EAAI0M,EAAK1M,EAAI0M,EAAMP,IAAQnM,EAChC2M,EAAYrK,KAAKtC,GAErB0B,EAAMA,EAAIkL,QAAQD,EACtB,CAIA,OAHyB,IAArBjL,EAAIf,MAAMZ,SACV2B,EAAMA,EAAImL,WAAW,IAElBnL,CAAG,GAElB,CA4CeoL,CAASd,EAAIC,EAAIpI,EAC5B,CACA,aAAAkI,CAAczC,EAAQC,GAClB,IAAI1F,EAYJ,OAFIA,EATC0B,MAAMC,QAAQtH,KAAK2F,MASb3F,KAAK2F,KAPL,CACH+H,EAAc1N,KAAK2F,KAAMyF,EAAOvJ,QAChC6L,EAAc1N,KAAK2F,KAAM0F,EAAOxJ,SAOjC8D,CACX,CACA,kBAAAhE,CAAmBvB,GACf,cAAgBiH,MAAMC,QAAQlH,IAAqC,IAAtBA,EAAWyB,QACpDwF,MAAMC,QAAQlH,EAAW,KAAOiH,MAAMC,QAAQlH,EAAW,KAAK,IAAM,kEACxE,MAAMgL,EAAShL,EAAW,GAAGyD,QACvBwH,EAASjL,EAAW,GAAGyD,QAC7B,GAAIuH,EAAOvJ,OAAS,GAAKwJ,EAAOxJ,OAAS,EACrC,MAAM,IAAI,KAAoB,gEAElC,MAAM8D,EAAO3F,KAAK6N,cAAczC,EAAQC,GACxCD,EAAOrF,OAAOJ,EAAK,GAAI,GACvB0F,EAAOtF,OAAOJ,EAAK,GAAI,GACvB0F,EAAOtF,OAAO,EAAG,GACjB,MAAMuF,EAAcF,EAAO7K,OAAO8K,GAIlC,OAH2B,IAAvBC,EAAYzJ,QACZyJ,EAAYlH,KAAK,GAEdkH,CACX,CACA,WAAAjK,CAAYC,EAAQC,GAChB,OAAO,IACX,CACA,SAAAmB,GACI,MAAMC,EAAS,CACX,KAAQ3C,KAAK2F,KACb,UAAa3F,KAAK4N,WAEhBhL,EAAa7C,MAAM2C,YAEzB,OADAG,OAAOC,OAAOH,EAAQC,GACfD,CACX,EAGJgL,EAAI5K,UAAY,MAChB,EAAAC,cAAA,cAA4B2K,E,iFC37BrB,SAASkB,EAAYlM,EAAQmM,EAAgB,CAAC,EAAGC,GAAiB,GACrE,OAAO,QAAuBpM,EAAQ,EAAAK,cAAA,iBAA+BgM,SAASC,aAAcH,EAAe,QAASC,EACxH,C,sTCyBO,SAASG,EAAgB5N,EAAQ6N,EAAcC,EAAWC,GAC7D,GAAIhI,MAAMC,QAAQhG,GAAS,CACvB,GAAoB,MAAhB6N,GAAqC,MAAbC,EACxB,MAAM,IAAI,KAAW,iFAGL,MAAhBC,IACAD,EAAY9N,EAAOuC,MAAMvC,EAAOO,OAASwN,EAAc/N,EAAOO,QAC9DP,EAASA,EAAOuC,MAAM,EAAGvC,EAAOO,OAASwN,IAEzC/N,EAAOO,OAAS,IAChBsN,EAAe7N,EAAOuC,MAAM,EAAGvC,EAAOO,SAE1CP,EAASA,EAAO,EACpB,CACA,SAASgO,EAAapM,GAClB,OAAS,MAALA,GAAamE,MAAMC,QAAQpE,GACpBA,EAGA,CAACA,EAEhB,CAGA,MAAO,CAAE5B,SAAQ6N,aAFjBA,EAAeG,EAAaH,GAEGC,UAD/BA,EAAYE,EAAaF,GAE7B,CA4CO,SAASG,EAAIC,EAAclO,EAAQmO,EAAeC,GAAc,EAAOnO,EAAM6N,EAAWO,GAAS,EAAOC,GAAqB,GAChI,OAAO,QAAS,KACZ,MAAMlK,EAAOpE,EAAOmB,MAAMZ,OAC1B,GAAI6D,EAAO,EACP,MAAM,IAAI,KAAW,uCAAuCA,OAIhE,MAAMC,EAAO,CAAC,EAAG,GAAGpF,OAAO,KAAiB,EAAGmF,IAE/C,GADApE,EAAS,YAAcA,EAAQqE,GACd,MAAbyJ,EACA,MAAM,IAAI,KAAoB,kFAQtB,MAAR7N,KACAA,EAAOA,EAAKiM,OAAO,QAAQA,OAAO,YACzB/J,OAASiC,EAAO,IACrBnE,EAAO,aAAeA,GAAO,IAEjCA,EAAO,YAAcA,EAAMoE,IAE3B+J,IACApO,EAAS,UAAYA,EAAQ,GACjB,MAARC,IACAA,EAAO,UAAYA,EAAM,KAYjC,MAAMsO,EAAiB,GACvB,IAAIC,EACAC,EAASN,EACb,MAAMO,EAAY1O,EAAOmB,MAAM,GACzBwN,EAAgB,UAAY3O,GAClC,IAAI4O,EA6BAC,EA5BQ,MAAR5O,IACA2O,EAAe,UAAY3O,IAE/B,IAAK,IAAI6O,EAAI,EAAGA,EAAIJ,IAAaI,EAAG,CAChC,MAAMC,EAAeJ,EAAcG,GAC7BE,EAAc,QAAS,IAAMd,EAAaa,EAAcN,KAC9D,GAAY,MAARxO,EACAuO,EAAaQ,EAAY,GACzBP,EAASO,EAAY,OAEpB,CACD,MAAMC,EAAgB,QAAS,KAC3B,MAAMC,EAAWN,EAAaE,GACxBK,EAAc,WAAaD,GAAU3J,IAAI2J,GAM/C,MAAO,CAAE1G,OAJMwG,EAAY,GAAGxJ,IAAI0J,GAAUpI,IAAI2H,EAAO,GAAGjJ,IAAI2J,IAI7CC,UAHCX,EAAOtI,KAAI,CAACkJ,EAAO7O,IAC1BwO,EAAY,GAAGxO,GAAGgF,IAAI0J,GAAUpI,IAAIuI,EAAM7J,IAAI2J,MAE7B,IAEhCX,EAAaS,EAAczG,OAC3BiG,EAASQ,EAAcG,SAC3B,CACId,GACAC,EAAezL,KAAK0L,EAE5B,CAEA,GAAIF,EAAoB,CACpB,MAAM1L,EAAO,EACbiM,EAAU,QAAUN,EAAgB3L,EACxC,CACA,MAAO,CAAC4L,EAAYK,EAASJ,EAAO,GAE5C,CACO,MAAMa,UAAY,KACrB,WAAA/Q,CAAYC,GAER,IAAI+Q,EACJ,GAFA9Q,MAAMD,GAEW,MAAbA,EAAK+Q,KACL,MAAM,IAAI,KAAW,wDAQzB,GALIA,EADKxJ,MAAMC,QAAQxH,EAAK+Q,MACjB,IAAIC,EAAgB,CAAEC,MAAOjR,EAAK+Q,OAGlC/Q,EAAK+Q,KAEM,MAAlBA,EAAKG,UACL,MAAM,IAAI,KAAW,qGAGzBhR,KAAK6Q,KAAOA,EACZ7Q,KAAKiR,gBACuB,MAAxBnR,EAAKmR,iBAAkCnR,EAAKmR,gBAChDjR,KAAKkR,YAAkC,MAApBpR,EAAKoR,aAA8BpR,EAAKoR,YAC3DlR,KAAK0P,YAAkC,MAApB5P,EAAK4P,aAA8B5P,EAAK4P,YAC3D1P,KAAKmR,UAA6B,MAAjBrR,EAAKsR,UAA2BtR,EAAKsR,SACtDpR,KAAK2P,OAAwB,MAAf7P,EAAK6P,QAAyB7P,EAAK6P,OACjD3P,KAAKe,iBAAkB,EACvBf,KAAKyF,UAAY,CAAC,IAAI,KAAU,CAAEC,KAAM,KACxC1F,KAAKqR,UAAY,KACjBrR,KAAKsR,QAAU,KAEftR,KAAKqP,aAAe,KAGpBrP,KAAKuR,WAAa,EACtB,CAGA,SAAAC,GACI,GAAoB,MAAhBxR,KAAKsR,QAAiB,CACtB,MAAMG,EAAYpK,MAAMC,QAAQtH,KAAK6Q,KAAKG,WAAahR,KAAK6Q,KAAKG,UAAUnP,OAAS,EACpF,OAAO,KAAiB,EAAG4P,GAAWhK,KAAIvE,GAAK,MACnD,CAEI,OAAOlD,KAAKsR,OAEpB,CAGA,SAAAI,CAAU3B,GACN/P,KAAKsR,QAAUvB,CACnB,CACA,kBAAApO,CAAmBvB,IACX,QAAgBA,KAChBA,EAAaA,EAAW,IAI5B,IAAI4Q,EAAYhR,KAAK6Q,KAAKG,UACrB3J,MAAMC,QAAQ0J,KACfA,EAAY,CAACA,IAEjB,MAAMvQ,EAAYuQ,EAAU,GAC5B,IAAI1F,EAOJ,GALIA,EADAtL,KAAKiR,gBACS,CAAC7Q,EAAW,GAAIA,EAAW,GAAIK,GAG/B,CAACL,EAAW,GAAIK,GAE9BT,KAAKkR,YAAa,CAClB,MAAMS,EAAa,GACnB,IAAK,MAAMrM,KAAO0L,EACdW,EAAWvN,KAAK,CAAChE,EAAW,GAAIkF,IAEpC,MAAO,CAACgG,GAAa/K,OAAOoR,EAChC,CAEI,OAAOrG,CAEf,CACA,WAAAjK,CAAYC,EAAQC,GAChB,OAAO,QAAS,KACR8F,MAAMC,QAAQ/F,KACdA,EAAOA,EAAK,IAEhB,MAAMqQ,EAAa5R,KAAKiR,gBAAkB1P,EAAO,KACjD,GAAIvB,KAAKkR,YAAa,CAClB,MAAMW,EAAY7R,KAAK+P,OAAOtI,KAAIqK,GAAK,OACvC,MAAO,CAACF,GAAYrR,OAAOsR,EAC/B,CAEI,OAAOD,CACX,GAER,CAOA,UAAI7B,GACA,GAAoB,MAAhB/P,KAAKsR,QAAiB,CACtB,MAAMG,EAAYpK,MAAMC,QAAQtH,KAAK6Q,KAAKG,WAAahR,KAAK6Q,KAAKG,UAAUnP,OAAS,EAC9EiI,EAAS,GACf,IAAK,IAAIhI,EAAI,EAAGA,EAAI2P,IAAa3P,EAC7BgI,EAAO1F,KAAK,MAEhB,OAAO0F,CACX,CAEI,OAAO9J,KAAKsR,OAEpB,CACA,UAAIvB,CAAO+B,GACP9R,KAAKsR,QAAUQ,CACnB,CACA,KAAA9Q,CAAMZ,GAIF,GAAyB,MAArBJ,KAAKqP,aACL,MAAM,IAAI,KAAoB,qDAE9B,QAAgBjP,KAChBA,EAAaA,EAAW,IAG5B,MAAMC,EAAYL,KAAKoR,SAAWhR,EAAW,GAAK,KAC5CI,EAAWJ,EAAWyD,MAAM,GAClC7D,KAAKyF,UAAU,GAAK,IAAI,KAAU,CAAEhD,MAAO,CAACpC,EAAW,QAASG,KAGhE,MAAMuR,EAAiB,CAAC3R,EAAW,IAAIG,OAAOH,EAAWyD,MAAM,IAQ/D,IAAImN,EAOJ,GAVIhR,KAAK6Q,KAAK7P,MAAM+Q,GAKhBf,EADA3J,MAAMC,QAAQtH,KAAK6Q,KAAKG,WACZhR,KAAK6Q,KAAKG,UAGV,CAAChR,KAAK6Q,KAAKG,WAEL,MAAlBhR,KAAKqR,WACL,IAAK,EAAAzN,KAAA,YAAiB5D,KAAKqR,UAAU5J,KAAIuK,GAAQA,EAAKvP,MAAMuP,EAAKvP,MAAMZ,OAAS,KAAKmP,GACjF,MAAM,IAAI,KACN,6FAAsChR,KAAKqR,wCACdrR,KAAK6Q,KAAKG,kBAI/ChR,KAAKqR,UACDL,EAAUvJ,KAAInC,GAAO,IAAI,KAAU,CAAE7C,MAAO,CAAC,KAAM6C,OAEvDtF,KAAKoR,UACLpR,KAAKiS,aAEb,CAkBA,WAAAA,CAAYlC,EAAQjK,GAAW,IAC3B,IAAAtE,OAAK,KACD,IAAKxB,KAAKoR,SACN,MAAM,IAAI,KAAe,mEAE7B,MAAM/Q,EAAYL,KAAKyF,UAAU,GAAGhD,MAAM,GAC1C,GAAiB,MAAbpC,EACA,MAAM,IAAI,KAAW,yUAQzB,GAAoB,MAAhBL,KAAKsR,QACDjK,MAAMC,QAAQtH,KAAK6Q,KAAKG,WACxBhR,KAAKsR,QACDtR,KAAK6Q,KAAKG,UAAUvJ,KAAInC,GAAO,QAAU,CAACjF,EAAWiF,MAGzDtF,KAAKsR,QAAU,CAAC,QAAU,CAACjR,EAAWL,KAAK6Q,KAAKG,kBAGnD,GAAc,MAAVjB,EAEL,UAAY/P,KAAKsR,SAEM,MAAnBtR,KAAKuR,aACL,UAAYvR,KAAKuR,YACjBvR,KAAKuR,WAAa,IAElBlK,MAAMC,QAAQtH,KAAK6Q,KAAKG,WACxBhR,KAAKsR,QACDtR,KAAK6Q,KAAKG,UAAUvJ,KAAInC,GAAO,QAAU,CAACjF,EAAWiF,MAGzDtF,KAAKsR,QAAQ,GAAK,QAAU,CAACjR,EAAWL,KAAK6Q,KAAKG,gBAGrD,CAID,GAHK3J,MAAMC,QAAQyI,KACfA,EAAS,CAACA,IAEVA,EAAOlO,SAAW7B,KAAKsR,QAAQzP,OAC/B,MAAM,IAAI,KAAW,SAAS7B,KAAKkS,gBAAgBlS,KAAKsR,QAAQzP,oCACzCkO,EAAOlO,0CACbkO,MAEJ,IAAbjK,EAKA9F,KAAKuR,WAAWnN,KAAKpE,KAAKsR,QAAQzN,SAGlC,UAAY7D,KAAKsR,SAErB,IAAK,IAAIa,EAAQ,EAAGA,EAAQnS,KAAKsR,QAAQzP,SAAUsQ,EAAO,CACtD,MAAM1L,EAAQsJ,EAAOoC,GACf7M,EAAM+B,MAAMC,QAAQtH,KAAK6Q,KAAKG,WAChChR,KAAK6Q,KAAKG,UAAUmB,GACpBnS,KAAK6Q,KAAKG,UACRoB,EAAgB,CAAC/R,EAAWiF,GAClC,IAAK,EAAA1B,KAAA,YAAiB6C,EAAMhE,MAAO2P,GAC/B,MAAM,IAAI,KAAW,SAASD,gCAAoCnS,KAAKkS,wBACjDE,qBAAiC3L,EAAMhE,SAEjEzC,KAAKsR,QAAQa,GAAS1L,CAC1B,CACJ,CACAzG,KAAKsR,QAAUtR,KAAKsR,QAAQ7J,KAAIkJ,GAAS,OAASA,EAAMlE,UAAS,GAEzE,CACA,KAAA4F,CAAM/Q,EAAQa,GAEV,IAAIgN,EAAyB,MAAVhN,EAAiB,KAAOA,EAAqB,aAC5DiN,EAAsB,MAAVjN,EAAiB,KAAOA,EAAkB,UAC5C,MAAVA,IACAA,EAAS,CAAC,GAEd,MAAMmQ,EAAepD,EAAgB5N,EAAQ6N,EAAcC,EAAWpP,KAAKqP,cAC3E/N,EAASgR,EAAahR,OACtB6N,EAAemD,EAAanD,aAC5BC,EAAYkD,EAAalD,UAIzB,IAAImD,EAAmB,GACnBC,EAAkB,GACtB,GAAoB,MAAhBrD,EAAsB,CACtBhN,EAAqB,aAAIgN,EACzBoD,EAAmBA,EAAiBhS,OAAO4O,GAC3CnP,KAAKqR,UAAY,GACjB,IAAK,MAAMV,KAASxB,EAChBnP,KAAKqR,UAAUjN,KAAK,IAAI,KAAU,CAAE3B,MAAOkO,EAAMlO,SAKrD+P,EAAkBA,EAAgBjS,OAAOP,KAAKqR,UAClD,CACiB,MAAbjC,IACAjN,EAAkB,UAAIiN,EACtBmD,EAAmBA,EAAiBhS,OAAO6O,GAE3CpP,KAAKqP,aAAeD,EAAUvN,QAGlC,GADiB0Q,EAAiB,aAAc,KAClC,CAEV,MAAME,EAAY,CAACnR,GAAQf,OAAOgS,GAC5BG,EAAgB1S,KAAKyF,UAAUlF,OAAOiS,GAEtCG,EAAoB3S,KAAKyF,UAC/BzF,KAAKyF,UAAYiN,EACjB,MAAM5I,EAAS/J,MAAMsS,MAAMI,EAAWtQ,GAEtC,OADAnC,KAAKyF,UAAYkN,EACV7I,CACX,CAEI,OAAO/J,MAAMsS,MAAM/Q,EAAQa,EAEnC,CAEA,IAAAD,CAAKZ,EAAQa,GAIT,OAAO,IAAAX,OAAK,KACR,MAAMD,EAAiB,MAAVY,EAAiB,KAAOA,EAAa,KAC5C2D,EAAqB,MAAV3D,EAAiB,KAAOA,EAAiB,SAC1D,IAAIgN,EAAyB,MAAVhN,EAAiB,KAAOA,EAAqB,aAChEb,GAAS,QAAoBA,GACT,MAAhB6N,IAEIA,EADAnP,KAAKoR,SACUpR,KAAKsR,QAGLtR,KAAK4S,gBAAgBtR,IAG5C,MAAMmQ,EAAYpK,MAAMC,QAAQtH,KAAK6Q,KAAKG,WAAahR,KAAK6Q,KAAKG,UAAUnP,OAAS,EACpF,GAAIsN,EAAatN,SAAW4P,EACxB,MAAM,IAAI,KAAW,iBAAiBA,6BAC/BtC,EAAatN,4BAEpB7B,KAAK2P,OAGT,MAAMkD,EAAiB,CAAE/M,YAUnBgN,EAAavD,GARN,CAACjO,EAAQyO,KAGlB,MAAMI,EAAUnQ,KAAK6Q,KAAK3O,KAAK,CAACZ,GAAQf,OAAOwP,GAAS8C,GAExD,MAAO,CAAC1C,EAAQ,GAAIA,EAAQtM,MAAM,GAAG,GAGZvC,EAAQ6N,EAAcnP,KAAK0P,YAAanO,EAAM,KAAMvB,KAAK2P,OAAQ3P,KAAKiR,iBAC7FnB,EAAagD,EAAW,GACxB3C,EAAU2C,EAAW,GACrB/C,EAAS+C,EAAW,GACtB9S,KAAKoR,UACLpR,KAAKiS,YAAYlC,EAAQjK,GAE7B,MAAMgE,EAAS9J,KAAKiR,gBAAkBd,EAAUL,EAEhD,OAAI9P,KAAKkR,YACE,CAACpH,GAAQvJ,OAAOwP,GAGhBjG,CACX,GAER,CACA,eAAA8I,CAAgBtR,GACZ,OAAO,IAAAE,OAAK,KAGR,IAAI2N,EAAe,QAAU7N,EAAOmB,OAIpC,OAFA0M,EAAe,MAAQA,EAAc,CAAC,EAAG,IACzCA,EAAe,KAAaA,GACxB9H,MAAMC,QAAQtH,KAAK6Q,KAAKG,WACjBhR,KAAK6Q,KAAKG,UAAUvJ,KAAInC,GAAOA,EAAM,EAAI,KAAO6J,EAAc,CAAC,EAAG7J,IAAQ6J,IAG1EnP,KAAK6Q,KAAKG,UAAY,EACzB,CAAC,KAAO7B,EAAc,CAAC,EAAGnP,KAAK6Q,KAAKG,aACpC,CAAC7B,EACT,GAER,CACA,oBAAI4D,GACA,OAAK/S,KAAK0H,UAIH1H,KAAK6Q,KAAKkC,iBAHN,EAIf,CACA,uBAAIC,GAEA,OAAKhT,KAAK0H,UAGH1H,KAAK6Q,KAAKmC,oBAFNhT,KAAK6Q,KAAKoC,OAGzB,CACA,4BAAAC,CAA6BzM,GACzB1G,MAAMmT,6BAA6BzM,GAClB,MAAbzG,KAAK6Q,MACL7Q,KAAK6Q,KAAKqC,6BAA6BzM,EAE/C,CACA,SAAA/D,GACI,MAAME,EAAa7C,MAAM2C,YACnBC,EAAS,CACXsO,gBAAiBjR,KAAKiR,gBACtBC,YAAalR,KAAKkR,YAClBxB,YAAa1P,KAAK0P,YAClB0B,SAAUpR,KAAKoR,SACfzB,OAAQ3P,KAAK2P,QAEQ,MAArB3P,KAAKqP,eACL1M,EAAqB,aAAI3C,KAAKqP,cAElC,MAAM8D,EAAanT,KAAK6Q,KAAKnO,YAQ7B,OAPI1C,KAAKoT,iBAAmBxC,EAAI7N,YAC5BJ,EAAa,KAAI,CACb,UAAa3C,KAAK6Q,KAAKuC,eACvB,OAAUD,IAIXtQ,OAAOC,OAAO,CAAC,EAAGqQ,EAAYvQ,EAAYD,EACrD,CAEA,iBAAO0Q,CAAWC,EAAK3Q,EAAQmM,EAAgB,CAAC,GAC5C,MAAMqE,EAAaxQ,EAAa,KAC1BkO,GAAO,OAAYsC,EAAYrE,GACrC,OAAO,IAAIwE,EAAIzQ,OAAOC,OAAOH,EAAQ,CAAEkO,SAC3C,EAGJD,EAAI7N,UAAY,MAChB,EAAAC,cAAA,cAA4B4N,GASrB,MAAM2C,UAAgB,MAEtB,MAAMC,UAAsBD,EAC/B,WAAA1T,CAAYC,GACRC,MAAMD,GACNE,KAAKyT,mBAAqB,OAC1BzT,KAAK0T,2BAA6B,eAClC1T,KAAK2T,8BAAgC,aACrC3T,KAAK4T,yBAA2B,QAChC5T,KAAK6T,MAAQ/T,EAAK+T,OAClB,QAAsB7T,KAAK6T,MAAO,SAClC7T,KAAK8T,YAAa,QAAiC,MAAnBhU,EAAKgU,WAAqB9T,KAAKyT,mBAAqB3T,EAAKgU,YACzF9T,KAAK+T,QAA0B,MAAhBjU,EAAKiU,SAAyBjU,EAAKiU,QAClD/T,KAAKgU,mBAAoB,QAAelU,EAAKkU,mBAAqBhU,KAAK0T,4BACvE1T,KAAKiU,sBAAuB,QAAenU,EAAKmU,sBAAwBjU,KAAK2T,+BAC7E3T,KAAKkU,iBACD,QAAepU,EAAKoU,iBAAmBlU,KAAK4T,0BAChD5T,KAAKmU,mBAAoB,QAAerU,EAAKqU,mBAC7CnU,KAAKoU,sBAAuB,QAAetU,EAAKsU,sBAChDpU,KAAKqU,iBAAkB,QAAevU,EAAKuU,iBAC3CrU,KAAKsU,kBAAmB,QAAcxU,EAAKwU,kBAC3CtU,KAAKuU,qBAAsB,QAAczU,EAAKyU,qBAC9CvU,KAAKwU,gBAAiB,QAAc1U,EAAK0U,gBACzCxU,KAAKyU,QAAU,KAAe,CAAC,EAAG,KAAe,CAAC,EAAmB,MAAhB3U,EAAK2U,QAAkB,EAAI3U,EAAK2U,YACrFzU,KAAK0U,iBAAmB,KAAe,CACnC,EACA,KAAe,CAAC,EAA4B,MAAzB5U,EAAK4U,iBAA2B,EAAI5U,EAAK4U,qBAEhE1U,KAAKgR,UAAYhR,KAAK6T,MACtB7T,KAAK2U,YAAc,KACnB3U,KAAK4U,qBAAuB,IAChC,CACA,KAAA5T,CAAMZ,GACFA,GAAa,QAAmBA,GAEhCJ,KAAK6U,OAAS7U,KAAKiB,UAAU,SAAU,CAACb,EAAWA,EAAWyB,OAAS,GAAI7B,KAAK6T,OAAQ,KAAM7T,KAAKgU,kBAAmBhU,KAAKmU,mBAAmB,EAAMnU,KAAKsU,kBACzJtU,KAAK8U,gBAAkB9U,KAAKiB,UAAU,mBAAoB,CAACjB,KAAK6T,MAAO7T,KAAK6T,OAAQ,KAAM7T,KAAKiU,qBAAsBjU,KAAKoU,sBAAsB,EAAMpU,KAAKuU,qBACvJvU,KAAK+T,QACL/T,KAAK+U,KAAO/U,KAAKiB,UAAU,OAAQ,CAACjB,KAAK6T,OAAQ,KAAM7T,KAAKkU,gBAAiBlU,KAAKqU,iBAAiB,EAAMrU,KAAKwU,gBAG9GxU,KAAK+U,KAAO,KAEhB/U,KAAKmB,OAAQ,CACjB,CAOA,IAAAe,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KAER,GAAsB,IAAlBF,EAAOO,OACP,MAAM,IAAI,KAAW,8CAA8CP,EAAOO,WAE9E,IAAImT,EAAa1T,EAAO,GACxBA,EAASA,EAAO,GAChB,MAAMwE,EAAiC,MAAtB3D,EAAiB,UAAoBA,EAAiB,SAgBvE,IAAI8S,EAfA,EAAIjV,KAAKyU,SAAWzU,KAAKyU,QAAU,GAAyB,MAApBzU,KAAK2U,cAC7C3U,KAAK2U,YAAcO,EAAoB,CACnCC,KAAM,IAAM,WAAa7T,GACzBgH,KAAMtI,KAAKyU,QACX3O,cAGJ,EAAI9F,KAAK0U,kBAAoB1U,KAAK0U,iBAAmB,GACxB,MAA7B1U,KAAK4U,uBACL5U,KAAK4U,qBAAuBM,EAAoB,CAC5CC,KAAM,IAAM,WAAaH,GACzB1M,KAAMtI,KAAK0U,iBACX5O,cAIR,MAAMsP,EAASpV,KAAK2U,YACdU,EAAYrV,KAAK4U,qBAEnBK,EADU,MAAVG,EACI,KAAM,MAAQ9T,EAAQ8T,GAASpV,KAAK6U,OAAOvS,QAG3C,KAAMhB,EAAQtB,KAAK6U,OAAOvS,QAEjB,MAAbtC,KAAK+U,OACLE,EAAI,KAAUA,EAAGjV,KAAK+U,KAAKzS,SAEd,MAAb+S,IACAL,EAAa,MAAQA,EAAYK,IAErC,IAAIvL,EAAS,MAAQmL,EAAG,KAAMD,EAAYhV,KAAK8U,gBAAgBxS,SAK/D,OAJuB,MAAnBtC,KAAK8T,aACLhK,EAAS9J,KAAK8T,WAAWzB,MAAMvI,IAG5B,CAACA,EAAQA,EAAO,GAE/B,CACA,SAAApH,GACI,MAAME,EAAa7C,MAAM2C,YACnBC,EAAS,CACXkR,MAAO7T,KAAK6T,MACZC,YAAY,QAAoB9T,KAAK8T,YACrCC,QAAS/T,KAAK+T,QACdC,mBAAmB,QAAqBhU,KAAKgU,mBAC7CC,sBAAsB,QAAqBjU,KAAKiU,sBAChDC,iBAAiB,QAAqBlU,KAAKkU,iBAC3CC,mBAAmB,QAAqBnU,KAAKmU,mBAC7CC,sBAAsB,QAAqBpU,KAAKoU,sBAChDC,iBAAiB,QAAqBrU,KAAKqU,iBAC3CzT,qBAAqB,QAAqBZ,KAAKY,qBAC/C0T,kBAAkB,QAAoBtU,KAAKsU,kBAC3CC,qBAAqB,QAAoBvU,KAAKuU,qBAC9CC,gBAAgB,QAAoBxU,KAAKwU,gBACzCC,QAASzU,KAAKyU,QACdC,iBAAkB1U,KAAK0U,kBAE3B,OAAO7R,OAAOC,OAAO,CAAC,EAAGF,EAAYD,EACzC,EAGJ6Q,EAAczQ,UAAY,gBAC1B,EAAAC,cAAA,cAA4BwQ,GACrB,MAAM8B,UAAkB1E,EAC3B,WAAA/Q,CAAYC,GACRA,EAAK+Q,KAAO,IAAI2C,EAAc1T,GAC9BC,MAAMD,EAEV,CACA,IAAAoC,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACqB,MAAzBxB,KAAK6Q,KAAK8D,cACV,UAAY3U,KAAK6Q,KAAK8D,aACtB3U,KAAK6Q,KAAK8D,YAAc,MAEU,MAAlC3U,KAAK6Q,KAAK+D,uBACV,UAAY5U,KAAK6Q,KAAK+D,sBACtB5U,KAAK6Q,KAAK+D,qBAAuB,MAErC,MAAMrT,EAAiB,MAAVY,EAAiB,KAAOA,EAAa,KAC5C2D,EAAqB,MAAV3D,EAAiB,KAAOA,EAAiB,SACpDgN,EAAyB,MAAVhN,EAAiB,KAAOA,EAAqB,aAClE,OAAOpC,MAAMmC,KAAKZ,EAAQ,CAAEC,OAAMuE,WAAUqJ,gBAAe,GAEnE,CAEA,iBAAOkE,CAAWC,EAAK3Q,GACnB,OAAO,IAAI2Q,EAAI3Q,EACnB,EAGJ2S,EAAUvS,UAAY,YACtB,EAAAC,cAAA,cAA4BsS,GACrB,MAAMC,UAAgBhC,EACzB,WAAA1T,CAAYC,GAOR,GANAC,MAAMD,GACNE,KAAKyT,mBAAqB,OAC1BzT,KAAKwV,6BAA+B,cACpCxV,KAAK0T,2BAA6B,eAClC1T,KAAK2T,8BAAgC,aACrC3T,KAAK4T,yBAA2B,QAC5B9T,EAAK2V,WACL,MAAM,IAAI,KAAW,+DAEzBzV,KAAK6T,MAAQ/T,EAAK+T,OAClB,QAAsB7T,KAAK6T,MAAO,SAClC7T,KAAK8T,YAAa,aAAkC4B,IAApB5V,EAAKgU,WAA2B9T,KAAKyT,mBACjE3T,EAAKgU,YACT9T,KAAK2V,qBAAsB,aAA2CD,IAA7B5V,EAAK6V,oBAC1C3V,KAAKwV,6BACL1V,EAAK6V,qBACT3V,KAAK+T,QAA0B,MAAhBjU,EAAKiU,SAAyBjU,EAAKiU,QAClD/T,KAAKgU,mBAAoB,QAAelU,EAAKkU,mBAAqBhU,KAAK0T,4BACvE1T,KAAKiU,sBAAuB,QAAenU,EAAKmU,sBAAwBjU,KAAK2T,+BAC7E3T,KAAKkU,iBACD,QAAepU,EAAKoU,iBAAmBlU,KAAK4T,0BAChD5T,KAAKmU,mBAAoB,QAAerU,EAAKqU,mBAC7CnU,KAAKoU,sBAAuB,QAAetU,EAAKsU,sBAChDpU,KAAKqU,iBAAkB,QAAevU,EAAKuU,iBAC3CrU,KAAKsU,kBAAmB,QAAcxU,EAAKwU,kBAC3CtU,KAAKuU,qBAAsB,QAAczU,EAAKyU,qBAC9CvU,KAAKwU,gBAAiB,QAAc1U,EAAK0U,gBACzCxU,KAAKyU,QAAU,KAAe,CAAC,EAAG,KAAe,CAAC,EAAmB,MAAhB3U,EAAK2U,QAAkB,EAAI3U,EAAK2U,YACrFzU,KAAK0U,iBAAmB,KAAe,CACnC,EACA,KAAe,CAAC,EAA4B,MAAzB5U,EAAK4U,iBAA2B,EAAI5U,EAAK4U,qBAEhE1U,KAAK4V,eAAiB9V,EAAK8V,eAC3B5V,KAAKgR,UAAYhR,KAAK6T,MACtB7T,KAAK2U,YAAc,KACnB3U,KAAK4U,qBAAuB,IAChC,CACA,KAAA5T,CAAMZ,GAEF,MAAMI,GADNJ,GAAa,QAAmBA,IACJA,EAAWyB,OAAS,GAChD7B,KAAK6U,OAAS7U,KAAKiB,UAAU,SAAU,CAACT,EAAuB,EAAbR,KAAK6T,OAAY,KAAM7T,KAAKgU,kBAAmBhU,KAAKmU,mBAAmB,EAAMnU,KAAKsU,kBACpItU,KAAK8U,gBAAkB9U,KAAKiB,UAAU,mBAAoB,CAACjB,KAAK6T,MAAoB,EAAb7T,KAAK6T,OAAY,KAAM7T,KAAKiU,qBAAsBjU,KAAKoU,sBAAsB,EAAMpU,KAAKuU,qBAC3JvU,KAAK+T,QACL/T,KAAK+U,KAAO/U,KAAKiB,UAAU,OAAQ,CAAc,EAAbjB,KAAK6T,OAAY,KAAM7T,KAAKkU,gBAAiBlU,KAAKqU,iBAAiB,EAAMrU,KAAKwU,gBAGlHxU,KAAK+U,KAAO,KAIhB/U,KAAKmB,OAAQ,CACjB,CACA,IAAAe,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KAER,GAAsB,IAAlBF,EAAOO,OACP,MAAM,IAAI,KACN,uDAAGP,EAAOO,WAElB,MAAMiE,EAAiC,MAAtB3D,EAAiB,UAAoBA,EAAiB,SACvE,IAAI0T,EAAWvU,EAAO,GACtBA,EAASA,EAAO,GAIZ,EAAItB,KAAKyU,SAAWzU,KAAKyU,QAAU,GAAyB,MAApBzU,KAAK2U,cAC7C3U,KAAK2U,YAAcO,EAAoB,CACnCC,KAAM,IAAM,WAAa7T,GACzBgH,KAAMtI,KAAKyU,QACX3O,WACAgQ,MAAO,KAGX,EAAI9V,KAAK0U,kBAAoB1U,KAAK0U,iBAAmB,GACxB,MAA7B1U,KAAK4U,uBACL5U,KAAK4U,qBAAuBM,EAAoB,CAC5CC,KAAM,IAAM,WAAaU,GACzBvN,KAAMtI,KAAK0U,iBACX5O,WACAgQ,MAAO,KAGf,MAAMV,EAASpV,KAAK2U,YACdU,EAAYrV,KAAK4U,qBACvB,IAAImB,EACAC,EACAC,EACA,EAAIjW,KAAKyU,SAAWzU,KAAKyU,QAAU,IACnCnT,EAAS,MAAQA,EAAQ8T,EAAO,KAEpC,IAAIc,EAAU,KAAM5U,EAAQtB,KAAK6U,OAAOvS,QACpCtC,KAAK+T,UACLmC,EAAU,KAAUA,EAASlW,KAAK+U,KAAKzS,SAEvC,EAAItC,KAAK0U,kBAAoB1U,KAAK0U,iBAAmB,IACrDmB,EAAW,MAAQA,EAAUR,EAAU,KAE3C,MAAMc,EAAuBnW,KAAK8U,gBAAgBxS,QAC3C8T,EAAKC,GAAO,QAAUF,EAAsB,CAAC,EAAInW,KAAK6T,MAAO7T,KAAK6T,OAAQsC,EAAqB1S,KAAO,GACvG6S,EAAc,KAAMT,EAAUO,IAC7BG,EAAIC,EAAIC,GAAM,QAAUP,EAAS,EAAGA,EAAQzS,KAAO,IACnDiT,EAAYC,GAAc,QAAUL,EAAa,EAAGA,EAAY7S,KAAO,GAC9EsS,EAAI/V,KAAK2V,oBAAoBtD,MAAM,MAAQkE,EAAIG,IAC/CV,EAAIhW,KAAK2V,oBAAoBtD,MAAM,MAAQmE,EAAIG,IAC/C,MAAMC,EAAa,KAAM,MAAQZ,EAAGH,GAAWQ,GAC/CJ,EAAKjW,KAAK8T,WAAWzB,MAAM,MAAQoE,EAAIG,IACvC,MAAM3B,EAAI,MAAQ,MAAQc,EAAGF,GAAW,MAAQ,MAAQ,EAAG,MAAQE,IAAKE,IAExE,MAAO,CAAChB,EAAGA,EAAE,GAErB,CACA,SAAAvS,GACI,MAAME,EAAa7C,MAAM2C,YACnBC,EAAS,CACXkR,MAAO7T,KAAK6T,MACZC,YAAY,QAAoB9T,KAAK8T,YACrC6B,qBAAqB,QAAoB3V,KAAK2V,qBAC9C5B,QAAS/T,KAAK+T,QACdC,mBAAmB,QAAqBhU,KAAKgU,mBAC7CC,sBAAsB,QAAqBjU,KAAKiU,sBAChDC,iBAAiB,QAAqBlU,KAAKkU,iBAC3CC,mBAAmB,QAAqBnU,KAAKmU,mBAC7CC,sBAAsB,QAAqBpU,KAAKoU,sBAChDC,iBAAiB,QAAqBrU,KAAKqU,iBAC3CzT,qBAAqB,QAAqBZ,KAAKY,qBAC/C0T,kBAAkB,QAAoBtU,KAAKsU,kBAC3CC,qBAAqB,QAAoBvU,KAAKuU,qBAC9CC,gBAAgB,QAAoBxU,KAAKwU,gBACzCC,QAASzU,KAAKyU,QACdC,iBAAkB1U,KAAK0U,iBACvBkB,eAAgB5V,KAAK4V,eACrBH,YAAY,GAEhB,OAAO5S,OAAOC,OAAO,CAAC,EAAGF,EAAYD,EACzC,EAGJ4S,EAAQxS,UAAY,UACpB,EAAAC,cAAA,cAA4BuS,GACrB,MAAMsB,UAAYjG,EACrB,WAAA/Q,CAAYC,GACJA,EAAK8V,eAIT9V,EAAK+Q,KAAO,IAAI0E,EAAQzV,GACxBC,MAAMD,EAEV,CACA,IAAAoC,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACqB,MAAzBxB,KAAK6Q,KAAK8D,cACV,UAAY3U,KAAK6Q,KAAK8D,aACtB3U,KAAK6Q,KAAK8D,YAAc,MAEU,MAAlC3U,KAAK6Q,KAAK+D,uBACV,UAAY5U,KAAK6Q,KAAK+D,sBACtB5U,KAAK6Q,KAAK+D,qBAAuB,MAErC,MAAMrT,EAAiB,MAAVY,EAAiB,KAAOA,EAAa,KAC5C2D,EAAqB,MAAV3D,EAAiB,KAAOA,EAAiB,SACpDgN,EAAyB,MAAVhN,EAAiB,KAAOA,EAAqB,aAClE,OAAOpC,MAAMmC,KAAKZ,EAAQ,CAAEC,OAAMuE,WAAUqJ,gBAAe,GAEnE,CAEA,iBAAOkE,CAAWC,EAAK3Q,GAInB,OAHgC,IAA5BA,EAAsB,gBACtBA,EAAuB,eAAI,GAExB,IAAI2Q,EAAI3Q,EACnB,EAGJkU,EAAI9T,UAAY,MAChB,EAAAC,cAAA,cAA4B6T,GACrB,MAAMC,UAAiBvD,EAC1B,WAAA1T,CAAYC,GACRC,MAAMD,GACNE,KAAKyT,mBAAqB,OAC1BzT,KAAKwV,6BAA+B,cACpCxV,KAAK0T,2BAA6B,eAClC1T,KAAK2T,8BAAgC,aACrC3T,KAAK4T,yBAA2B,QAChC5T,KAAK6T,MAAQ/T,EAAK+T,OAClB,QAAsB7T,KAAK6T,MAAO,SAClC7T,KAAK8T,YAAa,aAAkC4B,IAApB5V,EAAKgU,WAA2B9T,KAAKyT,mBACjE3T,EAAKgU,YACT9T,KAAK2V,qBAAsB,aAA2CD,IAA7B5V,EAAK6V,oBAC1C3V,KAAKwV,6BACL1V,EAAK6V,qBACT3V,KAAK+T,QAA0B,MAAhBjU,EAAKiU,SAAyBjU,EAAKiU,QAClD/T,KAAKgU,mBAAoB,QAAelU,EAAKkU,mBAAqBhU,KAAK0T,4BACvE1T,KAAKiU,sBAAuB,QAAenU,EAAKmU,sBAAwBjU,KAAK2T,+BAC7E3T,KAAKkU,iBACD,QAAepU,EAAKoU,iBAAmBlU,KAAK4T,0BAChD5T,KAAK+W,eAAiBjX,EAAKiX,eAC3B/W,KAAKmU,mBAAoB,QAAerU,EAAKqU,mBAC7CnU,KAAKoU,sBAAuB,QAAetU,EAAKsU,sBAChDpU,KAAKqU,iBAAkB,QAAevU,EAAKuU,iBAC3CrU,KAAKsU,kBAAmB,QAAcxU,EAAKwU,kBAC3CtU,KAAKuU,qBAAsB,QAAczU,EAAKyU,qBAC9CvU,KAAKwU,gBAAiB,QAAc1U,EAAK0U,gBACzCxU,KAAKyU,QAAU,KAAe,CAAC,EAAG,KAAe,CAAC,EAAmB,MAAhB3U,EAAK2U,QAAkB,EAAI3U,EAAK2U,YACrFzU,KAAK0U,iBAAmB,KAAe,CACnC,EACA,KAAe,CAAC,EAA4B,MAAzB5U,EAAK4U,iBAA2B,EAAI5U,EAAK4U,qBAEhE1U,KAAK4V,eAAiB9V,EAAK8V,eAC3B5V,KAAKgR,UAAY,CAAChR,KAAK6T,MAAO7T,KAAK6T,OACnC7T,KAAK2U,YAAc,KACnB3U,KAAK4U,qBAAuB,IAChC,CACA,KAAA5T,CAAMZ,GACF,IAAI4W,EAEJ,MAAMxW,GADNJ,GAAa,QAAmBA,IACJA,EAAWyB,OAAS,GAGhD,IAAIqS,EACJ,GAHAlU,KAAK6U,OAAS7U,KAAKiB,UAAU,SAAU,CAACT,EAAuB,EAAbR,KAAK6T,OAAY,KAAM7T,KAAKgU,kBAAmBhU,KAAKmU,mBAAmB,EAAMnU,KAAKsU,kBACpItU,KAAK8U,gBAAkB9U,KAAKiB,UAAU,mBAAoB,CAACjB,KAAK6T,MAAoB,EAAb7T,KAAK6T,OAAY,KAAM7T,KAAKiU,qBAAsBjU,KAAKoU,sBAAsB,EAAMpU,KAAKuU,qBAE3JvU,KAAK+T,QAAS,CACd,GAAI/T,KAAK+W,eAAgB,CACrB,MAAME,EAAmBjX,KAAKkU,gBACxBgD,EAAgBlX,KAAK6T,MAC3BK,EAAkB,KAAK8C,EAAK,cAAyB,KAC7C,KAAA3E,CAAM5P,EAAOvB,GAET,MAAMiW,EAAKF,EAAiB5E,MAAM,CAAC6E,IAC7BE,GAAK,IAAK,MAAQ/E,MAAM,CAAC6E,IACzBG,EAASJ,EAAiB5E,MAAM,CAAiB,EAAhB6E,IACvC,OAAO,KAAuB,KAAuBC,EAAIC,GAAKC,EAClE,IAGDtU,UAAY,aACfiU,EACR,MAEI9C,EAAkBlU,KAAKkU,gBAE3BlU,KAAK+U,KAAO/U,KAAKiB,UAAU,OAAQ,CAAc,EAAbjB,KAAK6T,OAAY,KAAMK,EAAiBlU,KAAKqU,iBAAiB,EAAMrU,KAAKwU,eACjH,MAEIxU,KAAK+U,KAAO,KAIhB/U,KAAKmB,OAAQ,CACjB,CACA,IAAAe,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACR,MAAMsE,EAAiC,MAAtB3D,EAAiB,UAAoBA,EAAiB,SAEvE,GAAsB,IAAlBb,EAAOO,OACP,MAAM,IAAI,KACN,wDAAGP,EAAOO,WAElB,IAAIgU,EAAWvU,EAAO,GACtB,MAAMgW,EAAWhW,EAAO,GACxBA,EAASA,EAAO,GACZ,EAAItB,KAAKyU,SAAWzU,KAAKyU,QAAU,GAAyB,MAApBzU,KAAK2U,cAC7C3U,KAAK2U,YAAcO,EAAoB,CACnCC,KAAM,IAAM,WAAa7T,GACzBgH,KAAMtI,KAAKyU,QACX3O,WACAgQ,MAAO,KAGX,EAAI9V,KAAK0U,kBAAoB1U,KAAK0U,iBAAmB,GACxB,MAA7B1U,KAAK4U,uBACL5U,KAAK4U,qBAAuBM,EAAoB,CAC5CC,KAAM,IAAM,WAAaU,GACzBvN,KAAMtI,KAAK0U,iBACX5O,WACAgQ,MAAO,KAGf,MAAMV,EAASpV,KAAK2U,YACdU,EAAYrV,KAAK4U,qBAIvB,IAAI9S,EACAyV,EACAC,EACAC,EACA,EAAIzX,KAAKyU,SAAWzU,KAAKyU,QAAU,IACnCnT,EAAS,MAAQA,EAAQ8T,EAAO,KAEpC,IAAIW,EAAI,KAAMzU,EAAQtB,KAAK6U,OAAOvS,QAC9B,EAAItC,KAAK0U,kBAAoB1U,KAAK0U,iBAAmB,IACrDmB,EAAW,MAAQA,EAAUR,EAAU,KAE3CU,EAAI,MAAQA,EAAG,KAAMF,EAAU7V,KAAK8U,gBAAgBxS,SAChDtC,KAAK+T,UACLgC,EAAI,KAAUA,EAAG/V,KAAK+U,KAAKzS,SAE/B,MAAOoV,EAAIC,EAAIC,EAAIC,GAAM,QAAU9B,EAAG,EAAGA,EAAEtS,KAAO,GAClD3B,EAAI9B,KAAK2V,oBAAoBtD,MAAMqF,GACnCH,EAAIvX,KAAK2V,oBAAoBtD,MAAMsF,GACnCH,EAAI,MAAQ,MAAQD,EAAGD,GAAW,MAAQxV,EAAG9B,KAAK8T,WAAWzB,MAAMuF,KACnEH,EAAIzX,KAAK2V,oBAAoBtD,MAAMwF,GACnC,MAAM5C,EAAI,MAAQwC,EAAGzX,KAAK8T,WAAWzB,MAAMmF,IAE3C,MAAO,CAACvC,EAAGA,EAAGuC,EAAE,GAExB,CACA,SAAA9U,GACI,MAAME,EAAa7C,MAAM2C,YACnBC,EAAS,CACXkR,MAAO7T,KAAK6T,MACZC,YAAY,QAAoB9T,KAAK8T,YACrC6B,qBAAqB,QAAoB3V,KAAK2V,qBAC9C5B,QAAS/T,KAAK+T,QACdC,mBAAmB,QAAqBhU,KAAKgU,mBAC7CC,sBAAsB,QAAqBjU,KAAKiU,sBAChDC,iBAAiB,QAAqBlU,KAAKkU,iBAC3C6C,eAAgB/W,KAAK+W,eACrB5C,mBAAmB,QAAqBnU,KAAKmU,mBAC7CC,sBAAsB,QAAqBpU,KAAKoU,sBAChDC,iBAAiB,QAAqBrU,KAAKqU,iBAC3CzT,qBAAqB,QAAqBZ,KAAKY,qBAC/C0T,kBAAkB,QAAoBtU,KAAKsU,kBAC3CC,qBAAqB,QAAoBvU,KAAKuU,qBAC9CC,gBAAgB,QAAoBxU,KAAKwU,gBACzCC,QAASzU,KAAKyU,QACdC,iBAAkB1U,KAAK0U,iBACvBkB,eAAgB5V,KAAK4V,gBAEzB,OAAO/S,OAAOC,OAAO,CAAC,EAAGF,EAAYD,EACzC,EAGJmU,EAAS/T,UAAY,WACrB,EAAAC,cAAA,cAA4B8T,GACrB,MAAMgB,UAAalH,EACtB,WAAA/Q,CAAYC,GACJA,EAAK8V,eAIT9V,EAAK+Q,KAAO,IAAIiG,EAAShX,GACzBC,MAAMD,EAEV,CACA,IAAAoC,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KACqB,MAAzBxB,KAAK6Q,KAAK8D,cACV,UAAY3U,KAAK6Q,KAAK8D,aACtB3U,KAAK6Q,KAAK8D,YAAc,MAEU,MAAlC3U,KAAK6Q,KAAK+D,uBACV,UAAY5U,KAAK6Q,KAAK+D,sBACtB5U,KAAK6Q,KAAK+D,qBAAuB,MAErC,MAAMrT,EAAiB,MAAVY,EAAiB,KAAOA,EAAa,KAC5C2D,EAAqB,MAAV3D,EAAiB,KAAOA,EAAiB,SACpDgN,EAAyB,MAAVhN,EAAiB,KAAOA,EAAqB,aAClE,OAAOpC,MAAMmC,KAAKZ,EAAQ,CAAEC,OAAMuE,WAAUqJ,gBAAe,GAEnE,CAEA,iBAAOkE,CAAWC,EAAK3Q,GAInB,OAHgC,IAA5BA,EAAsB,gBACtBA,EAAuB,eAAI,GAExB,IAAI2Q,EAAI3Q,EACnB,EAGJmV,EAAK/U,UAAY,OACjB,EAAAC,cAAA,cAA4B8U,GACrB,MAAMhH,UAAwByC,EACjC,WAAA1T,CAAYC,GACRC,MAAMD,GACNE,KAAK+Q,MAAQjR,EAAKiR,KACtB,CACA,aAAIC,GAKA,MAAMA,EAAY,GAClB,IAAK,MAAMH,KAAQ7Q,KAAK+Q,MAAMlN,QAAQkU,UAC9B1Q,MAAMC,QAAQuJ,EAAKG,WACnBA,EAAU5M,QAAQyM,EAAKG,WAGvBA,EAAU5M,KAAKyM,EAAKG,WAG5B,OAAOA,CACX,CACA,IAAA9O,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KAER,IAAIuO,EAASzO,EAAOuC,MAAM,GAE1B,MAAMmU,EAAe,GACrB,IAAK,MAAMnH,KAAQ7Q,KAAK+Q,MAAMlN,QAAQkU,UAC9B1Q,MAAMC,QAAQuJ,EAAKG,WACnBgH,EAAa5T,KAAK2L,EAAOhK,OAAO,EAAG8K,EAAKG,UAAUnP,SAGlDmW,EAAa5T,KAAK2L,EAAOhK,OAAO,EAAG,IAG3CiS,EAAaD,UAEb,MAAME,EAAkB,GACxB,IAAIC,EACJ,IAAK,IAAIpW,EAAI,EAAGA,EAAI9B,KAAK+Q,MAAMlP,SAAUC,EAAG,CACxC,MAAM+O,EAAO7Q,KAAK+Q,MAAMjP,GACxBiO,EAASiI,EAAalW,GAGlBoW,EADM,IAANpW,EACa,CAACR,EAAO,IAAIf,OAAOwP,GAGnB,CAACmI,EAAW,IAAI3X,OAAOwP,GAExCmI,EAAarH,EAAK3O,KAAKgW,EAAY/V,GACnC8V,EAAgB7T,KAAK8T,EAAWrU,MAAM,GAC1C,CAEAkM,EAAS,GACT,IAAK,MAAMoI,KAAcF,EAAgBpU,QAAQkU,UAC7ChI,EAAO3L,QAAQ+T,GAEnB,MAAO,CAACD,EAAW,IAAI3X,OAAOwP,EAAO,GAE7C,CACA,KAAA/O,CAAMZ,GAOF,IAAIK,GANA,QAAgBL,KAGhBA,EAAaA,EAAW,IAI5BJ,KAAK+Q,MAAMzD,SAAQ,CAACuD,EAAM/O,MACtB,QAAU,WAAWA,KAAK,KAEtB+O,EAAK7P,MAAMZ,GAEPK,EADA4G,MAAMC,QAAQuJ,EAAKG,WACPH,EAAKG,UAAU,GAGfH,EAAKG,UAErB5Q,EAAa,CAACA,EAAW,GAAIK,EAAU,GACzC,IAENT,KAAKmB,OAAQ,CACjB,CACA,SAAAuB,GACI,MAAME,EAAa7C,MAAM2C,YAQnBC,EAAS,CAAE,MADG3C,KAAK+Q,MAAMtJ,KANRoJ,IACZ,CACH,UAAaA,EAAKuC,eAClB,OAAUvC,EAAKnO,iBAKvB,OAAOG,OAAOC,OAAO,CAAC,EAAGF,EAAYD,EACzC,CAEA,iBAAO0Q,CAAWC,EAAK3Q,EAAQmM,EAAgB,CAAC,GAC5C,MAAMiC,EAAQ,GACd,IAAK,MAAMoC,KAAcxQ,EAAc,MACnCoO,EAAM3M,MAAK,OAAY+O,EAAYrE,IAEvC,OAAO,IAAIwE,EAAI,CAAEvC,SACrB,CACA,oBAAIgC,GACA,IAAK/S,KAAK0H,UACN,MAAO,GAEX,MAAMuL,EAAU,GAChB,IAAK,MAAMpC,KAAQ7Q,KAAK+Q,MACpBkC,EAAQ7O,QAAQyM,EAAKkC,kBAEzB,OAAOE,CACX,CACA,uBAAID,GACA,MAAMC,EAAU,GAChB,IAAK,MAAMpC,KAAQ7Q,KAAK+Q,MACpBkC,EAAQ7O,QAAQyM,EAAKmC,qBAEzB,IAAKhT,KAAK0H,UAAW,CACjB,MAAMqL,EAAmB,GACzB,IAAK,MAAMlC,KAAQ7Q,KAAK+Q,MACpBgC,EAAiB3O,QAAQyM,EAAKkC,kBAElC,OAAOA,EAAiBxS,OAAO0S,EACnC,CACA,OAAOA,CACX,CAMA,UAAAmF,GACI,MAAMnF,EAAU,GAChB,IAAK,MAAMpC,KAAQ7Q,KAAK+Q,MACpBkC,EAAQ7O,QAAQyM,EAAKoC,SAEzB,OAAO,QAAcA,EACzB,CAOA,UAAAoF,CAAWpF,GACP,MAAMqF,EAAS,GACf,IAAK,MAAMzH,KAAQ7Q,KAAK+Q,MAAO,CAC3B,MAAMwH,EAAY1H,EAAKoC,QAAQpR,OACzB2W,EAAevF,EAAQlN,OAAOwS,GACpC,IAAK,IAAIzW,EAAI,EAAGA,EAAI+O,EAAKoC,QAAQpR,SAAUC,EACvCwW,EAAOlU,KAAK,CAACyM,EAAKoC,QAAQnR,GAAI0W,EAAa1W,IAEnD,EACA,QAAcwW,EAClB,EAKG,SAASpD,EAAoBpV,GAChC,MAAM,KAAEqV,EAAI,KAAE7M,EAAI,SAAExC,GAAW,EAAK,MAAEgQ,EAAQ,GAAMhW,EAC9C+I,EAAgB,IAAM,KAAUsM,IAAQ7M,GACxCmQ,EAAa,IAAM,KAAe5P,EAAesM,EAAMrP,GAE7D,IAAKgQ,GAASA,GAAS,EACnB,OAAO,OAAS2C,IAAahM,SAGjC,OADcpF,MAAMyO,GAAO4C,UAAKhD,GAAWjO,IAAIgR,GAClChR,KAAI8E,GAAK,OAASA,EAAEE,UACrC,CAZAqE,EAAgB/N,UAAY,kBAC5B,EAAAC,cAAA,cAA4B8N,E,iFCttCrB,MAAM6H,UAAsB,KAC/B,WAAA9Y,CAAYC,GASR,GARY,MAARA,IACAA,EAAO,CAAC,GAEZC,MAAMD,GACNE,KAAKwJ,WACkB,MAAnB1J,EAAK0J,YAAqB,UAAoB1J,EAAK0J,WAGnC,MAAhB1J,EAAKyJ,QACLvJ,KAAKuJ,QAAU,CAAC,CAAC,EAAG,GAAI,CAAC,EAAG,SAE3B,GAA4B,kBAAjBzJ,EAAKyJ,QACjBvJ,KAAKuJ,QACD,CAAC,CAACzJ,EAAKyJ,QAASzJ,EAAKyJ,SAAU,CAACzJ,EAAKyJ,QAASzJ,EAAKyJ,cAEtD,CAED,GADAzJ,EAAKyJ,QAAUzJ,EAAKyJ,QACQ,IAAxBzJ,EAAKyJ,QAAQ1H,OACb,MAAM,IAAI,KACN,+EAAqB/B,EAAKyJ,QAAQ1H,iBAE1C,IAAI+W,EACAC,EACJ,GAA+B,kBAApB/Y,EAAKyJ,QAAQ,GACpBqP,EAAgB,CAAC9Y,EAAKyJ,QAAQ,GAAIzJ,EAAKyJ,QAAQ,IAC/CsP,EAAe,CAAC/Y,EAAKyJ,QAAQ,GAAIzJ,EAAKyJ,QAAQ,QAE7C,CAED,GADAzJ,EAAKyJ,QAAUzJ,EAAKyJ,QACW,IAA3BzJ,EAAKyJ,QAAQ,GAAG1H,OAChB,MAAM,IAAI,KACN,sFAAyB/B,EAAKyJ,QAAQ,GAAG1H,iBAGjD,GADA+W,EAAgB9Y,EAAKyJ,QAAQ,GACE,IAA3BzJ,EAAKyJ,QAAQ,GAAG1H,OAChB,MAAM,IAAI,KACN,qFAAyB/B,EAAKyJ,QAAQ,GAAG1H,iBAEjDgX,EAAe/Y,EAAKyJ,QAAQ,EAChC,CACAvJ,KAAKuJ,QAAU,CAACqP,EAAeC,EACnC,CACA7Y,KAAKyF,UAAY,CAAC,IAAI,KAAU,CAAEC,KAAM,IAC5C,CACA,kBAAA/D,CAAmBvB,GAEf,IAAI+J,EACAC,EACJ,OAHAhK,GAAa,QAAmBA,GAGR,kBAApBJ,KAAKwJ,YAEDW,EADiB,MAAjB/J,EAAW,IAAcA,EAAW,IAAM,EACnCA,EAAW,GAAKJ,KAAKuJ,QAAQ,GAAG,GAAKvJ,KAAKuJ,QAAQ,GAAG,GAGrD,KAGPa,EADiB,MAAjBhK,EAAW,IAAcA,EAAW,IAAM,EACnCA,EAAW,GAAKJ,KAAKuJ,QAAQ,GAAG,GAAKvJ,KAAKuJ,QAAQ,GAAG,GAGrD,KAEJ,CAACnJ,EAAW,GAAIA,EAAW,GAAI+J,EAAMC,KAIxCD,EADiB,MAAjB/J,EAAW,IAAcA,EAAW,IAAM,EACnCA,EAAW,GAAKJ,KAAKuJ,QAAQ,GAAG,GAAKvJ,KAAKuJ,QAAQ,GAAG,GAGrD,KAGPa,EADiB,MAAjBhK,EAAW,IAAcA,EAAW,IAAM,EACnCA,EAAW,GAAKJ,KAAKuJ,QAAQ,GAAG,GAAKvJ,KAAKuJ,QAAQ,GAAG,GAGrD,KAEJ,CAACnJ,EAAW,GAAI+J,EAAMC,EAAMhK,EAAW,IAEtD,CACA,IAAA8B,CAAKZ,EAAQa,GACT,OAAO,IAAAX,OAAK,KAAMsX,OAjHO5V,GAiHU,QAAoB5B,GAjH3BiI,EAiHoCvJ,KAAKuJ,QAjHhCC,EAiHyCxJ,KAAKwJ,YAhHhF,IAAAhI,OAAK,KACR,GAAe,IAAX0B,EAAEO,KACF,MAAM,IAAI,KACN,kEAAGP,EAAEO,kBAKb,GAHe,MAAX8F,IACAA,EAAU,CAAC,CAAC,EAAG,GAAI,CAAC,EAAG,KAEJ,IAAnBA,EAAQ1H,QAAsC,IAAtB0H,EAAQ,GAAG1H,QACb,IAAtB0H,EAAQ,GAAG1H,OACX,MAAM,IAAI,KAAW,+GAMzB,GAHkB,MAAd2H,IACAA,GAAa,WAEE,iBAAfA,GAAgD,kBAAfA,EACjC,MAAM,IAAI,KAAW,wBAAwBA,oEAGjD,IAAIuP,EAOJ,OALIA,EADe,kBAAfvP,EACU,CAAC,CAAC,EAAG,GAAI,CAAC,EAAG,GAAID,EAAQ,GAAIA,EAAQ,IAGrC,CAAC,CAAC,EAAG,GAAIA,EAAQ,GAAIA,EAAQ,GAAI,CAAC,EAAG,IAE5C,MAAQrG,EAAG6V,EAAQ,IA5B3B,IAA0B7V,EAAGqG,EAASC,CAiHyD,GAClG,CACA,SAAA9G,GACI,MAAMC,EAAS,CACX4G,QAASvJ,KAAKuJ,QACdC,WAAYxJ,KAAKwJ,YAEf5G,EAAa7C,MAAM2C,YAEzB,OADAG,OAAOC,OAAOH,EAAQC,GACfD,CACX,EAGJgW,EAAc5V,UAAY,gBAC1B,EAAAC,cAAA,cAA4B2V,E","sources":["webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/layers/embeddings.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/layers/normalization.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/layers/noise.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/layers/pooling.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/layers/merge.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/layers/serialization.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-layers/dist/layers/padding.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Embedding Layer.\n *\n * Original source: keras/constraints.py\n */\nimport { notEqual, serialization, tidy, zerosLike } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { Layer } from '../engine/topology';\nimport { ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport class Embedding extends Layer {\n    constructor(args) {\n        super(args);\n        this.embeddings = null;\n        this.DEFAULT_EMBEDDINGS_INITIALIZER = 'randomUniform';\n        if (args.batchInputShape == null && args.inputShape == null) {\n            // Porting Note: This logic is copied from Layer's constructor, since we\n            // can't do exactly what the Python constructor does for Embedding().\n            // Specifically, the super constructor can not be called after the\n            // mutation of the `config` argument.\n            let batchSize = null;\n            if (args.batchSize != null) {\n                batchSize = args.batchSize;\n            }\n            if (args.inputLength == null) {\n                // Fix super-constructor to what it would have done if\n                // 'config.inputShape' were (None, )\n                this.batchInputShape = [batchSize, null];\n            }\n            else {\n                // Fix super-constructor to what it would have done if\n                // 'config.inputShape' were (config.inputLength, )\n                this.batchInputShape =\n                    [batchSize].concat(generic_utils.toList(args.inputLength));\n            }\n        }\n        this.inputDim = args.inputDim;\n        generic_utils.assertPositiveInteger(this.inputDim, 'inputDim');\n        this.outputDim = args.outputDim;\n        generic_utils.assertPositiveInteger(this.outputDim, 'outputDim');\n        this.embeddingsInitializer = getInitializer(args.embeddingsInitializer || this.DEFAULT_EMBEDDINGS_INITIALIZER);\n        this.embeddingsRegularizer = getRegularizer(args.embeddingsRegularizer);\n        this.activityRegularizer = getRegularizer(args.activityRegularizer);\n        this.embeddingsConstraint = getConstraint(args.embeddingsConstraint);\n        this.maskZero = args.maskZero;\n        this.supportsMasking = args.maskZero;\n        this.inputLength = args.inputLength;\n    }\n    build(inputShape) {\n        this.embeddings = this.addWeight('embeddings', [this.inputDim, this.outputDim], this.dtype, this.embeddingsInitializer, this.embeddingsRegularizer, true, this.embeddingsConstraint);\n        this.built = true;\n    }\n    // Override warnOnIncompatibleInputShape because an embedding layer allows\n    // the input to have varying ranks.\n    warnOnIncompatibleInputShape(inputShape) { }\n    computeMask(inputs, mask) {\n        return tidy(() => {\n            if (!this.maskZero) {\n                return null;\n            }\n            else {\n                inputs = getExactlyOneTensor(inputs);\n                return notEqual(inputs, zerosLike(inputs));\n            }\n        });\n    }\n    computeOutputShape(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        if (this.inputLength == null) {\n            return [...inputShape, this.outputDim];\n        }\n        // inputLength can be an array if input is 3D or higher.\n        const inLens = generic_utils.toList(this.inputLength);\n        if (inLens.length !== inputShape.length - 1) {\n            throw new ValueError(`\"inputLength\" is ${this.inputLength}, but received ` +\n                `input shape has shape ${inputShape}`);\n        }\n        else {\n            let i = 0;\n            for (let k = 0; k < inLens.length; ++k) {\n                const s1 = inLens[k];\n                const s2 = inputShape[k + 1];\n                if ((s1 != null) && (s2 != null) && (s1 !== s2)) {\n                    throw new ValueError(`\"inputLength\" is ${this.inputLength}, but received ` +\n                        `input shape has shape ${inputShape}`);\n                }\n                else if (s1 == null) {\n                    inLens[i] = s2;\n                }\n                i++;\n            }\n        }\n        return [inputShape[0], ...inLens, this.outputDim];\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            // Embedding layer accepts only a single input.\n            let input = getExactlyOneTensor(inputs);\n            if (input.dtype !== 'int32') {\n                input = K.cast(input, 'int32');\n            }\n            const output = K.gather(this.embeddings.read(), input.as1D());\n            return output.reshape(getExactlyOneShape(this.computeOutputShape(input.shape)));\n        });\n    }\n    getConfig() {\n        const config = {\n            inputDim: this.inputDim,\n            outputDim: this.outputDim,\n            embeddingsInitializer: serializeInitializer(this.embeddingsInitializer),\n            embeddingsRegularizer: serializeRegularizer(this.embeddingsRegularizer),\n            activityRegularizer: serializeRegularizer(this.activityRegularizer),\n            embeddingsConstraint: serializeConstraint(this.embeddingsConstraint),\n            maskZero: this.maskZero,\n            inputLength: this.inputLength\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nEmbedding.className = 'Embedding';\nserialization.registerClass(Embedding);\n//# sourceMappingURL=embeddings.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(x, mean, variance, beta, gamma, epsilon = 1e-3) {\n    let out;\n    if (x.rank === 2) {\n        out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else if (x.rank === 3) {\n        // TODO(cais): Check rank; give proper error message.\n        out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else if (x.rank === 4) {\n        out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else {\n        throw new NotImplementedError(`batchNormalization is not implemented for array of rank ${x.rank} ` +\n            `yet`);\n    }\n    return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    return tidy(() => {\n        const meanAndVariance = tfc.moments(x, reductionAxes);\n        const mean = meanAndVariance.mean;\n        const variance = meanAndVariance.variance;\n        const normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    return tidy(() => {\n        const meanAndVariance = tfc.moments(x, reductionAxes);\n        const mean = meanAndVariance.mean;\n        const variance = meanAndVariance.variance;\n        const targetShape = [];\n        for (const axis of math_utils.range(0, x.rank)) {\n            if (reductionAxes.indexOf(axis) !== -1) {\n                targetShape.push(1);\n            }\n            else {\n                targetShape.push(x.shape[axis]);\n            }\n        }\n        const broadcastMean = mean.reshape(targetShape);\n        const broadcastVariance = variance.reshape(targetShape);\n        const broadcastGamma = gamma == null ? null : gamma.reshape(targetShape);\n        const broadcastBeta = beta == null ? null : beta.reshape(targetShape);\n        const normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n        return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n    else {\n        return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n}\nexport class BatchNormalization extends Layer {\n    constructor(args) {\n        if (args == null) {\n            args = {};\n        }\n        super(args);\n        this.supportsMasking = true;\n        this.axis = args.axis == null ? -1 : args.axis;\n        this.momentum = args.momentum == null ? 0.99 : args.momentum;\n        this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n        this.center = args.center == null ? true : args.center;\n        this.scale = args.scale == null ? true : args.scale;\n        this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n        this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n        this.movingMeanInitializer =\n            getInitializer(args.movingMeanInitializer || 'zeros');\n        this.movingVarianceInitializer =\n            getInitializer(args.movingVarianceInitializer || 'ones');\n        this.betaConstraint = getConstraint(args.betaConstraint);\n        this.gammaConstraint = getConstraint(args.gammaConstraint);\n        this.betaRegularizer = getRegularizer(args.betaRegularizer);\n        this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n        const dim = inputShape[axis];\n        if (dim == null) {\n            throw new ValueError(`Axis ${axis} of input tensor should have a defined dimension but ` +\n                `the layer received an input with shape ` +\n                `${JSON.stringify(inputShape)}.`);\n        }\n        this.inputSpec =\n            [new InputSpec({ ndim: inputShape.length, axes: { [axis]: dim } })];\n        const shape = [dim];\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n        }\n        this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n        this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            const input = getExactlyOneTensor(inputs);\n            const inputShape = input.shape;\n            const ndim = inputShape.length;\n            const reductionAxes = math_utils.range(0, ndim);\n            const axis = this.axis >= 0 ? this.axis : (this.axis + ndim);\n            reductionAxes.splice(axis, 1);\n            const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n            broadcastShape[axis] = inputShape[axis];\n            const sortedReductionAxes = reductionAxes.slice();\n            sortedReductionAxes.sort();\n            const needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n            const normalizeInference = () => {\n                if (needsBroadcasting) {\n                    const broadcastMovingMean = this.movingMean.read().reshape(broadcastShape);\n                    const broadcastMovingVariance = this.movingVariance.read().reshape(broadcastShape);\n                    const broadcastBeta = this.center ? this.beta.read().reshape(broadcastShape) : null;\n                    const broadcastGamma = this.scale ? this.gamma.read().reshape(broadcastShape) : null;\n                    return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, this.epsilon);\n                }\n                else {\n                    return batchNormalization(input, this.movingMean.read(), this.movingVariance.read(), this.beta == null ? null : this.beta.read(), this.gamma == null ? null : this.gamma.read(), this.epsilon);\n                }\n            };\n            if (!training) {\n                return normalizeInference();\n            }\n            const [normedTraining, mean, variance] = normalizeBatchInTraining(input, this.gamma.read(), this.beta.read(), reductionAxes, this.epsilon);\n            const doMovingAverage = (variable, value, momentum) => {\n                tfc.tidy(() => {\n                    const decay = 1 - momentum;\n                    const origValue = variable.read();\n                    const updateDelta = origValue.sub(value).mul(decay);\n                    variable.write(origValue.sub(updateDelta));\n                });\n            };\n            // Perform updates to moving mean and moving variance for training.\n            // Porting Note: In PyKeras, these updates to `movingMean` and\n            //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n            //   `update`s using the `add_update()` method. Here we do it imperatively\n            //   and encapsulate the updates in a function that is invoked\n            //   immediately.\n            const updateMovingMeanAndVariance = () => {\n                doMovingAverage(this.movingMean, mean, this.momentum);\n                doMovingAverage(this.movingVariance, variance, this.momentum);\n            };\n            updateMovingMeanAndVariance();\n            return normedTraining;\n        });\n    }\n    getConfig() {\n        const config = {\n            axis: this.axis,\n            momentum: this.momentum,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: serializeInitializer(this.betaInitializer),\n            gammaInitializer: serializeInitializer(this.gammaInitializer),\n            movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n            movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n            betaRegularizer: serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n            betaConstraint: serializeConstraint(this.betaConstraint),\n            gammaConstraint: serializeConstraint(this.gammaConstraint)\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport class LayerNormalization extends Layer {\n    constructor(args) {\n        if (args == null) {\n            args = {};\n        }\n        super(args);\n        this.axis = args.axis == null ? -1 : args.axis;\n        if (typeof this.axis === 'number') {\n            if (!Number.isInteger(this.axis)) {\n                throw new Error(`Expected axis to be an integer, but received ${this.axis}`);\n            }\n        }\n        else if (Array.isArray(this.axis)) {\n            for (const axis of this.axis) {\n                if (!Number.isInteger(axis)) {\n                    throw new Error(`Expected axis to be an array of integers, ` +\n                        `but received ${JSON.stringify(this.axis)}`);\n                }\n            }\n        }\n        else {\n            throw new Error(`Expected axis to be an integer or an array of integers, ` +\n                `but received ${JSON.stringify(this.axis)}`);\n        }\n        this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n        this.center = args.center == null ? true : args.center;\n        this.scale = args.scale == null ? true : args.scale;\n        this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n        this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n        this.betaRegularizer = getRegularizer(args.betaRegularizer);\n        this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n        this.supportsMasking = true;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const nDims = inputShape.length;\n        // Convert axis to array and resolve negatives.\n        if (typeof this.axis === 'number') {\n            this.axis = [this.axis];\n        }\n        for (let i = 0; i < this.axis.length; ++i) {\n            if (this.axis[i] < 0) {\n                this.axis[i] += nDims;\n            }\n        }\n        // Further validate axes.\n        for (const axis of this.axis) {\n            if (axis < 0 || axis >= nDims) {\n                throw new Error(`Invalid axis: ${axis}`);\n            }\n        }\n        if (this.axis.length !== generic_utils.unique(this.axis).length) {\n            throw new Error(`Found duplicate axes in: ${this.axis}`);\n        }\n        const paramShape = this.axis.map(axis => inputShape[axis]);\n        const trainable = true;\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n        }\n        else {\n            this.gamma = null;\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n        }\n        else {\n            this.beta = null;\n        }\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        const input = getExactlyOneTensor(inputs);\n        const inputShape = input.shape;\n        const nDims = inputShape.length;\n        return tidy(() => {\n            const keepDims = true;\n            let { mean, variance } = moments(input, this.axis, keepDims);\n            const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n            for (const dim of this.axis) {\n                broadcastShape[dim] = inputShape[dim];\n            }\n            const broadcast = (v) => {\n                if (v != null && v.shape.length !== nDims &&\n                    this.axis !== [nDims - 1]) {\n                    return v.reshape(broadcastShape);\n                }\n                else {\n                    return v;\n                }\n            };\n            let scale = broadcast(this.gamma.read());\n            let offset = broadcast(this.beta.read());\n            // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n            // is a workaround for the limitation of core's batchNormalization?d don't\n            // support broadcasting in their gradients. In addition, the tiling is\n            // necessary to ensure correctness on the browser CPU backend regardless\n            // of forward or backward computation. Remove this workaround once the\n            // limitation is addressed. See .\n            const momentsTiling = [];\n            const scaleOffsetTiling = [];\n            for (let i = 0; i < nDims; ++i) {\n                if (this.axis.indexOf(i) !== -1) {\n                    momentsTiling.push(inputShape[i]);\n                    scaleOffsetTiling.push(1);\n                }\n                else {\n                    momentsTiling.push(1);\n                    scaleOffsetTiling.push(inputShape[i]);\n                }\n            }\n            mean = mean.tile(momentsTiling);\n            variance = variance.tile(momentsTiling);\n            scale = scale.tile(scaleOffsetTiling);\n            offset = offset.tile(scaleOffsetTiling);\n            return batchNormalization(input, mean, variance, offset, scale, this.epsilon);\n        });\n    }\n    getConfig() {\n        const config = {\n            axis: this.axis,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: serializeInitializer(this.betaInitializer),\n            gammaInitializer: serializeInitializer(this.gammaInitializer),\n            betaRegularizer: serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);\n//# sourceMappingURL=normalization.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\nimport { greaterEqual, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { getExactlyOneTensor } from '../utils/types_utils';\nexport class GaussianNoise extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.stddev = args.stddev;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { stddev: this.stddev };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            const input = getExactlyOneTensor(inputs);\n            const noised = () => K.randomNormal(input.shape, 0, this.stddev).add(input);\n            const output = K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n            return output;\n        });\n    }\n}\n/** @nocollapse */\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport class GaussianDropout extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.rate = args.rate;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { rate: this.rate };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            const input = getExactlyOneTensor(inputs);\n            if (this.rate > 0 && this.rate < 1) {\n                const noised = () => {\n                    const stddev = Math.sqrt(this.rate / (1 - this.rate));\n                    return input.mul(K.randomNormal(input.shape, 1, stddev));\n                };\n                return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n            }\n            return input;\n        });\n    }\n}\n/** @nocollapse */\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\nexport class AlphaDropout extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.rate = args.rate;\n        this.noiseShape = args.noiseShape;\n    }\n    _getNoiseShape(inputs) {\n        return this.noiseShape || getExactlyOneTensor(inputs).shape;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { rate: this.rate };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.rate < 1 && this.rate > 0) {\n                const noiseShape = this._getNoiseShape(inputs);\n                const droppedInputs = () => {\n                    const input = getExactlyOneTensor(inputs);\n                    const alpha = 1.6732632423543772848170429916717;\n                    const scale = 1.0507009873554804934193349852946;\n                    const alphaP = -alpha * scale;\n                    let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\n                    keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n                    // Get affine transformation params.\n                    const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\n                    const b = -a * alphaP * this.rate;\n                    // Apply mask.\n                    const x = input.mul(keptIdx).add(keptIdx.add(-1).mul(alphaP));\n                    return x.mul(a).add(b);\n                };\n                return K.inTrainPhase(droppedInputs, () => getExactlyOneTensor(inputs), kwargs['training'] || false);\n            }\n            return inputs;\n        });\n    }\n}\n/** @nocollapse */\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);\n//# sourceMappingURL=noise.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Pooling Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport { imageDataFormat } from '../backend/common';\nimport * as K from '../backend/tfjs_backend';\nimport { checkDataFormat, checkPaddingMode, checkPoolMode } from '../common';\nimport { InputSpec } from '../engine/topology';\nimport { Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { convOutputLength } from '../utils/conv_utils';\nimport { assertPositiveInteger } from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { preprocessConv2DInput, preprocessConv3DInput } from './convolutional';\n/**\n * 2D pooling.\n * @param x\n * @param poolSize\n * @param stridesdes strides. Defaults to [1, 1].\n * @param padding padding. Defaults to 'valid'.\n * @param dataFormat data format. Defaults to 'channelsLast'.\n * @param poolMode Mode of pooling. Defaults to 'max'.\n * @returns Result of the 2D pooling.\n */\nexport function pool2d(x, poolSize, strides, padding, dataFormat, poolMode) {\n    return tidy(() => {\n        checkDataFormat(dataFormat);\n        checkPoolMode(poolMode);\n        checkPaddingMode(padding);\n        if (strides == null) {\n            strides = [1, 1];\n        }\n        if (padding == null) {\n            padding = 'valid';\n        }\n        if (dataFormat == null) {\n            dataFormat = imageDataFormat();\n        }\n        if (poolMode == null) {\n            poolMode = 'max';\n        }\n        // TODO(cais): Remove the preprocessing step once deeplearn.js supports\n        // dataFormat as an input argument.\n        x = preprocessConv2DInput(x, dataFormat); // x is NHWC after preprocessing.\n        let y;\n        const paddingString = (padding === 'same') ? 'same' : 'valid';\n        if (poolMode === 'max') {\n            // TODO(cais): Rank check?\n            y = tfc.maxPool(x, poolSize, strides, paddingString);\n        }\n        else { // 'avg'\n            // TODO(cais): Check the dtype and rank of x and give clear error message\n            //   if those are incorrect.\n            y = tfc.avgPool(\n            // TODO(cais): Rank check?\n            x, poolSize, strides, paddingString);\n        }\n        if (dataFormat === 'channelsFirst') {\n            y = tfc.transpose(y, [0, 3, 1, 2]); // NHWC -> NCHW.\n        }\n        return y;\n    });\n}\n/**\n * 3D pooling.\n * @param x\n * @param poolSize. Default to [1, 1, 1].\n * @param strides strides. Defaults to [1, 1, 1].\n * @param padding padding. Defaults to 'valid'.\n * @param dataFormat data format. Defaults to 'channelsLast'.\n * @param poolMode Mode of pooling. Defaults to 'max'.\n * @returns Result of the 3D pooling.\n */\nexport function pool3d(x, poolSize, strides, padding, dataFormat, poolMode) {\n    return tidy(() => {\n        checkDataFormat(dataFormat);\n        checkPoolMode(poolMode);\n        checkPaddingMode(padding);\n        if (strides == null) {\n            strides = [1, 1, 1];\n        }\n        if (padding == null) {\n            padding = 'valid';\n        }\n        if (dataFormat == null) {\n            dataFormat = imageDataFormat();\n        }\n        if (poolMode == null) {\n            poolMode = 'max';\n        }\n        // x is NDHWC after preprocessing.\n        x = preprocessConv3DInput(x, dataFormat);\n        let y;\n        const paddingString = (padding === 'same') ? 'same' : 'valid';\n        if (poolMode === 'max') {\n            y = tfc.maxPool3d(x, poolSize, strides, paddingString);\n        }\n        else { // 'avg'\n            y = tfc.avgPool3d(x, poolSize, strides, paddingString);\n        }\n        if (dataFormat === 'channelsFirst') {\n            y = tfc.transpose(y, [0, 4, 1, 2, 3]); // NDHWC -> NCDHW.\n        }\n        return y;\n    });\n}\n/**\n * Abstract class for different pooling 1D layers.\n */\nexport class Pooling1D extends Layer {\n    /**\n     *\n     * @param args Parameters for the Pooling layer.\n     *\n     * config.poolSize defaults to 2.\n     */\n    constructor(args) {\n        if (args.poolSize == null) {\n            args.poolSize = 2;\n        }\n        super(args);\n        if (typeof args.poolSize === 'number') {\n            this.poolSize = [args.poolSize];\n        }\n        else if (Array.isArray(args.poolSize) &&\n            args.poolSize.length === 1 &&\n            typeof args.poolSize[0] === 'number') {\n            this.poolSize = args.poolSize;\n        }\n        else {\n            throw new ValueError(`poolSize for 1D convolutional layer must be a number or an ` +\n                `Array of a single number, but received ` +\n                `${JSON.stringify(args.poolSize)}`);\n        }\n        assertPositiveInteger(this.poolSize, 'poolSize');\n        if (args.strides == null) {\n            this.strides = this.poolSize;\n        }\n        else {\n            if (typeof args.strides === 'number') {\n                this.strides = [args.strides];\n            }\n            else if (Array.isArray(args.strides) &&\n                args.strides.length === 1 &&\n                typeof args.strides[0] === 'number') {\n                this.strides = args.strides;\n            }\n            else {\n                throw new ValueError(`strides for 1D convolutional layer must be a number or an ` +\n                    `Array of a single number, but received ` +\n                    `${JSON.stringify(args.strides)}`);\n            }\n        }\n        assertPositiveInteger(this.strides, 'strides');\n        this.padding = args.padding == null ? 'valid' : args.padding;\n        checkPaddingMode(this.padding);\n        this.inputSpec = [new InputSpec({ ndim: 3 })];\n    }\n    computeOutputShape(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const length = convOutputLength(inputShape[1], this.poolSize[0], this.padding, this.strides[0]);\n        return [inputShape[0], length, inputShape[2]];\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            // Add dummy last dimension.\n            inputs = K.expandDims(getExactlyOneTensor(inputs), 2);\n            const output = this.poolingFunction(getExactlyOneTensor(inputs), [this.poolSize[0], 1], [this.strides[0], 1], this.padding, 'channelsLast');\n            // Remove dummy last dimension.\n            return tfc.squeeze(output, [2]);\n        });\n    }\n    getConfig() {\n        const config = {\n            poolSize: this.poolSize,\n            padding: this.padding,\n            strides: this.strides,\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\nexport class MaxPooling1D extends Pooling1D {\n    constructor(args) {\n        super(args);\n    }\n    poolingFunction(inputs, poolSize, strides, padding, dataFormat) {\n        checkDataFormat(dataFormat);\n        checkPaddingMode(padding);\n        return pool2d(inputs, poolSize, strides, padding, dataFormat, 'max');\n    }\n}\n/** @nocollapse */\nMaxPooling1D.className = 'MaxPooling1D';\nserialization.registerClass(MaxPooling1D);\nexport class AveragePooling1D extends Pooling1D {\n    constructor(args) {\n        super(args);\n    }\n    poolingFunction(inputs, poolSize, strides, padding, dataFormat) {\n        checkDataFormat(dataFormat);\n        checkPaddingMode(padding);\n        return pool2d(inputs, poolSize, strides, padding, dataFormat, 'avg');\n    }\n}\n/** @nocollapse */\nAveragePooling1D.className = 'AveragePooling1D';\nserialization.registerClass(AveragePooling1D);\n/**\n * Abstract class for different pooling 2D layers.\n */\nexport class Pooling2D extends Layer {\n    constructor(args) {\n        if (args.poolSize == null) {\n            args.poolSize = [2, 2];\n        }\n        super(args);\n        this.poolSize = Array.isArray(args.poolSize) ?\n            args.poolSize :\n            [args.poolSize, args.poolSize];\n        if (args.strides == null) {\n            this.strides = this.poolSize;\n        }\n        else if (Array.isArray(args.strides)) {\n            if (args.strides.length !== 2) {\n                throw new ValueError(`If the strides property of a 2D pooling layer is an Array, ` +\n                    `it is expected to have a length of 2, but received length ` +\n                    `${args.strides.length}.`);\n            }\n            this.strides = args.strides;\n        }\n        else {\n            // `config.strides` is a number.\n            this.strides = [args.strides, args.strides];\n        }\n        assertPositiveInteger(this.poolSize, 'poolSize');\n        assertPositiveInteger(this.strides, 'strides');\n        this.padding = args.padding == null ? 'valid' : args.padding;\n        this.dataFormat =\n            args.dataFormat == null ? 'channelsLast' : args.dataFormat;\n        checkDataFormat(this.dataFormat);\n        checkPaddingMode(this.padding);\n        this.inputSpec = [new InputSpec({ ndim: 4 })];\n    }\n    computeOutputShape(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        let rows = this.dataFormat === 'channelsFirst' ? inputShape[2] : inputShape[1];\n        let cols = this.dataFormat === 'channelsFirst' ? inputShape[3] : inputShape[2];\n        rows =\n            convOutputLength(rows, this.poolSize[0], this.padding, this.strides[0]);\n        cols =\n            convOutputLength(cols, this.poolSize[1], this.padding, this.strides[1]);\n        if (this.dataFormat === 'channelsFirst') {\n            return [inputShape[0], inputShape[1], rows, cols];\n        }\n        else {\n            return [inputShape[0], rows, cols, inputShape[3]];\n        }\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            return this.poolingFunction(getExactlyOneTensor(inputs), this.poolSize, this.strides, this.padding, this.dataFormat);\n        });\n    }\n    getConfig() {\n        const config = {\n            poolSize: this.poolSize,\n            padding: this.padding,\n            strides: this.strides,\n            dataFormat: this.dataFormat\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\nexport class MaxPooling2D extends Pooling2D {\n    constructor(args) {\n        super(args);\n    }\n    poolingFunction(inputs, poolSize, strides, padding, dataFormat) {\n        checkDataFormat(dataFormat);\n        checkPaddingMode(padding);\n        return pool2d(inputs, poolSize, strides, padding, dataFormat, 'max');\n    }\n}\n/** @nocollapse */\nMaxPooling2D.className = 'MaxPooling2D';\nserialization.registerClass(MaxPooling2D);\nexport class AveragePooling2D extends Pooling2D {\n    constructor(args) {\n        super(args);\n    }\n    poolingFunction(inputs, poolSize, strides, padding, dataFormat) {\n        checkDataFormat(dataFormat);\n        checkPaddingMode(padding);\n        return pool2d(inputs, poolSize, strides, padding, dataFormat, 'avg');\n    }\n}\n/** @nocollapse */\nAveragePooling2D.className = 'AveragePooling2D';\nserialization.registerClass(AveragePooling2D);\n/**\n * Abstract class for different pooling 3D layers.\n */\nexport class Pooling3D extends Layer {\n    constructor(args) {\n        if (args.poolSize == null) {\n            args.poolSize = [2, 2, 2];\n        }\n        super(args);\n        this.poolSize = Array.isArray(args.poolSize) ?\n            args.poolSize :\n            [args.poolSize, args.poolSize, args.poolSize];\n        if (args.strides == null) {\n            this.strides = this.poolSize;\n        }\n        else if (Array.isArray(args.strides)) {\n            if (args.strides.length !== 3) {\n                throw new ValueError(`If the strides property of a 3D pooling layer is an Array, ` +\n                    `it is expected to have a length of 3, but received length ` +\n                    `${args.strides.length}.`);\n            }\n            this.strides = args.strides;\n        }\n        else {\n            // `config.strides` is a number.\n            this.strides = [args.strides, args.strides, args.strides];\n        }\n        assertPositiveInteger(this.poolSize, 'poolSize');\n        assertPositiveInteger(this.strides, 'strides');\n        this.padding = args.padding == null ? 'valid' : args.padding;\n        this.dataFormat =\n            args.dataFormat == null ? 'channelsLast' : args.dataFormat;\n        checkDataFormat(this.dataFormat);\n        checkPaddingMode(this.padding);\n        this.inputSpec = [new InputSpec({ ndim: 5 })];\n    }\n    computeOutputShape(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        let depths = this.dataFormat === 'channelsFirst' ? inputShape[2] : inputShape[1];\n        let rows = this.dataFormat === 'channelsFirst' ? inputShape[3] : inputShape[2];\n        let cols = this.dataFormat === 'channelsFirst' ? inputShape[4] : inputShape[3];\n        depths = convOutputLength(depths, this.poolSize[0], this.padding, this.strides[0]);\n        rows =\n            convOutputLength(rows, this.poolSize[1], this.padding, this.strides[1]);\n        cols =\n            convOutputLength(cols, this.poolSize[2], this.padding, this.strides[2]);\n        if (this.dataFormat === 'channelsFirst') {\n            return [inputShape[0], inputShape[1], depths, rows, cols];\n        }\n        else {\n            return [inputShape[0], depths, rows, cols, inputShape[4]];\n        }\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            return this.poolingFunction(getExactlyOneTensor(inputs), this.poolSize, this.strides, this.padding, this.dataFormat);\n        });\n    }\n    getConfig() {\n        const config = {\n            poolSize: this.poolSize,\n            padding: this.padding,\n            strides: this.strides,\n            dataFormat: this.dataFormat\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\nexport class MaxPooling3D extends Pooling3D {\n    constructor(args) {\n        super(args);\n    }\n    poolingFunction(inputs, poolSize, strides, padding, dataFormat) {\n        checkDataFormat(dataFormat);\n        checkPaddingMode(padding);\n        return pool3d(inputs, poolSize, strides, padding, dataFormat, 'max');\n    }\n}\n/** @nocollapse */\nMaxPooling3D.className = 'MaxPooling3D';\nserialization.registerClass(MaxPooling3D);\nexport class AveragePooling3D extends Pooling3D {\n    constructor(args) {\n        super(args);\n    }\n    poolingFunction(inputs, poolSize, strides, padding, dataFormat) {\n        checkDataFormat(dataFormat);\n        checkPaddingMode(padding);\n        return pool3d(inputs, poolSize, strides, padding, dataFormat, 'avg');\n    }\n}\n/** @nocollapse */\nAveragePooling3D.className = 'AveragePooling3D';\nserialization.registerClass(AveragePooling3D);\n/**\n * Abstract class for different global pooling 1D layers.\n */\nexport class GlobalPooling1D extends Layer {\n    constructor(args) {\n        super(args);\n        this.inputSpec = [new InputSpec({ ndim: 3 })];\n    }\n    computeOutputShape(inputShape) {\n        return [inputShape[0], inputShape[2]];\n    }\n    call(inputs, kwargs) {\n        throw new NotImplementedError();\n    }\n}\nexport class GlobalAveragePooling1D extends GlobalPooling1D {\n    constructor(args) {\n        super(args || {});\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const input = getExactlyOneTensor(inputs);\n            return tfc.mean(input, 1);\n        });\n    }\n}\n/** @nocollapse */\nGlobalAveragePooling1D.className = 'GlobalAveragePooling1D';\nserialization.registerClass(GlobalAveragePooling1D);\nexport class GlobalMaxPooling1D extends GlobalPooling1D {\n    constructor(args) {\n        super(args || {});\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const input = getExactlyOneTensor(inputs);\n            return tfc.max(input, 1);\n        });\n    }\n}\n/** @nocollapse */\nGlobalMaxPooling1D.className = 'GlobalMaxPooling1D';\nserialization.registerClass(GlobalMaxPooling1D);\n/**\n * Abstract class for different global pooling 2D layers.\n */\nexport class GlobalPooling2D extends Layer {\n    constructor(args) {\n        super(args);\n        this.dataFormat =\n            args.dataFormat == null ? 'channelsLast' : args.dataFormat;\n        checkDataFormat(this.dataFormat);\n        this.inputSpec = [new InputSpec({ ndim: 4 })];\n    }\n    computeOutputShape(inputShape) {\n        inputShape = inputShape;\n        if (this.dataFormat === 'channelsLast') {\n            return [inputShape[0], inputShape[3]];\n        }\n        else {\n            return [inputShape[0], inputShape[1]];\n        }\n    }\n    call(inputs, kwargs) {\n        throw new NotImplementedError();\n    }\n    getConfig() {\n        const config = { dataFormat: this.dataFormat };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\nexport class GlobalAveragePooling2D extends GlobalPooling2D {\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const input = getExactlyOneTensor(inputs);\n            if (this.dataFormat === 'channelsLast') {\n                return tfc.mean(input, [1, 2]);\n            }\n            else {\n                return tfc.mean(input, [2, 3]);\n            }\n        });\n    }\n}\n/** @nocollapse */\nGlobalAveragePooling2D.className = 'GlobalAveragePooling2D';\nserialization.registerClass(GlobalAveragePooling2D);\nexport class GlobalMaxPooling2D extends GlobalPooling2D {\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const input = getExactlyOneTensor(inputs);\n            if (this.dataFormat === 'channelsLast') {\n                return tfc.max(input, [1, 2]);\n            }\n            else {\n                return tfc.max(input, [2, 3]);\n            }\n        });\n    }\n}\n/** @nocollapse */\nGlobalMaxPooling2D.className = 'GlobalMaxPooling2D';\nserialization.registerClass(GlobalMaxPooling2D);\n//# sourceMappingURL=pooling.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Merge Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { l2Normalize } from '../losses';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as mathUtils from '../utils/math_utils';\nimport { getExactlyOneShape } from '../utils/types_utils';\n/**\n * Generic Merge layer for element-wise merge functions.\n *\n * Used to implement `Sum`, `Average`, `Concatenate`, etc.\n */\nexport class Merge extends Layer {\n    constructor(args) {\n        super(args || {});\n        this.supportsMasking = true;\n    }\n    /**\n     * Logic for merging multiple tensors, to be overridden by subclasses.\n     * @param inputs\n     */\n    mergeFunction(inputs) {\n        throw new NotImplementedError();\n    }\n    /**\n     * Computes the shape of the result of an elementwise operation.\n     *\n     * @param shape1: Shape of the first tensor.\n     * @param shape2: Shape of the second tensor.\n     * @returns Expected output shape when an elementwise operation is carried\n     *   out on 2 tensors with shapes `shape1` and `shape2`.\n     * @throws ValueError: If `shape1` and `shape2` are not compatible for\n     *   element-wise operations.\n     */\n    computeElementwiseOpOutputShape(shape1, shape2) {\n        if (shape1 == null || shape2 == null) {\n            return null;\n        }\n        else if (shape1.length < shape2.length) {\n            return this.computeElementwiseOpOutputShape(shape2, shape1);\n        }\n        else if (shape2.length === 0) {\n            return shape1;\n        }\n        const outputShape = shape1.slice(0, shape1.length - shape2.length);\n        for (let k = 0; k < shape2.length; ++k) {\n            const i = shape1[shape1.length - shape2.length + k];\n            const j = shape2[k];\n            if (i == null || j == null || i < 0 || j < 0) {\n                outputShape.push(null);\n            }\n            else if (i === 1) {\n                outputShape.push(j);\n            }\n            else if (j === 1) {\n                outputShape.push(i);\n            }\n            else {\n                if (i !== j) {\n                    throw new ValueError('Operands could not be broadcast together with shapes ' +\n                        JSON.stringify(shape1) + ' ' + JSON.stringify(shape2));\n                }\n                outputShape.push(i);\n            }\n        }\n        return outputShape;\n    }\n    build(inputShape) {\n        // Used purely for shape validation.\n        if (Array.isArray(inputShape) && !Array.isArray(inputShape[0])) {\n            // Make sure that inputShape is an Array of shape.\n            inputShape = [getExactlyOneShape(inputShape)];\n        }\n        inputShape = inputShape;\n        if (inputShape.length < 2) {\n            throw new ValueError('A merge layer should be called on an Array of at least 2 inputs.' +\n                ` Got ${inputShape.length} input(s).`);\n        }\n        // Make sure that there is at most one unique batch size among the input\n        // shapes.\n        let batchSizes = [];\n        for (const shape of inputShape) {\n            if (shape != null && shape[0] !== null) {\n                batchSizes.push(shape[0]);\n            }\n        }\n        batchSizes = generic_utils.unique(batchSizes);\n        if (batchSizes.length > 1) {\n            throw new ValueError(`Can not merge tensors with different batch sizes. ` +\n                `Got tensors with shapes: ${JSON.stringify(inputShape)}.`);\n        }\n        let outputShape = inputShape[0] == null ? null : inputShape[0].slice(1);\n        for (let i = 1; i < inputShape.length; ++i) {\n            const shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n            outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n        }\n        // If the inputs have different ranks, we have to reshape them to make them\n        // broadcastable.\n        const allRanks = inputShape.map(shape => shape.length);\n        if (inputShape.indexOf(null) === -1 &&\n            generic_utils.unique(allRanks).length === 1) {\n            this.reshapeRequired = false;\n        }\n        else {\n            this.reshapeRequired = true;\n        }\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            inputs = inputs;\n            if (this.reshapeRequired) {\n                const reshapedInputs = [];\n                const inputDims = inputs.map(input => input.rank);\n                if (inputDims.indexOf(null) === -1) {\n                    // If ranks of all inputs are available, we simply expand each of them\n                    // at axis=1 until all of them have the same rank.\n                    const maxNDim = mathUtils.max(inputDims);\n                    for (let x of inputs) {\n                        const xNDim = x.rank;\n                        for (let k = 0; k < maxNDim - xNDim; ++k) {\n                            x = K.expandDims(x, 1);\n                        }\n                        reshapedInputs.push(x);\n                    }\n                    return this.mergeFunction(reshapedInputs);\n                }\n                else {\n                    // Transpose all inputs so that batch size is the last dimension.\n                    // [batchSize, dim1, dim2, ...] -> [dim1, dim2, ..., batchSize]\n                    let transposed = false;\n                    for (const x of inputs) {\n                        const xNDim = x.rank;\n                        if (xNDim == null) {\n                            const xShape = x.shape;\n                            const batchSize = xShape[0];\n                            const newShape = xShape.slice(1).concat([batchSize]);\n                            let xTransposed = x.reshape([batchSize].concat(mathUtils.arrayProd(xShape.slice(1))));\n                            xTransposed = tfc.transpose(xTransposed, [1, 0]);\n                            xTransposed = xTransposed.reshape(newShape);\n                            reshapedInputs.push(xTransposed);\n                            transposed = true;\n                        }\n                        else if (xNDim > 1) {\n                            const dims = mathUtils.range(1, xNDim).concat([0]);\n                            reshapedInputs.push(tfc.transpose(x, dims));\n                            transposed = true;\n                        }\n                        else {\n                            // We don't transpose inputs if they are 1D vectors or scalars.\n                            reshapedInputs.push(x);\n                        }\n                    }\n                    let y = this.mergeFunction(reshapedInputs);\n                    const yNDim = y.rank;\n                    if (transposed) {\n                        // If inputs have been transposed, we have to transpose the output\n                        // too.\n                        if (yNDim == null) {\n                            const yShape = y.shape;\n                            const yNDim = yShape.length;\n                            const batchSize = yShape[yNDim - 1];\n                            const newShape = [batchSize].concat(yShape.slice(0, yShape.length - 1));\n                            y = tfc.transpose(y.reshape([-1, batchSize]), [1, 0])\n                                .reshape(newShape);\n                        }\n                        else if (yNDim > 1) {\n                            const dims = [yNDim - 1].concat(mathUtils.range(0, yNDim - 1));\n                            y = tfc.transpose(y, dims);\n                        }\n                    }\n                    return y;\n                }\n            }\n            else {\n                return this.mergeFunction(inputs);\n            }\n        });\n    }\n    computeOutputShape(inputShape) {\n        inputShape = inputShape;\n        let outputShape;\n        if (inputShape[0] == null) {\n            outputShape = null;\n        }\n        else {\n            outputShape = inputShape[0].slice(1);\n        }\n        for (let i = 1; i < inputShape.length; ++i) {\n            const shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n            outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n        }\n        let batchSizes = [];\n        for (const shape of inputShape) {\n            if (shape != null && shape[0] !== null) {\n                batchSizes.push(shape[0]);\n            }\n        }\n        batchSizes = generic_utils.unique(batchSizes);\n        if (batchSizes.length === 1) {\n            outputShape = batchSizes.concat(outputShape);\n        }\n        else {\n            outputShape = [null].concat(outputShape);\n        }\n        return outputShape;\n    }\n    computeMask(inputs, mask) {\n        return tfc.tidy(() => {\n            if (mask == null) {\n                return null;\n            }\n            if (!Array.isArray(mask)) {\n                throw new ValueError('`mask` should be an Array');\n            }\n            if (!Array.isArray(inputs)) {\n                throw new ValueError('`inputs` should be an Array');\n            }\n            if (mask.length !== inputs.length) {\n                throw new ValueError(`The Array 'inputs' and 'mask' are expected to have the same ` +\n                    `length, but have different lengths ` +\n                    `(${inputs.length} vs ${mask.length})`);\n            }\n            if (mask.every(m => m == null)) {\n                return null;\n            }\n            mask = mask.map(m => m == null ? m : tfc.expandDims(m, 0));\n            let output = mask[0];\n            for (let i = 1; i < mask.length - 1; ++i) {\n                output = tfc.logicalAnd(output, mask[i]);\n            }\n            return output;\n        });\n    }\n}\nexport class Add extends Merge {\n    constructor(args) {\n        super(args);\n    }\n    mergeFunction(inputs) {\n        return tidy(() => {\n            let output = inputs[0].clone();\n            for (let i = 1; i < inputs.length; ++i) {\n                output = tfc.add(output, inputs[i]);\n            }\n            return output;\n        });\n    }\n}\n/** @nocollapse */\nAdd.className = 'Add';\nserialization.registerClass(Add);\n/**\n * Calculate the element-wise sum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Add` layer, by using no input argument\n *    or a single configuration argument. The resultant `Add` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const addLayer = tf.layers.add();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = addLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.add([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.add([input1, input2]).print();\n * // Gives [[11, 22], [33, 44]].\n *\n */\nexport function add(config) {\n    if (Array.isArray(config)) {\n        const layer = new Add({});\n        return layer.apply(config);\n    }\n    else {\n        return new Add(config);\n    }\n}\nexport class Multiply extends Merge {\n    constructor(args) {\n        super(args);\n    }\n    mergeFunction(inputs) {\n        return tidy(() => {\n            let output = inputs[0].clone();\n            for (let i = 1; i < inputs.length; ++i) {\n                output = tfc.mul(output, inputs[i]);\n            }\n            return output;\n        });\n    }\n}\n/** @nocollapse */\nMultiply.className = 'Multiply';\nserialization.registerClass(Multiply);\n/**\n * Calculate the element-wise product of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Multiply` layer, by using no input argument\n *    or a single configuration argument. The resultant `Multiply` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const multiplyLayer = tf.layers.multiply();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = multiplyLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.multiply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.multiply([input1, input2]).print();\n * // Gives [[10, 40], [90, 160]].\n *\n */\nexport function multiply(config) {\n    if (Array.isArray(config)) {\n        const layer = new Multiply({});\n        return layer.apply(config);\n    }\n    else {\n        return new Multiply(config);\n    }\n}\nexport class Average extends Merge {\n    constructor(args) {\n        super(args);\n    }\n    mergeFunction(inputs) {\n        return tidy(() => {\n            let output = inputs[0].clone();\n            for (let i = 1; i < inputs.length; ++i) {\n                output = tfc.add(output, inputs[i]);\n            }\n            return tfc.mul(1 / inputs.length, output);\n        });\n    }\n}\n/** @nocollapse */\nAverage.className = 'Average';\nserialization.registerClass(Average);\n/**\n * Calculate the element-wise arithmetic mean of inputs, which all have the same\n * shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Average` layer, by using no input argument\n *    or a single configuration argument. The resultant `Average` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const averageLayer = tf.layers.average();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = averageLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.average([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.average([input1, input2]).print();\n * // Gives [[5.5, 11], [16.5, 22]].\n *\n */\nexport function average(config) {\n    if (Array.isArray(config)) {\n        const layer = new Average({});\n        return layer.apply(config);\n    }\n    else {\n        return new Average(config);\n    }\n}\nexport class Maximum extends Merge {\n    constructor(args) {\n        super(args);\n    }\n    mergeFunction(inputs) {\n        return tidy(() => {\n            let output = inputs[0];\n            for (let i = 1; i < inputs.length; ++i) {\n                output = tfc.maximum(output, inputs[i]);\n            }\n            return output;\n        });\n    }\n}\n/** @nocollapse */\nMaximum.className = 'Maximum';\nserialization.registerClass(Maximum);\n/**\n * Calculate the element-wise maximum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Maximum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Maximum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const maximumLayer = tf.layers.maximum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = maximumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.maximum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.maximum([input1, input2]).print();\n * // Gives [[10, 20], [30, 40]].\n *\n */\nexport function maximum(config) {\n    if (Array.isArray(config)) {\n        const layer = new Maximum({});\n        return layer.apply(config);\n    }\n    else {\n        return new Maximum(config);\n    }\n}\nexport class Minimum extends Merge {\n    constructor(args) {\n        super(args);\n    }\n    mergeFunction(inputs) {\n        return tidy(() => {\n            let output = inputs[0];\n            for (let i = 1; i < inputs.length; ++i) {\n                output = tfc.minimum(output, inputs[i]);\n            }\n            return output;\n        });\n    }\n}\n/** @nocollapse */\nMinimum.className = 'Minimum';\nserialization.registerClass(Minimum);\n/**\n * Calculate the element-wise minimum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Minimum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Minimum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const minimumLayer = tf.layers.minimum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = minimumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.minimum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.minimum([input1, input2]).print();\n * // Gives [[1, 2], [3, 4]].\n *\n */\nexport function minimum(config) {\n    if (Array.isArray(config)) {\n        const layer = new Minimum({});\n        return layer.apply(config);\n    }\n    else {\n        return new Minimum(config);\n    }\n}\nexport class Concatenate extends Merge {\n    constructor(args) {\n        super(args);\n        this.DEFAULT_AXIS = -1;\n        if (args == null) {\n            args = {};\n        }\n        this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n        this.supportsMasking = true;\n        this.reshapeRequired = false;\n    }\n    build(inputShape) {\n        // Used purely for shape validation.]\n        if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0])) ||\n            inputShape.length === 1) {\n            throw new ValueError('A `Concatenate` layer should be called on a list of at least 2 ' +\n                'inputs');\n        }\n        inputShape = inputShape;\n        let allNoneShape = true;\n        for (const shape of inputShape) {\n            if (shape != null) {\n                allNoneShape = false;\n                break;\n            }\n        }\n        if (allNoneShape) {\n            return;\n        }\n        const shapeSet = [];\n        for (let i = 0; i < inputShape.length; ++i) {\n            const shapeWithoutConcatAxis = inputShape[i].slice();\n            shapeWithoutConcatAxis.splice(this.axis, 1);\n            let exists = false;\n            for (const shape of shapeSet) {\n                if (util.arraysEqual(shape, shapeWithoutConcatAxis)) {\n                    exists = true;\n                    break;\n                }\n            }\n            if (!exists) {\n                shapeSet.push(shapeWithoutConcatAxis);\n            }\n        }\n        if (shapeSet.length > 1) {\n            throw new ValueError('A `Concatenate` layer requires inputs with matching shapes ' +\n                'except for the concat axis. Got input shapes: ' +\n                JSON.stringify(inputShape));\n        }\n    }\n    mergeFunction(inputs) {\n        return tidy(() => {\n            return K.concatenate(inputs, this.axis);\n        });\n    }\n    computeOutputShape(inputShape) {\n        if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0]))) {\n            throw new ValueError('A `Concatenate` layer should be called on a list of inputs.');\n        }\n        const inputShapes = inputShape;\n        const outputShape = inputShapes[0].slice();\n        const axis = this.axis < 0 ? outputShape.length + this.axis : this.axis;\n        // Porting Note: the line above is because TypeScript doesn't support\n        //   negative indices.\n        for (const shape of inputShapes.slice(1)) {\n            if (outputShape[axis] == null || shape[axis] == null) {\n                outputShape[axis] = null;\n                break;\n            }\n            outputShape[axis] += shape[axis];\n        }\n        return outputShape;\n    }\n    computeMask(inputs, mask) {\n        if (mask == null) {\n            return null;\n        }\n        if (!Array.isArray(mask)) {\n            throw new ValueError('`mask` should be an array for Concatenate');\n        }\n        if (!Array.isArray(inputs)) {\n            throw new ValueError('`inputs` should be an array for Concatenate');\n        }\n        if (mask.length !== inputs.length) {\n            throw new ValueError(`Mismatch in the length of mask (${mask.length}) ` +\n                `and the legnth of inputs (${inputs.length})`);\n        }\n        return tfc.tidy(() => {\n            let allNullMasks = true;\n            mask.forEach(m => {\n                if (m != null) {\n                    allNullMasks = false;\n                    return;\n                }\n            });\n            if (allNullMasks) {\n                return null;\n            }\n            const outputMasks = [];\n            for (let i = 0; i < inputs.length; ++i) {\n                if (mask[i] == null) {\n                    // Input is unmasked. Append all 1's to masks.\n                    outputMasks.push(tfc.onesLike(inputs[i]).asType('bool'));\n                }\n                else if (mask[i].rank < inputs[i].rank) {\n                    // Mask is smaller than the input, expand it.\n                    outputMasks.push(tfc.expandDims(mask[i], -1));\n                }\n                else {\n                    outputMasks.push(mask[i]);\n                }\n            }\n            const concatenatedMasks = tfc.concat(outputMasks, this.axis);\n            return tfc.all(concatenatedMasks, -1, false);\n        });\n    }\n    getConfig() {\n        const config = {\n            'axis': this.axis,\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nConcatenate.className = 'Concatenate';\nserialization.registerClass(Concatenate);\n/**\n * Concatenate an `Array` of inputs.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Concatenate` layer, by using no input argument\n *    or a single configuration argument. The resultant `Concatenate` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const concatLayer = tf.layers.concatenate();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = concatLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 7], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = tf.layers.concatenate([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([[1, 2], [3, 4]], [2, 2]);\n * const input2 = tf.tensor2d([[10, 20], [30, 40]], [2, 2]);\n * tf.layers.concatenate([input1, input2]).print();\n * // Gives [[1, 2, 10, 20], [3, 4, 30, 40]].\n *\n */\nexport function concatenate(config) {\n    if (Array.isArray(config)) {\n        const layer = new Concatenate({});\n        return layer.apply(config);\n    }\n    else {\n        return new Concatenate(config);\n    }\n}\n/**\n * Interpretable potentially negative axis index.\n *\n * For example, given axis = -1, and dim = 3, this function will return 2.\n *\n * @param axis The axis index, may be a positive, zero or negative integer.\n * @param dim Total number of dimensions, a positive integer.\n * @returns A non-negative axis index equivalent to the input `axis`.\n */\nfunction interpretAxis(axis, dim) {\n    while (axis < 0) {\n        axis += dim;\n    }\n    return axis;\n}\nfunction batchDot(x, y, axes) {\n    if (x.shape.length > 3 || y.shape.length > 3) {\n        throw new NotImplementedError('batchDot is not implemented for tensors of 4D or higher rank yet');\n    }\n    tfc.util.assert(x.shape.length >= 2, () => `batchDot requires the rank of x to be >= 2, ` +\n        `but got ${x.shape.length}`);\n    tfc.util.assert(x.shape.length >= 2, () => `batchDot requires the rank of y to be >= 2, ` +\n        `but got ${y.shape.length}`);\n    if (typeof axes === 'number') {\n        axes = [axes, axes];\n    }\n    if (x.dtype === 'complex64' || y.dtype === 'complex64') {\n        throw new NotImplementedError('batchDot is not implemented for complex64-type Tensors yet.');\n    }\n    const xNDim = x.shape.length;\n    const yNDim = y.shape.length;\n    if (axes == null) {\n        // Behave like batchMatmul by default.\n        axes = [xNDim - 1, yNDim - 2];\n    }\n    const axesArray = axes;\n    return tfc.tidy(() => {\n        let diff;\n        if (xNDim > yNDim) {\n            diff = xNDim - yNDim;\n            const diffShape = [];\n            for (let i = 0; i < diff; ++i) {\n                diffShape.push(1);\n            }\n            y = y.reshape(y.shape.concat(diffShape));\n        }\n        else if (yNDim > xNDim) {\n            diff = yNDim - xNDim;\n            const diffShape = [];\n            for (let i = 0; i < diff; ++i) {\n                diffShape.push(1);\n            }\n            x = x.reshape(x.shape.concat(diffShape));\n        }\n        else {\n            diff = 0;\n        }\n        let out;\n        if (x.shape.length === 2 && y.shape.length === 2) {\n            if (axesArray[0] === axesArray[1]) {\n                out = x.mul(y).sum(axesArray[0]);\n            }\n            else {\n                out = x.transpose([1, 0]).mul(y).sum(axesArray[1]);\n            }\n        }\n        else {\n            const adjX = axesArray[0] !== x.shape.length - 1;\n            const adjY = axesArray[1] === y.shape.length - 1;\n            out = x.matMul(y, adjX, adjY);\n        }\n        if (diff > 0) {\n            let idx;\n            if (xNDim > yNDim) {\n                idx = xNDim + yNDim - 3;\n            }\n            else {\n                idx = xNDim - 1;\n            }\n            const squeezeAxes = [];\n            for (let i = idx; i < idx + diff; ++i) {\n                squeezeAxes.push(i);\n            }\n            out = out.squeeze(squeezeAxes);\n        }\n        if (out.shape.length === 1) {\n            out = out.expandDims(1);\n        }\n        return out;\n    });\n}\nexport class Dot extends Merge {\n    constructor(args) {\n        super(args);\n        this.axes = args.axes;\n        this.normalize = args.normalize == null ? false : args.normalize;\n        this.supportsMasking = true;\n        this.reshapeRequired = false;\n    }\n    build(inputShape) {\n        tfc.util.assert(Array.isArray(inputShape) && inputShape.length === 2 &&\n            Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), () => 'A `Dot` layer should be called on a list of exactly 2 inputs.');\n        const shape1 = inputShape[0];\n        const shape2 = inputShape[1];\n        if (shape1.length > 3 || shape2.length > 3) {\n            throw new NotImplementedError('Dot layer does not support tensors of 4D or higher rank yet.');\n        }\n        const axes = this.interpretAxes(shape1, shape2);\n        if (shape1[axes[0]] !== shape2[axes[1]]) {\n            throw new ValueError(`Dimension incompatibility: ` +\n                `${shape1[axes[0]]} !== ${shape2[axes[1]]}`);\n        }\n    }\n    mergeFunction(inputs) {\n        if (inputs.length !== 2) {\n            throw new ValueError('A `Dot` layer must be called on exactly 2 inputs, ' +\n                `but received ${inputs.length} input(s).`);\n        }\n        let x1 = inputs[0];\n        let x2 = inputs[1];\n        let axes;\n        if (!Array.isArray(this.axes)) {\n            axes = [\n                interpretAxis(this.axes, x1.shape.length),\n                interpretAxis(this.axes, x2.shape.length)\n            ];\n        }\n        else {\n            axes = this.axes.map((axis, i) => interpretAxis(axis, inputs[i].shape.length));\n        }\n        if (this.normalize) {\n            x1 = l2Normalize(x1, axes[0]);\n            x2 = l2Normalize(x2, axes[1]);\n        }\n        return batchDot(x1, x2, axes);\n    }\n    interpretAxes(shape1, shape2) {\n        let axes;\n        if (!Array.isArray(this.axes)) {\n            // `this.axes` is a single integer.\n            axes = [\n                interpretAxis(this.axes, shape1.length),\n                interpretAxis(this.axes, shape2.length)\n            ];\n        }\n        else {\n            // `this.axes` is an Array of integers.\n            axes = this.axes;\n        }\n        return axes;\n    }\n    computeOutputShape(inputShape) {\n        tfc.util.assert(Array.isArray(inputShape) && inputShape.length === 2 &&\n            Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), () => 'A `Dot` layer should be called on a list of exactly 2 inputs.');\n        const shape1 = inputShape[0].slice();\n        const shape2 = inputShape[1].slice();\n        if (shape1.length > 3 || shape2.length > 3) {\n            throw new NotImplementedError('Dot layer does not support tensors of 4D or higher rank yet.');\n        }\n        const axes = this.interpretAxes(shape1, shape2);\n        shape1.splice(axes[0], 1);\n        shape2.splice(axes[1], 1);\n        shape2.splice(0, 1);\n        const outputShape = shape1.concat(shape2);\n        if (outputShape.length === 1) {\n            outputShape.push(1);\n        }\n        return outputShape;\n    }\n    computeMask(inputs, mask) {\n        return null;\n    }\n    getConfig() {\n        const config = {\n            'axes': this.axes,\n            'normalize': this.normalize\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nDot.className = 'Dot';\nserialization.registerClass(Dot);\n// TODO(cais): Add functional interfaces for the merge layers.\n//# sourceMappingURL=merge.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/* Original Source layers/__init__.py */\nimport { serialization } from '@tensorflow/tfjs-core';\nimport { deserializeKerasObject } from '../utils/generic_utils';\n/**\n * Instantiate a layer from a config dictionary.\n * @param config dict of the form {class_name: str, config: dict}\n * @param customObjects dict mapping class names (or function names)\n *   of custom (non-Keras) objects to class/functions\n * @param fastWeightInit Optional flag to use fast weight initialization\n *   during deserialization. This is applicable to cases in which\n *   the initialization will be immediately overwritten by loaded weight\n *   values. Default: `false`.\n * @returns Layer instance (may be LayersModel, Sequential, Layer...)\n */\nexport function deserialize(config, customObjects = {}, fastWeightInit = false) {\n    return deserializeKerasObject(config, serialization.SerializationMap.getMap().classNameMap, customObjects, 'layer', fastWeightInit);\n}\n//# sourceMappingURL=serialization.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getActivation, serializeActivation } from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, SymbolicTensor } from '../engine/topology';\nimport { Layer } from '../engine/topology';\nimport { AttributeError, NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, Initializer, Ones, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { assertPositiveInteger } from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes } from '../utils/types_utils';\nimport { batchGetValue, batchSetValue } from '../variables';\nimport { deserialize } from './serialization';\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\nexport function standardizeArgs(inputs, initialState, constants, numConstants) {\n    if (Array.isArray(inputs)) {\n        if (initialState != null || constants != null) {\n            throw new ValueError('When inputs is an array, neither initialState or constants ' +\n                'should be provided');\n        }\n        if (numConstants != null) {\n            constants = inputs.slice(inputs.length - numConstants, inputs.length);\n            inputs = inputs.slice(0, inputs.length - numConstants);\n        }\n        if (inputs.length > 1) {\n            initialState = inputs.slice(1, inputs.length);\n        }\n        inputs = inputs[0];\n    }\n    function toListOrNull(x) {\n        if (x == null || Array.isArray(x)) {\n            return x;\n        }\n        else {\n            return [x];\n        }\n    }\n    initialState = toListOrNull(initialState);\n    constants = toListOrNull(constants);\n    return { inputs, initialState, constants };\n}\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\nexport function rnn(stepFunction, inputs, initialStates, goBackwards = false, mask, constants, unroll = false, needPerStepOutputs = false) {\n    return tfc.tidy(() => {\n        const ndim = inputs.shape.length;\n        if (ndim < 3) {\n            throw new ValueError(`Input should be at least 3D, but is ${ndim}D.`);\n        }\n        // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n        // ...].\n        const axes = [1, 0].concat(math_utils.range(2, ndim));\n        inputs = tfc.transpose(inputs, axes);\n        if (constants != null) {\n            throw new NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' +\n                'constants yet.');\n        }\n        // Porting Note: the unroll option is ignored by the imperative backend.\n        if (unroll) {\n            console.warn('Backend rnn(): the unroll = true option is not applicable to the ' +\n                'imperative deeplearn.js backend.');\n        }\n        if (mask != null) {\n            mask = mask.asType('bool').asType('float32');\n            if (mask.rank === ndim - 1) {\n                mask = tfc.expandDims(mask, -1);\n            }\n            mask = tfc.transpose(mask, axes);\n        }\n        if (goBackwards) {\n            inputs = tfc.reverse(inputs, 0);\n            if (mask != null) {\n                mask = tfc.reverse(mask, 0);\n            }\n        }\n        // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n        //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n        //   use the usual TypeScript control flow to iterate over the time steps in\n        //   the inputs.\n        // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n        // outputs.\n        //   This is not idiomatic in TypeScript. The info regarding whether we are\n        //   in a learning (i.e., training) phase for RNN is passed in a different\n        //   way.\n        const perStepOutputs = [];\n        let lastOutput;\n        let states = initialStates;\n        const timeSteps = inputs.shape[0];\n        const perStepInputs = tfc.unstack(inputs);\n        let perStepMasks;\n        if (mask != null) {\n            perStepMasks = tfc.unstack(mask);\n        }\n        for (let t = 0; t < timeSteps; ++t) {\n            const currentInput = perStepInputs[t];\n            const stepOutputs = tfc.tidy(() => stepFunction(currentInput, states));\n            if (mask == null) {\n                lastOutput = stepOutputs[0];\n                states = stepOutputs[1];\n            }\n            else {\n                const maskedOutputs = tfc.tidy(() => {\n                    const stepMask = perStepMasks[t];\n                    const negStepMask = tfc.onesLike(stepMask).sub(stepMask);\n                    // TODO(cais): Would tfc.where() be better for performance?\n                    const output = stepOutputs[0].mul(stepMask).add(states[0].mul(negStepMask));\n                    const newStates = states.map((state, i) => {\n                        return stepOutputs[1][i].mul(stepMask).add(state.mul(negStepMask));\n                    });\n                    return { output, newStates };\n                });\n                lastOutput = maskedOutputs.output;\n                states = maskedOutputs.newStates;\n            }\n            if (needPerStepOutputs) {\n                perStepOutputs.push(lastOutput);\n            }\n        }\n        let outputs;\n        if (needPerStepOutputs) {\n            const axis = 1;\n            outputs = tfc.stack(perStepOutputs, axis);\n        }\n        return [lastOutput, outputs, states];\n    });\n}\nexport class RNN extends Layer {\n    constructor(args) {\n        super(args);\n        let cell;\n        if (args.cell == null) {\n            throw new ValueError('cell property is missing for the constructor of RNN.');\n        }\n        else if (Array.isArray(args.cell)) {\n            cell = new StackedRNNCells({ cells: args.cell });\n        }\n        else {\n            cell = args.cell;\n        }\n        if (cell.stateSize == null) {\n            throw new ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' +\n                'integers, one integer per RNN state).');\n        }\n        this.cell = cell;\n        this.returnSequences =\n            args.returnSequences == null ? false : args.returnSequences;\n        this.returnState = args.returnState == null ? false : args.returnState;\n        this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n        this._stateful = args.stateful == null ? false : args.stateful;\n        this.unroll = args.unroll == null ? false : args.unroll;\n        this.supportsMasking = true;\n        this.inputSpec = [new InputSpec({ ndim: 3 })];\n        this.stateSpec = null;\n        this.states_ = null;\n        // TODO(cais): Add constantsSpec and numConstants.\n        this.numConstants = null;\n        // TODO(cais): Look into the use of initial_state in the kwargs of the\n        //   constructor.\n        this.keptStates = [];\n    }\n    // Porting Note: This is the equivalent of `RNN.states` property getter in\n    //   PyKeras.\n    getStates() {\n        if (this.states_ == null) {\n            const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n            return math_utils.range(0, numStates).map(x => null);\n        }\n        else {\n            return this.states_;\n        }\n    }\n    // Porting Note: This is the equivalent of the `RNN.states` property setter in\n    //   PyKeras.\n    setStates(states) {\n        this.states_ = states;\n    }\n    computeOutputShape(inputShape) {\n        if (isArrayOfShapes(inputShape)) {\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        // TODO(cais): Remove the casting once stacked RNN cells become supported.\n        let stateSize = this.cell.stateSize;\n        if (!Array.isArray(stateSize)) {\n            stateSize = [stateSize];\n        }\n        const outputDim = stateSize[0];\n        let outputShape;\n        if (this.returnSequences) {\n            outputShape = [inputShape[0], inputShape[1], outputDim];\n        }\n        else {\n            outputShape = [inputShape[0], outputDim];\n        }\n        if (this.returnState) {\n            const stateShape = [];\n            for (const dim of stateSize) {\n                stateShape.push([inputShape[0], dim]);\n            }\n            return [outputShape].concat(stateShape);\n        }\n        else {\n            return outputShape;\n        }\n    }\n    computeMask(inputs, mask) {\n        return tfc.tidy(() => {\n            if (Array.isArray(mask)) {\n                mask = mask[0];\n            }\n            const outputMask = this.returnSequences ? mask : null;\n            if (this.returnState) {\n                const stateMask = this.states.map(s => null);\n                return [outputMask].concat(stateMask);\n            }\n            else {\n                return outputMask;\n            }\n        });\n    }\n    /**\n     * Get the current state tensors of the RNN.\n     *\n     * If the state hasn't been set, return an array of `null`s of the correct\n     * length.\n     */\n    get states() {\n        if (this.states_ == null) {\n            const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n            const output = [];\n            for (let i = 0; i < numStates; ++i) {\n                output.push(null);\n            }\n            return output;\n        }\n        else {\n            return this.states_;\n        }\n    }\n    set states(s) {\n        this.states_ = s;\n    }\n    build(inputShape) {\n        // Note inputShape will be an Array of Shapes of initial states and\n        // constants if these are passed in apply().\n        const constantShape = null;\n        if (this.numConstants != null) {\n            throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n        }\n        if (isArrayOfShapes(inputShape)) {\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        const batchSize = this.stateful ? inputShape[0] : null;\n        const inputDim = inputShape.slice(2);\n        this.inputSpec[0] = new InputSpec({ shape: [batchSize, null, ...inputDim] });\n        // Allow cell (if RNNCell Layer) to build before we set or validate\n        // stateSpec.\n        const stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (constantShape != null) {\n            throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n        }\n        else {\n            this.cell.build(stepInputShape);\n        }\n        // Set or validate stateSpec.\n        let stateSize;\n        if (Array.isArray(this.cell.stateSize)) {\n            stateSize = this.cell.stateSize;\n        }\n        else {\n            stateSize = [this.cell.stateSize];\n        }\n        if (this.stateSpec != null) {\n            if (!util.arraysEqual(this.stateSpec.map(spec => spec.shape[spec.shape.length - 1]), stateSize)) {\n                throw new ValueError(`An initialState was passed that is not compatible with ` +\n                    `cell.stateSize. Received stateSpec=${this.stateSpec}; ` +\n                    `However cell.stateSize is ${this.cell.stateSize}`);\n            }\n        }\n        else {\n            this.stateSpec =\n                stateSize.map(dim => new InputSpec({ shape: [null, dim] }));\n        }\n        if (this.stateful) {\n            this.resetStates();\n        }\n    }\n    /**\n     * Reset the state tensors of the RNN.\n     *\n     * If the `states` argument is `undefined` or `null`, will set the\n     * state tensor(s) of the RNN to all-zero tensors of the appropriate\n     * shape(s).\n     *\n     * If `states` is provided, will set the state tensors of the RNN to its\n     * value.\n     *\n     * @param states Optional externally-provided initial states.\n     * @param training Whether this call is done during training. For stateful\n     *   RNNs, this affects whether the old states are kept or discarded. In\n     *   particular, if `training` is `true`, the old states will be kept so\n     *   that subsequent backpropgataion through time (BPTT) may work properly.\n     *   Else, the old states will be discarded.\n     */\n    resetStates(states, training = false) {\n        tidy(() => {\n            if (!this.stateful) {\n                throw new AttributeError('Cannot call resetStates() on an RNN Layer that is not stateful.');\n            }\n            const batchSize = this.inputSpec[0].shape[0];\n            if (batchSize == null) {\n                throw new ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' +\n                    'the batch size of your input tensors: \\n' +\n                    '- If using a Sequential model, specify the batch size by ' +\n                    'passing a `batchInputShape` option to your first layer.\\n' +\n                    '- If using the functional API, specify the batch size by ' +\n                    'passing a `batchShape` option to your Input layer.');\n            }\n            // Initialize state if null.\n            if (this.states_ == null) {\n                if (Array.isArray(this.cell.stateSize)) {\n                    this.states_ =\n                        this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n                }\n                else {\n                    this.states_ = [tfc.zeros([batchSize, this.cell.stateSize])];\n                }\n            }\n            else if (states == null) {\n                // Dispose old state tensors.\n                tfc.dispose(this.states_);\n                // For stateful RNNs, fully dispose kept old states.\n                if (this.keptStates != null) {\n                    tfc.dispose(this.keptStates);\n                    this.keptStates = [];\n                }\n                if (Array.isArray(this.cell.stateSize)) {\n                    this.states_ =\n                        this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n                }\n                else {\n                    this.states_[0] = tfc.zeros([batchSize, this.cell.stateSize]);\n                }\n            }\n            else {\n                if (!Array.isArray(states)) {\n                    states = [states];\n                }\n                if (states.length !== this.states_.length) {\n                    throw new ValueError(`Layer ${this.name} expects ${this.states_.length} state(s), ` +\n                        `but it received ${states.length} state value(s). Input ` +\n                        `received: ${states}`);\n                }\n                if (training === true) {\n                    // Store old state tensors for complete disposal later, i.e., during\n                    // the next no-arg call to this method. We do not dispose the old\n                    // states immediately because that BPTT (among other things) require\n                    // them.\n                    this.keptStates.push(this.states_.slice());\n                }\n                else {\n                    tfc.dispose(this.states_);\n                }\n                for (let index = 0; index < this.states_.length; ++index) {\n                    const value = states[index];\n                    const dim = Array.isArray(this.cell.stateSize) ?\n                        this.cell.stateSize[index] :\n                        this.cell.stateSize;\n                    const expectedShape = [batchSize, dim];\n                    if (!util.arraysEqual(value.shape, expectedShape)) {\n                        throw new ValueError(`State ${index} is incompatible with layer ${this.name}: ` +\n                            `expected shape=${expectedShape}, received shape=${value.shape}`);\n                    }\n                    this.states_[index] = value;\n                }\n            }\n            this.states_ = this.states_.map(state => tfc.keep(state.clone()));\n        });\n    }\n    apply(inputs, kwargs) {\n        // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n        let initialState = kwargs == null ? null : kwargs['initialState'];\n        let constants = kwargs == null ? null : kwargs['constants'];\n        if (kwargs == null) {\n            kwargs = {};\n        }\n        const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n        inputs = standardized.inputs;\n        initialState = standardized.initialState;\n        constants = standardized.constants;\n        // If any of `initial_state` or `constants` are specified and are\n        // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n        // the input_spec to include them.\n        let additionalInputs = [];\n        let additionalSpecs = [];\n        if (initialState != null) {\n            kwargs['initialState'] = initialState;\n            additionalInputs = additionalInputs.concat(initialState);\n            this.stateSpec = [];\n            for (const state of initialState) {\n                this.stateSpec.push(new InputSpec({ shape: state.shape }));\n            }\n            // TODO(cais): Use the following instead.\n            // this.stateSpec = initialState.map(state => new InputSpec({shape:\n            // state.shape}));\n            additionalSpecs = additionalSpecs.concat(this.stateSpec);\n        }\n        if (constants != null) {\n            kwargs['constants'] = constants;\n            additionalInputs = additionalInputs.concat(constants);\n            // TODO(cais): Add this.constantsSpec.\n            this.numConstants = constants.length;\n        }\n        const isTensor = additionalInputs[0] instanceof SymbolicTensor;\n        if (isTensor) {\n            // Compute full input spec, including state and constants.\n            const fullInput = [inputs].concat(additionalInputs);\n            const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n            // Perform the call with temporarily replaced inputSpec.\n            const originalInputSpec = this.inputSpec;\n            this.inputSpec = fullInputSpec;\n            const output = super.apply(fullInput, kwargs);\n            this.inputSpec = originalInputSpec;\n            return output;\n        }\n        else {\n            return super.apply(inputs, kwargs);\n        }\n    }\n    // tslint:disable-next-line:no-any\n    call(inputs, kwargs) {\n        // Input shape: `[samples, time (padded with zeros), input_dim]`.\n        // Note that the .build() method of subclasses **must** define\n        // this.inputSpec and this.stateSpec owith complete input shapes.\n        return tidy(() => {\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            let initialState = kwargs == null ? null : kwargs['initialState'];\n            inputs = getExactlyOneTensor(inputs);\n            if (initialState == null) {\n                if (this.stateful) {\n                    initialState = this.states_;\n                }\n                else {\n                    initialState = this.getInitialState(inputs);\n                }\n            }\n            const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n            if (initialState.length !== numStates) {\n                throw new ValueError(`RNN Layer has ${numStates} state(s) but was passed ` +\n                    `${initialState.length} initial state(s).`);\n            }\n            if (this.unroll) {\n                console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n            }\n            const cellCallKwargs = { training };\n            // TODO(cais): Add support for constants.\n            const step = (inputs, states) => {\n                // `inputs` and `states` are concatenated to form a single `Array` of\n                // `tf.Tensor`s as the input to `cell.call()`.\n                const outputs = this.cell.call([inputs].concat(states), cellCallKwargs);\n                // Marshall the return value into output and new states.\n                return [outputs[0], outputs.slice(1)];\n            };\n            // TODO(cais): Add support for constants.\n            const rnnOutputs = rnn(step, inputs, initialState, this.goBackwards, mask, null, this.unroll, this.returnSequences);\n            const lastOutput = rnnOutputs[0];\n            const outputs = rnnOutputs[1];\n            const states = rnnOutputs[2];\n            if (this.stateful) {\n                this.resetStates(states, training);\n            }\n            const output = this.returnSequences ? outputs : lastOutput;\n            // TODO(cais): Porperty set learning phase flag.\n            if (this.returnState) {\n                return [output].concat(states);\n            }\n            else {\n                return output;\n            }\n        });\n    }\n    getInitialState(inputs) {\n        return tidy(() => {\n            // Build an all-zero tensor of shape [samples, outputDim].\n            // [Samples, timeSteps, inputDim].\n            let initialState = tfc.zeros(inputs.shape);\n            // [Samples].\n            initialState = tfc.sum(initialState, [1, 2]);\n            initialState = K.expandDims(initialState); // [Samples, 1].\n            if (Array.isArray(this.cell.stateSize)) {\n                return this.cell.stateSize.map(dim => dim > 1 ? K.tile(initialState, [1, dim]) : initialState);\n            }\n            else {\n                return this.cell.stateSize > 1 ?\n                    [K.tile(initialState, [1, this.cell.stateSize])] :\n                    [initialState];\n            }\n        });\n    }\n    get trainableWeights() {\n        if (!this.trainable) {\n            return [];\n        }\n        // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n        return this.cell.trainableWeights;\n    }\n    get nonTrainableWeights() {\n        // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n        if (!this.trainable) {\n            return this.cell.weights;\n        }\n        return this.cell.nonTrainableWeights;\n    }\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.cell != null) {\n            this.cell.setFastWeightInitDuringBuild(value);\n        }\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            returnSequences: this.returnSequences,\n            returnState: this.returnState,\n            goBackwards: this.goBackwards,\n            stateful: this.stateful,\n            unroll: this.unroll,\n        };\n        if (this.numConstants != null) {\n            config['numConstants'] = this.numConstants;\n        }\n        const cellConfig = this.cell.getConfig();\n        if (this.getClassName() === RNN.className) {\n            config['cell'] = {\n                'className': this.cell.getClassName(),\n                'config': cellConfig,\n            };\n        }\n        // this order is necessary, to prevent cell name from replacing layer name\n        return Object.assign({}, cellConfig, baseConfig, config);\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const cellConfig = config['cell'];\n        const cell = deserialize(cellConfig, customObjects);\n        return new cls(Object.assign(config, { cell }));\n    }\n}\n/** @nocollapse */\nRNN.className = 'RNN';\nserialization.registerClass(RNN);\n// Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n/**\n * An RNNCell layer.\n *\n * @doc {heading: 'Layers', subheading: 'Classes'}\n */\nexport class RNNCell extends Layer {\n}\nexport class SimpleRNNCell extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.DEFAULT_ACTIVATION = 'tanh';\n        this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        this.units = args.units;\n        assertPositiveInteger(this.units, `units`);\n        this.activation = getActivation(args.activation == null ? this.DEFAULT_ACTIVATION : args.activation);\n        this.useBias = args.useBias == null ? true : args.useBias;\n        this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n        this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n        this.biasInitializer =\n            getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n        this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n        this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n        this.biasRegularizer = getRegularizer(args.biasRegularizer);\n        this.kernelConstraint = getConstraint(args.kernelConstraint);\n        this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n        this.biasConstraint = getConstraint(args.biasConstraint);\n        this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n        this.recurrentDropout = math_utils.min([\n            1,\n            math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n        ]);\n        this.stateSize = this.units;\n        this.dropoutMask = null;\n        this.recurrentDropoutMask = null;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        // TODO(cais): Use regularizer.\n        this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        if (this.useBias) {\n            this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        }\n        else {\n            this.bias = null;\n        }\n        this.built = true;\n    }\n    // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n    //   `inputs` and `states`. Here, the two tensors are combined into an\n    //   `Tensor[]` Array as the first input argument.\n    //   Similarly, PyKeras' equivalent of this method returns two values:\n    //    `output` and `[output]`. Here the two are combined into one length-2\n    //    `Tensor[]`, consisting of `output` repeated.\n    call(inputs, kwargs) {\n        return tidy(() => {\n            inputs = inputs;\n            if (inputs.length !== 2) {\n                throw new ValueError(`SimpleRNNCell expects 2 input Tensors, got ${inputs.length}.`);\n            }\n            let prevOutput = inputs[1];\n            inputs = inputs[0];\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n                this.dropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(inputs),\n                    rate: this.dropout,\n                    training\n                });\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n                this.recurrentDropoutMask == null) {\n                this.recurrentDropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(prevOutput),\n                    rate: this.recurrentDropout,\n                    training\n                });\n            }\n            let h;\n            const dpMask = this.dropoutMask;\n            const recDpMask = this.recurrentDropoutMask;\n            if (dpMask != null) {\n                h = K.dot(tfc.mul(inputs, dpMask), this.kernel.read());\n            }\n            else {\n                h = K.dot(inputs, this.kernel.read());\n            }\n            if (this.bias != null) {\n                h = K.biasAdd(h, this.bias.read());\n            }\n            if (recDpMask != null) {\n                prevOutput = tfc.mul(prevOutput, recDpMask);\n            }\n            let output = tfc.add(h, K.dot(prevOutput, this.recurrentKernel.read()));\n            if (this.activation != null) {\n                output = this.activation.apply(output);\n            }\n            // TODO(cais): Properly set learning phase on output tensor?\n            return [output, output];\n        });\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            units: this.units,\n            activation: serializeActivation(this.activation),\n            useBias: this.useBias,\n            kernelInitializer: serializeInitializer(this.kernelInitializer),\n            recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n            biasInitializer: serializeInitializer(this.biasInitializer),\n            kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n            recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n            biasRegularizer: serializeRegularizer(this.biasRegularizer),\n            activityRegularizer: serializeRegularizer(this.activityRegularizer),\n            kernelConstraint: serializeConstraint(this.kernelConstraint),\n            recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n            biasConstraint: serializeConstraint(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n        };\n        return Object.assign({}, baseConfig, config);\n    }\n}\n/** @nocollapse */\nSimpleRNNCell.className = 'SimpleRNNCell';\nserialization.registerClass(SimpleRNNCell);\nexport class SimpleRNN extends RNN {\n    constructor(args) {\n        args.cell = new SimpleRNNCell(args);\n        super(args);\n        // TODO(cais): Add activityRegularizer.\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.cell.dropoutMask != null) {\n                tfc.dispose(this.cell.dropoutMask);\n                this.cell.dropoutMask = null;\n            }\n            if (this.cell.recurrentDropoutMask != null) {\n                tfc.dispose(this.cell.recurrentDropoutMask);\n                this.cell.recurrentDropoutMask = null;\n            }\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            const initialState = kwargs == null ? null : kwargs['initialState'];\n            return super.call(inputs, { mask, training, initialState });\n        });\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config);\n    }\n}\n/** @nocollapse */\nSimpleRNN.className = 'SimpleRNN';\nserialization.registerClass(SimpleRNN);\nexport class GRUCell extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.DEFAULT_ACTIVATION = 'tanh';\n        this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n        this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        if (args.resetAfter) {\n            throw new ValueError(`GRUCell does not support reset_after parameter set to true.`);\n        }\n        this.units = args.units;\n        assertPositiveInteger(this.units, 'units');\n        this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION :\n            args.activation);\n        this.recurrentActivation = getActivation(args.recurrentActivation === undefined ?\n            this.DEFAULT_RECURRENT_ACTIVATION :\n            args.recurrentActivation);\n        this.useBias = args.useBias == null ? true : args.useBias;\n        this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n        this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n        this.biasInitializer =\n            getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n        this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n        this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n        this.biasRegularizer = getRegularizer(args.biasRegularizer);\n        this.kernelConstraint = getConstraint(args.kernelConstraint);\n        this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n        this.biasConstraint = getConstraint(args.biasConstraint);\n        this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n        this.recurrentDropout = math_utils.min([\n            1,\n            math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n        ]);\n        this.implementation = args.implementation;\n        this.stateSize = this.units;\n        this.dropoutMask = null;\n        this.recurrentDropoutMask = null;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const inputDim = inputShape[inputShape.length - 1];\n        this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        if (this.useBias) {\n            this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        }\n        else {\n            this.bias = null;\n        }\n        // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n        //   of the weights and bias in the call() method, at execution time.\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            inputs = inputs;\n            if (inputs.length !== 2) {\n                throw new ValueError(`GRUCell expects 2 input Tensors (inputs, h, c), got ` +\n                    `${inputs.length}.`);\n            }\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            let hTMinus1 = inputs[1]; // Previous memory state.\n            inputs = inputs[0];\n            // Note: For superior performance, TensorFlow.js always uses\n            // implementation 2, regardless of the actual value of\n            // config.implementation.\n            if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n                this.dropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(inputs),\n                    rate: this.dropout,\n                    training,\n                    count: 3\n                });\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n                this.recurrentDropoutMask == null) {\n                this.recurrentDropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(hTMinus1),\n                    rate: this.recurrentDropout,\n                    training,\n                    count: 3\n                });\n            }\n            const dpMask = this.dropoutMask;\n            const recDpMask = this.recurrentDropoutMask;\n            let z;\n            let r;\n            let hh;\n            if (0 < this.dropout && this.dropout < 1) {\n                inputs = tfc.mul(inputs, dpMask[0]);\n            }\n            let matrixX = K.dot(inputs, this.kernel.read());\n            if (this.useBias) {\n                matrixX = K.biasAdd(matrixX, this.bias.read());\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n                hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n            }\n            const recurrentKernelValue = this.recurrentKernel.read();\n            const [rk1, rk2] = tfc.split(recurrentKernelValue, [2 * this.units, this.units], recurrentKernelValue.rank - 1);\n            const matrixInner = K.dot(hTMinus1, rk1);\n            const [xZ, xR, xH] = tfc.split(matrixX, 3, matrixX.rank - 1);\n            const [recurrentZ, recurrentR] = tfc.split(matrixInner, 2, matrixInner.rank - 1);\n            z = this.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n            r = this.recurrentActivation.apply(tfc.add(xR, recurrentR));\n            const recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n            hh = this.activation.apply(tfc.add(xH, recurrentH));\n            const h = tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh));\n            // TODO(cais): Add use_learning_phase flag properly.\n            return [h, h];\n        });\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            units: this.units,\n            activation: serializeActivation(this.activation),\n            recurrentActivation: serializeActivation(this.recurrentActivation),\n            useBias: this.useBias,\n            kernelInitializer: serializeInitializer(this.kernelInitializer),\n            recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n            biasInitializer: serializeInitializer(this.biasInitializer),\n            kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n            recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n            biasRegularizer: serializeRegularizer(this.biasRegularizer),\n            activityRegularizer: serializeRegularizer(this.activityRegularizer),\n            kernelConstraint: serializeConstraint(this.kernelConstraint),\n            recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n            biasConstraint: serializeConstraint(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n            implementation: this.implementation,\n            resetAfter: false\n        };\n        return Object.assign({}, baseConfig, config);\n    }\n}\n/** @nocollapse */\nGRUCell.className = 'GRUCell';\nserialization.registerClass(GRUCell);\nexport class GRU extends RNN {\n    constructor(args) {\n        if (args.implementation === 0) {\n            console.warn('`implementation=0` has been deprecated, and now defaults to ' +\n                '`implementation=1`. Please update your layer call.');\n        }\n        args.cell = new GRUCell(args);\n        super(args);\n        // TODO(cais): Add activityRegularizer.\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.cell.dropoutMask != null) {\n                tfc.dispose(this.cell.dropoutMask);\n                this.cell.dropoutMask = null;\n            }\n            if (this.cell.recurrentDropoutMask != null) {\n                tfc.dispose(this.cell.recurrentDropoutMask);\n                this.cell.recurrentDropoutMask = null;\n            }\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            const initialState = kwargs == null ? null : kwargs['initialState'];\n            return super.call(inputs, { mask, training, initialState });\n        });\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        if (config['implmentation'] === 0) {\n            config['implementation'] = 1;\n        }\n        return new cls(config);\n    }\n}\n/** @nocollapse */\nGRU.className = 'GRU';\nserialization.registerClass(GRU);\nexport class LSTMCell extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.DEFAULT_ACTIVATION = 'tanh';\n        this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n        this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        this.units = args.units;\n        assertPositiveInteger(this.units, 'units');\n        this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION :\n            args.activation);\n        this.recurrentActivation = getActivation(args.recurrentActivation === undefined ?\n            this.DEFAULT_RECURRENT_ACTIVATION :\n            args.recurrentActivation);\n        this.useBias = args.useBias == null ? true : args.useBias;\n        this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n        this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n        this.biasInitializer =\n            getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n        this.unitForgetBias = args.unitForgetBias;\n        this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n        this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n        this.biasRegularizer = getRegularizer(args.biasRegularizer);\n        this.kernelConstraint = getConstraint(args.kernelConstraint);\n        this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n        this.biasConstraint = getConstraint(args.biasConstraint);\n        this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n        this.recurrentDropout = math_utils.min([\n            1,\n            math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n        ]);\n        this.implementation = args.implementation;\n        this.stateSize = [this.units, this.units];\n        this.dropoutMask = null;\n        this.recurrentDropoutMask = null;\n    }\n    build(inputShape) {\n        var _a;\n        inputShape = getExactlyOneShape(inputShape);\n        const inputDim = inputShape[inputShape.length - 1];\n        this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        let biasInitializer;\n        if (this.useBias) {\n            if (this.unitForgetBias) {\n                const capturedBiasInit = this.biasInitializer;\n                const capturedUnits = this.units;\n                biasInitializer = new (_a = class CustomInit extends Initializer {\n                        apply(shape, dtype) {\n                            // TODO(cais): More informative variable names?\n                            const bI = capturedBiasInit.apply([capturedUnits]);\n                            const bF = (new Ones()).apply([capturedUnits]);\n                            const bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n                            return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n                        }\n                    },\n                    /** @nocollapse */\n                    _a.className = 'CustomInit',\n                    _a)();\n            }\n            else {\n                biasInitializer = this.biasInitializer;\n            }\n            this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        }\n        else {\n            this.bias = null;\n        }\n        // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n        //   of the weights and bias in the call() method, at execution time.\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            inputs = inputs;\n            if (inputs.length !== 3) {\n                throw new ValueError(`LSTMCell expects 3 input Tensors (inputs, h, c), got ` +\n                    `${inputs.length}.`);\n            }\n            let hTMinus1 = inputs[1]; // Previous memory state.\n            const cTMinus1 = inputs[2]; // Previous carry state.\n            inputs = inputs[0];\n            if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n                this.dropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(inputs),\n                    rate: this.dropout,\n                    training,\n                    count: 4\n                });\n            }\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n                this.recurrentDropoutMask == null) {\n                this.recurrentDropoutMask = generateDropoutMask({\n                    ones: () => tfc.onesLike(hTMinus1),\n                    rate: this.recurrentDropout,\n                    training,\n                    count: 4\n                });\n            }\n            const dpMask = this.dropoutMask;\n            const recDpMask = this.recurrentDropoutMask;\n            // Note: For superior performance, TensorFlow.js always uses\n            // implementation 2 regardless of the actual value of\n            // config.implementation.\n            let i;\n            let f;\n            let c;\n            let o;\n            if (0 < this.dropout && this.dropout < 1) {\n                inputs = tfc.mul(inputs, dpMask[0]);\n            }\n            let z = K.dot(inputs, this.kernel.read());\n            if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n                hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n            }\n            z = tfc.add(z, K.dot(hTMinus1, this.recurrentKernel.read()));\n            if (this.useBias) {\n                z = K.biasAdd(z, this.bias.read());\n            }\n            const [z0, z1, z2, z3] = tfc.split(z, 4, z.rank - 1);\n            i = this.recurrentActivation.apply(z0);\n            f = this.recurrentActivation.apply(z1);\n            c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, this.activation.apply(z2)));\n            o = this.recurrentActivation.apply(z3);\n            const h = tfc.mul(o, this.activation.apply(c));\n            // TODO(cais): Add use_learning_phase flag properly.\n            return [h, h, c];\n        });\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = {\n            units: this.units,\n            activation: serializeActivation(this.activation),\n            recurrentActivation: serializeActivation(this.recurrentActivation),\n            useBias: this.useBias,\n            kernelInitializer: serializeInitializer(this.kernelInitializer),\n            recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n            biasInitializer: serializeInitializer(this.biasInitializer),\n            unitForgetBias: this.unitForgetBias,\n            kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n            recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n            biasRegularizer: serializeRegularizer(this.biasRegularizer),\n            activityRegularizer: serializeRegularizer(this.activityRegularizer),\n            kernelConstraint: serializeConstraint(this.kernelConstraint),\n            recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n            biasConstraint: serializeConstraint(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n            implementation: this.implementation,\n        };\n        return Object.assign({}, baseConfig, config);\n    }\n}\n/** @nocollapse */\nLSTMCell.className = 'LSTMCell';\nserialization.registerClass(LSTMCell);\nexport class LSTM extends RNN {\n    constructor(args) {\n        if (args.implementation === 0) {\n            console.warn('`implementation=0` has been deprecated, and now defaults to ' +\n                '`implementation=1`. Please update your layer call.');\n        }\n        args.cell = new LSTMCell(args);\n        super(args);\n        // TODO(cais): Add activityRegularizer.\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.cell.dropoutMask != null) {\n                tfc.dispose(this.cell.dropoutMask);\n                this.cell.dropoutMask = null;\n            }\n            if (this.cell.recurrentDropoutMask != null) {\n                tfc.dispose(this.cell.recurrentDropoutMask);\n                this.cell.recurrentDropoutMask = null;\n            }\n            const mask = kwargs == null ? null : kwargs['mask'];\n            const training = kwargs == null ? null : kwargs['training'];\n            const initialState = kwargs == null ? null : kwargs['initialState'];\n            return super.call(inputs, { mask, training, initialState });\n        });\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        if (config['implmentation'] === 0) {\n            config['implementation'] = 1;\n        }\n        return new cls(config);\n    }\n}\n/** @nocollapse */\nLSTM.className = 'LSTM';\nserialization.registerClass(LSTM);\nexport class StackedRNNCells extends RNNCell {\n    constructor(args) {\n        super(args);\n        this.cells = args.cells;\n    }\n    get stateSize() {\n        // States are a flat list in reverse order of the cell stack.\n        // This allows perserving the requirement `stack.statesize[0] ===\n        // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n        // assuming one LSTM has states `[h, c]`.\n        const stateSize = [];\n        for (const cell of this.cells.slice().reverse()) {\n            if (Array.isArray(cell.stateSize)) {\n                stateSize.push(...cell.stateSize);\n            }\n            else {\n                stateSize.push(cell.stateSize);\n            }\n        }\n        return stateSize;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            inputs = inputs;\n            let states = inputs.slice(1);\n            // Recover per-cell states.\n            const nestedStates = [];\n            for (const cell of this.cells.slice().reverse()) {\n                if (Array.isArray(cell.stateSize)) {\n                    nestedStates.push(states.splice(0, cell.stateSize.length));\n                }\n                else {\n                    nestedStates.push(states.splice(0, 1));\n                }\n            }\n            nestedStates.reverse();\n            // Call the cells in order and store the returned states.\n            const newNestedStates = [];\n            let callInputs;\n            for (let i = 0; i < this.cells.length; ++i) {\n                const cell = this.cells[i];\n                states = nestedStates[i];\n                // TODO(cais): Take care of constants.\n                if (i === 0) {\n                    callInputs = [inputs[0]].concat(states);\n                }\n                else {\n                    callInputs = [callInputs[0]].concat(states);\n                }\n                callInputs = cell.call(callInputs, kwargs);\n                newNestedStates.push(callInputs.slice(1));\n            }\n            // Format the new states as a flat list in reverse cell order.\n            states = [];\n            for (const cellStates of newNestedStates.slice().reverse()) {\n                states.push(...cellStates);\n            }\n            return [callInputs[0]].concat(states);\n        });\n    }\n    build(inputShape) {\n        if (isArrayOfShapes(inputShape)) {\n            // TODO(cais): Take care of input constants.\n            // const constantShape = inputShape.slice(1);\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        let outputDim;\n        this.cells.forEach((cell, i) => {\n            nameScope(`RNNCell_${i}`, () => {\n                // TODO(cais): Take care of input constants.\n                cell.build(inputShape);\n                if (Array.isArray(cell.stateSize)) {\n                    outputDim = cell.stateSize[0];\n                }\n                else {\n                    outputDim = cell.stateSize;\n                }\n                inputShape = [inputShape[0], outputDim];\n            });\n        });\n        this.built = true;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const getCellConfig = (cell) => {\n            return {\n                'className': cell.getClassName(),\n                'config': cell.getConfig(),\n            };\n        };\n        const cellConfigs = this.cells.map(getCellConfig);\n        const config = { 'cells': cellConfigs };\n        return Object.assign({}, baseConfig, config);\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const cells = [];\n        for (const cellConfig of config['cells']) {\n            cells.push(deserialize(cellConfig, customObjects));\n        }\n        return new cls({ cells });\n    }\n    get trainableWeights() {\n        if (!this.trainable) {\n            return [];\n        }\n        const weights = [];\n        for (const cell of this.cells) {\n            weights.push(...cell.trainableWeights);\n        }\n        return weights;\n    }\n    get nonTrainableWeights() {\n        const weights = [];\n        for (const cell of this.cells) {\n            weights.push(...cell.nonTrainableWeights);\n        }\n        if (!this.trainable) {\n            const trainableWeights = [];\n            for (const cell of this.cells) {\n                trainableWeights.push(...cell.trainableWeights);\n            }\n            return trainableWeights.concat(weights);\n        }\n        return weights;\n    }\n    /**\n     * Retrieve the weights of a the model.\n     *\n     * @returns A flat `Array` of `tf.Tensor`s.\n     */\n    getWeights() {\n        const weights = [];\n        for (const cell of this.cells) {\n            weights.push(...cell.weights);\n        }\n        return batchGetValue(weights);\n    }\n    /**\n     * Set the weights of the model.\n     *\n     * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n     *     the output of `getWeights()`.\n     */\n    setWeights(weights) {\n        const tuples = [];\n        for (const cell of this.cells) {\n            const numParams = cell.weights.length;\n            const inputWeights = weights.splice(numParams);\n            for (let i = 0; i < cell.weights.length; ++i) {\n                tuples.push([cell.weights[i], inputWeights[i]]);\n            }\n        }\n        batchSetValue(tuples);\n    }\n}\n/** @nocollapse */\nStackedRNNCells.className = 'StackedRNNCells';\nserialization.registerClass(StackedRNNCells);\nexport function generateDropoutMask(args) {\n    const { ones, rate, training = false, count = 1 } = args;\n    const droppedInputs = () => K.dropout(ones(), rate);\n    const createMask = () => K.inTrainPhase(droppedInputs, ones, training);\n    // just in case count is provided with null or undefined\n    if (!count || count <= 1) {\n        return tfc.keep(createMask().clone());\n    }\n    const masks = Array(count).fill(undefined).map(createMask);\n    return masks.map(m => tfc.keep(m.clone()));\n}\n//# sourceMappingURL=recurrent.js.map","/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Padding Layers.\n */\n// Porting Note: In Python Keras, the padding layers are in convolutional.py,\n//   but we decided to put them in a separate file (padding.ts) for clarity.\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport { imageDataFormat } from '../backend/common';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { ValueError } from '../errors';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Pads the middle dimension of a 3D tensor.\n *\n * @param x Input `tf.Tensor` to be padded.\n * @param padding `Array` of 2 integers, how many zeros to add at the start and\n *   end of the middle dimension (i.e., dimension 1).\n * @return A padded 3D `tf.Tensor`.\n */\nexport function temporalPadding(x, padding) {\n    return tidy(() => {\n        if (x.rank !== 3) {\n            throw new ValueError(`temporalPadding expects input tensor to be 3-D, but received a ` +\n                `${x.rank}-D tensor.`);\n        }\n        if (padding == null) {\n            padding = [1, 1];\n        }\n        if (padding.length !== 2) {\n            throw new ValueError(`temporalPadding expects input padding pattern to be a length-2 ` +\n                `array, but received a length-${padding.length} array.`);\n        }\n        const pattern = [[0, 0], padding, [0, 0]];\n        return tfc.pad(x, pattern);\n    });\n}\n/**\n * Pads the 2nd and 3rd dimensions of a 4D tensor.\n *\n * @param x Input `tf.Tensor` to be padded.\n * @param padding `Array` of two `Array`s, each of which is an `Array` of two\n *   integers. The amount of padding at the beginning and end of the 2nd and 3rd\n *   dimensions, respectively.\n * @param dataFormat 'channelsLast' (default) or 'channelsFirst'.\n * @return Padded 4D `tf.Tensor`.\n */\nexport function spatial2dPadding(x, padding, dataFormat) {\n    return tidy(() => {\n        if (x.rank !== 4) {\n            throw new ValueError(`temporalPadding expects input tensor to be 4-D, but received a ` +\n                `${x.rank}-D tensor.`);\n        }\n        if (padding == null) {\n            padding = [[1, 1], [1, 1]];\n        }\n        if (padding.length !== 2 || padding[0].length !== 2 ||\n            padding[1].length !== 2) {\n            throw new ValueError('spatial2dPadding expects `padding` to be an Array of two Arrays, ' +\n                'each of which is an Array of two integers.');\n        }\n        if (dataFormat == null) {\n            dataFormat = imageDataFormat();\n        }\n        if (dataFormat !== 'channelsLast' && dataFormat !== 'channelsFirst') {\n            throw new ValueError(`Unknown data format: ${dataFormat}. ` +\n                `Supported data formats are 'channelsLast' and 'channelsFirst.`);\n        }\n        let pattern;\n        if (dataFormat === 'channelsFirst') {\n            pattern = [[0, 0], [0, 0], padding[0], padding[1]];\n        }\n        else {\n            pattern = [[0, 0], padding[0], padding[1], [0, 0]];\n        }\n        return tfc.pad(x, pattern);\n    });\n}\nexport class ZeroPadding2D extends Layer {\n    constructor(args) {\n        if (args == null) {\n            args = {};\n        }\n        super(args);\n        this.dataFormat =\n            args.dataFormat == null ? imageDataFormat() : args.dataFormat;\n        // TODO(cais): Maybe refactor the following logic surrounding `padding`\n        //   into a helper method.\n        if (args.padding == null) {\n            this.padding = [[1, 1], [1, 1]];\n        }\n        else if (typeof args.padding === 'number') {\n            this.padding =\n                [[args.padding, args.padding], [args.padding, args.padding]];\n        }\n        else {\n            args.padding = args.padding;\n            if (args.padding.length !== 2) {\n                throw new ValueError(`ZeroPadding2D expects padding to be a length-2 array, but ` +\n                    `received a length-${args.padding.length} array.`);\n            }\n            let heightPadding;\n            let widthPadding;\n            if (typeof args.padding[0] === 'number') {\n                heightPadding = [args.padding[0], args.padding[0]];\n                widthPadding = [args.padding[1], args.padding[1]];\n            }\n            else {\n                args.padding = args.padding;\n                if (args.padding[0].length !== 2) {\n                    throw new ValueError(`ZeroPadding2D expects height padding to be a length-2 array, ` +\n                        `but received a length-${args.padding[0].length} array.`);\n                }\n                heightPadding = args.padding[0];\n                if (args.padding[1].length !== 2) {\n                    throw new ValueError(`ZeroPadding2D expects width padding to be a length-2 array, ` +\n                        `but received a length-${args.padding[1].length} array.`);\n                }\n                widthPadding = args.padding[1];\n            }\n            this.padding = [heightPadding, widthPadding];\n        }\n        this.inputSpec = [new InputSpec({ ndim: 4 })];\n    }\n    computeOutputShape(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        let rows;\n        let cols;\n        if (this.dataFormat === 'channelsFirst') {\n            if (inputShape[2] != null && inputShape[2] >= 0) {\n                rows = inputShape[2] + this.padding[0][0] + this.padding[0][1];\n            }\n            else {\n                rows = null;\n            }\n            if (inputShape[3] != null && inputShape[3] >= 0) {\n                cols = inputShape[3] + this.padding[1][0] + this.padding[1][1];\n            }\n            else {\n                cols = null;\n            }\n            return [inputShape[0], inputShape[1], rows, cols];\n        }\n        else {\n            if (inputShape[1] != null && inputShape[1] >= 0) {\n                rows = inputShape[1] + this.padding[0][0] + this.padding[0][1];\n            }\n            else {\n                rows = null;\n            }\n            if (inputShape[2] != null && inputShape[2] >= 0) {\n                cols = inputShape[2] + this.padding[1][0] + this.padding[1][1];\n            }\n            else {\n                cols = null;\n            }\n            return [inputShape[0], rows, cols, inputShape[3]];\n        }\n    }\n    call(inputs, kwargs) {\n        return tidy(() => spatial2dPadding(getExactlyOneTensor(inputs), this.padding, this.dataFormat));\n    }\n    getConfig() {\n        const config = {\n            padding: this.padding,\n            dataFormat: this.dataFormat,\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nZeroPadding2D.className = 'ZeroPadding2D';\nserialization.registerClass(ZeroPadding2D);\n//# sourceMappingURL=padding.js.map"],"names":["Embedding","constructor","args","super","this","embeddings","DEFAULT_EMBEDDINGS_INITIALIZER","batchInputShape","inputShape","batchSize","inputLength","concat","inputDim","outputDim","embeddingsInitializer","embeddingsRegularizer","activityRegularizer","embeddingsConstraint","maskZero","supportsMasking","build","addWeight","dtype","built","warnOnIncompatibleInputShape","computeMask","inputs","mask","tidy","notEqual","zerosLike","computeOutputShape","inLens","length","i","k","s1","s2","call","kwargs","invokeCallHook","input","read","as1D","reshape","shape","getConfig","config","baseConfig","Object","assign","className","serialization","batchNormalization","x","mean","variance","beta","gamma","epsilon","out","rank","normalizeBatchInTraining","reductionAxes","util","slice","sort","meanAndVariance","regularNormalizeBatchInTraining","targetShape","axis","indexOf","push","broadcastMean","broadcastVariance","broadcastGamma","broadcastBeta","broadcastNormalizeBatchInTraining","BatchNormalization","momentum","center","scale","betaInitializer","gammaInitializer","movingMeanInitializer","movingVarianceInitializer","betaConstraint","gammaConstraint","betaRegularizer","gammaRegularizer","dim","JSON","stringify","inputSpec","ndim","axes","movingMean","movingVariance","training","splice","broadcastShape","sortedReductionAxes","needsBroadcasting","broadcastMovingMean","broadcastMovingVariance","normalizeInference","normedTraining","doMovingAverage","variable","value","decay","origValue","updateDelta","sub","mul","write","updateMovingMeanAndVariance","LayerNormalization","Number","isInteger","Error","Array","isArray","nDims","paramShape","map","trainable","moments","broadcast","v","offset","momentsTiling","scaleOffsetTiling","tile","GaussianNoise","stddev","add","GaussianDropout","rate","noised","Math","sqrt","AlphaDropout","noiseShape","_getNoiseShape","droppedInputs","alphaP","keptIdx","greaterEqual","randomUniform","a","b","pool2d","poolSize","strides","padding","dataFormat","poolMode","y","paddingString","pool3d","Pooling1D","output","poolingFunction","MaxPooling1D","AveragePooling1D","Pooling2D","rows","cols","MaxPooling2D","AveragePooling2D","Pooling3D","depths","MaxPooling3D","AveragePooling3D","GlobalPooling1D","GlobalAveragePooling1D","GlobalMaxPooling1D","GlobalPooling2D","GlobalAveragePooling2D","GlobalMaxPooling2D","Merge","mergeFunction","computeElementwiseOpOutputShape","shape1","shape2","outputShape","j","batchSizes","allRanks","reshapeRequired","reshapedInputs","inputDims","maxNDim","xNDim","transposed","xShape","newShape","xTransposed","dims","yNDim","yShape","every","m","Add","clone","Multiply","Average","Maximum","Minimum","Concatenate","DEFAULT_AXIS","allNoneShape","shapeSet","shapeWithoutConcatAxis","exists","inputShapes","allNullMasks","forEach","outputMasks","asType","concatenatedMasks","interpretAxis","Dot","normalize","interpretAxes","x1","x2","axesArray","diff","diffShape","sum","transpose","adjX","adjY","matMul","idx","squeezeAxes","squeeze","expandDims","batchDot","deserialize","customObjects","fastWeightInit","getMap","classNameMap","standardizeArgs","initialState","constants","numConstants","toListOrNull","rnn","stepFunction","initialStates","goBackwards","unroll","needPerStepOutputs","perStepOutputs","lastOutput","states","timeSteps","perStepInputs","perStepMasks","outputs","t","currentInput","stepOutputs","maskedOutputs","stepMask","negStepMask","newStates","state","RNN","cell","StackedRNNCells","cells","stateSize","returnSequences","returnState","_stateful","stateful","stateSpec","states_","keptStates","getStates","numStates","setStates","stateShape","outputMask","stateMask","s","stepInputShape","spec","resetStates","name","index","expectedShape","apply","standardized","additionalInputs","additionalSpecs","fullInput","fullInputSpec","originalInputSpec","getInitialState","cellCallKwargs","rnnOutputs","trainableWeights","nonTrainableWeights","weights","setFastWeightInitDuringBuild","cellConfig","getClassName","fromConfig","cls","RNNCell","SimpleRNNCell","DEFAULT_ACTIVATION","DEFAULT_KERNEL_INITIALIZER","DEFAULT_RECURRENT_INITIALIZER","DEFAULT_BIAS_INITIALIZER","units","activation","useBias","kernelInitializer","recurrentInitializer","biasInitializer","kernelRegularizer","recurrentRegularizer","biasRegularizer","kernelConstraint","recurrentConstraint","biasConstraint","dropout","recurrentDropout","dropoutMask","recurrentDropoutMask","kernel","recurrentKernel","bias","prevOutput","h","generateDropoutMask","ones","dpMask","recDpMask","SimpleRNN","GRUCell","DEFAULT_RECURRENT_ACTIVATION","resetAfter","undefined","recurrentActivation","implementation","hTMinus1","count","z","r","hh","matrixX","recurrentKernelValue","rk1","rk2","matrixInner","xZ","xR","xH","recurrentZ","recurrentR","recurrentH","GRU","LSTMCell","unitForgetBias","_a","capturedBiasInit","capturedUnits","bI","bF","bCAndH","cTMinus1","f","c","o","z0","z1","z2","z3","LSTM","reverse","nestedStates","newNestedStates","callInputs","cellStates","getWeights","setWeights","tuples","numParams","inputWeights","createMask","fill","ZeroPadding2D","heightPadding","widthPadding","spatial2dPadding","pattern"],"sourceRoot":""}