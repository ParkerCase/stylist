{"version":3,"file":"stylist-vendors-e656afa6.f59eddeebdf1337d0c7b.js","mappings":"8XA4GA,SAASA,EAAKC,EAAGC,GAAe,GAC5B,OAAO,KAAOC,MAAK,MACf,QAA0B,IAAnBF,EAAEG,MAAMC,QAAc,IAAM,0CAA0CJ,EAAEG,MAAMC,oBACrF,MAAMC,EAAIL,EAAEG,MAAM,GACZG,EAAIN,EAAEG,MAAM,GAClB,IAAII,GAAI,OAAIF,GACRG,GAAI,OAAMR,GACd,MAAMS,GAAQ,OAAS,CAAC,CAAC,IAAK,CAAC,EAAG,IAClC,IAAIC,GAAI,OAAMD,GACd,MAAME,EAAQN,GAAKC,EAAIA,EAAID,EAC3B,IAAK,IAAIO,EAAI,EAAGA,EAAID,IAASC,EAAG,CAG5B,MAAMC,EAAQL,EACRM,EAAQJ,EACRK,EAAQR,GACbG,EAAGF,EAAGD,GAAK,KAAOL,MAAK,KAEpB,MAAMc,GAAS,OAAMR,EAAG,CAACI,EAAGA,GAAI,CAACP,EAAIO,EAAG,IAClCK,GAAQ,OAAKD,GACbE,GAAM,OAAMV,EAAG,CAACI,EAAGA,GAAI,CAAC,EAAG,IAE3BO,GAAI,QAAM,OAAQD,EAAK,IAAI,OAAS,CAAC,EAAE,MAAM,OAAS,CAAC,CAAC,MACxDE,GAAK,OAAIF,GAAK,OAAIC,EAAGF,IACrBI,GAAO,OAAIL,EAAQI,GAErBV,EADkB,IAAlBW,EAAKlB,MAAM,IACP,OAAMM,IAGN,OAAO,CACPA,GACA,OAAMY,EAAM,CAAC,EAAG,GAAI,CAACA,EAAKlB,MAAM,GAAK,EAAGkB,EAAKlB,MAAM,MACpD,GAEP,MAAMmB,GAAM,QAAI,QAAI,OAAOH,EAAGC,GAAKH,IAE7BM,GAAW,OAAMf,EAAG,CAACI,EAAG,GAAI,CAACP,EAAIO,EAAGN,IACpCkB,GAAY,OAAIF,EAAKZ,GACrBe,GAAK,OAAUf,GACrB,GAAU,IAANE,EACAJ,GAAI,OAAIe,GAAU,OAAOC,GAAW,OAAOC,EAAIF,SAE9C,CACD,MAAMG,GAAY,OAAIH,GAAU,OAAOC,GAAW,OAAOC,EAAIF,KAC7Df,GAAI,OAAO,EAAC,OAAMA,EAAG,CAAC,EAAG,GAAI,CAACI,EAAGN,IAAKoB,GAAY,EACtD,CACA,MAAMC,GAAa,OAAUH,GACvBI,GAAW,OAAMrB,EAAG,CAAC,EAAGK,GAAI,CAACP,EAAGE,EAAEJ,MAAM,GAAKS,IACnD,GAAU,IAANA,EACAL,GAAI,OAAIqB,GAAU,QAAO,OAAOA,EAAUlB,GAAIiB,QAE7C,CACD,MAAME,GAAY,OAAID,GAAU,QAAO,OAAOA,EAAUlB,GAAIiB,IAC5DpB,GAAI,OAAO,EAAC,OAAMA,EAAG,CAAC,EAAG,GAAI,CAACF,EAAGO,IAAKiB,GAAY,EACtD,CACA,MAAO,CAACnB,EAAGF,EAAGD,EAAE,KAEpB,QAAQ,CAACM,EAAOC,EAAOC,GAC3B,CAKA,OAJKd,GAAgBI,EAAIC,IACrBC,GAAI,OAAMA,EAAG,CAAC,EAAG,GAAI,CAACF,EAAGC,IACzBE,GAAI,OAAMA,EAAG,CAAC,EAAG,GAAI,CAACF,EAAGA,KAEtB,CAACC,EAAGC,EAAE,GAErB,CACO,MAAMsB,GAAK,IAAAC,IAAG,CAAEC,IA9FvB,SAAahC,EAAGC,GAAe,GAE3B,IADA,QAAOD,EAAEiC,MAAQ,GAAG,IAAM,gEAAgEjC,EAAEiC,SAC7E,IAAXjC,EAAEiC,KACF,OAAOlC,EAAKC,EAAGC,GAEd,CAKD,MAAMiC,EAAgBlC,EAAEG,MAAMgC,MAAM,EAAGnC,EAAEG,MAAMC,OAAS,GACnDgC,QAAO,CAACC,EAAOC,IAASD,EAAQC,IAC/BC,GAAO,QAAQ,OAAQvC,EAAG,CAC5BkC,EAAelC,EAAEG,MAAMH,EAAEG,MAAMC,OAAS,GACxCJ,EAAEG,MAAMH,EAAEG,MAAMC,OAAS,KACzB,GACEoC,EAAO,GACPC,EAAO,GACbF,EAAKG,SAAQC,IACT,MAAOC,EAAKC,GAAO9C,EAAK4C,EAAK1C,GAC7BuC,EAAKM,KAAKF,GACVH,EAAKK,KAAKD,EAAI,IAIlB,MAAO,EAFG,QAAQ,OAAML,EAAM,GAAIxC,EAAEG,QAC1B,QAAQ,OAAMsC,EAAM,GAAIzC,EAAEG,OAExC,CACJ,G,uGCzCO,MAAM4C,GAAY,E,SAAAhB,IAAG,CAAEiB,WAf9B,SAAoBC,EAAOC,EAAYC,EAAgB,UAAWC,EAAW,WAAYC,EAAY,EAAGC,GACpG,MAAMC,GAAS,QAAgBN,EAAO,QAAS,YAAa,WACtDO,GAAc,QAAgBN,EAAY,aAAc,YAAa,WAC3E,KAA4B,IAAhBK,EAAOtB,MAAY,IAC3B,yDAAgBsB,EAAOtB,UAC3B,KAAiC,IAArBuB,EAAYvB,OACnBuB,EAAYrD,MAAM,KAAOoD,EAAOpD,MAAM,IACV,IAAzBqD,EAAYrD,MAAM,KACG,IAAzBqD,EAAYrD,MAAM,IAAU,IAAM,qEACtC,KAA2B,MAAfmD,GAA8C,IAAvBA,EAAYlD,QAAc,IACzD,4EAAWkD,OACf,MAAMG,EAAS,CAAER,MAAOM,EAAQL,WAAYM,GACtCE,EAAQ,CAAEP,gBAAeC,WAAUC,YAAWC,eACpD,OAAO,KAAOK,UAAU,KAAWF,EAAQC,EAC/C,G,4FCeO,MAAME,EA5CbC,eAA4BC,EAAaC,EAASC,EAAI,GAClD,MAAMC,GAAe,QAAgBH,EAAa,cAAe,UAC3DI,GAAW,QAAgBH,EAAS,UAAW,WACrD,QAAOE,EAAahC,KAAO,GAAG,IAC1B,uEAAWgC,EAAahC,UAC5B,QAAOgC,EAAahC,KAAO,IAAMiC,EAASjC,MAAM,IAE5C,mFAAGgC,EAAahC,yBAAyBiC,EAASjC,UACtD,QAAkBgC,EAAa9D,MAAMgC,MAAM,EAAG8B,EAAa9D,MAAMC,OAAS,GAAI8D,EAAS/D,MAAO,2FAE9F,MAAMgE,EAAUF,EAAa9D,MAAM8D,EAAa9D,MAAMC,OAAS,IAC/D,QAAO4D,EAAI,GAAKA,GAAKG,GAAS,IAC1B,4EAAcA,eAAqBH,MACvC,MAAMI,QAAwBH,EAAaI,OACrCC,QAAoBJ,EAASG,QAG5BE,EAAOC,GAAQ,CAACJ,EAAgBhE,OAAS+D,EAASA,GACnDM,GAAY,QAAuB,OAAQF,GACjD,IAAK,IAAIG,EAAI,EAAGA,EAAIH,EAAOG,IAAK,CAC5B,MAAMC,EAASD,EAAIF,EACbI,EAAOR,EAAgBS,SAASF,EAAQA,EAASH,GACjDM,EAAY,GAClB,IAAK,IAAIC,EAAI,EAAGA,EAAIH,EAAKxE,OAAQ2E,IAC7BD,EAAUhC,KAAK,CAAET,MAAOuC,EAAKG,GAAIC,MAAOD,IAE5CD,EAAUG,MAAK,CAACC,EAAGR,IAAMA,EAAErC,MAAQ6C,EAAE7C,QACrCoC,EAAUC,GAAK,EACf,IAAK,IAAIK,EAAI,EAAGA,EAAIf,EAAGe,IACnB,GAAID,EAAUC,GAAGC,QAAUV,EAAYI,GAAI,CACvCD,EAAUC,GAAK,EACf,KACJ,CAER,CAQA,OAPIZ,IAAgBG,GAChBA,EAAakB,UAEbpB,IAAYG,GACZA,EAASiB,WAGN,OAAOV,EAAWP,EAAS/D,MAAO,OAC7C,C,iHCZO,MAAMiF,EAvBbvB,eAA6CwB,EAAOC,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBC,GAAqB,GAC3J,MAAMC,GAAS,QAAgBR,EAAO,QAAS,0BACzCS,GAAU,QAAgBR,EAAQ,SAAU,0BAC5CS,GAAS,OAAsBF,EAAQC,EAASP,EAAeC,EAAcC,EAAgB,MAC7FO,EAAiBD,EAAOR,cACxBU,EAAgBF,EAAOP,aACvBU,EAAkBH,EAAON,gBACxBU,EAAWC,SAAoBC,QAAQC,IAAI,CAACT,EAAOxB,OAAQyB,EAAQzB,UAIpE,gBAAEkC,EAAe,aAAEC,IAAiB,QAAwBL,EAAWC,EAAYJ,EAAgBC,EAAeC,EAAiBN,GAOzI,OANIC,IAAWR,GACXQ,EAAOV,UAEPW,IAAYR,GACZQ,EAAQX,UAEL,CACHoB,iBAAiB,IAAAE,GAASF,EAAiB,SAC3CC,cAAc,OAAOA,EAAc,SAE3C,C,4FC7BO,MAAME,GAAQ,E,SAAA3E,IAAG,CAAE4E,OAL1B,SAAgB3G,GACZ,MACMyD,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,UAEnC,OAAO,KAAO2D,UAAU,KAAOF,EACnC,G,sGCuCO,MAAMmD,EA1Bb/C,eAAgDwB,EAAOC,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBkB,EAAe,GACxJ,MAAMhB,GAAS,QAAgBR,EAAO,QAAS,0BACzCS,GAAU,QAAgBR,EAAQ,SAAU,0BAC5CS,GAAS,OAAsBF,EAAQC,EAASP,EAAeC,EAAcC,EAAgBoB,GACnGtB,EAAgBQ,EAAOR,cACvBC,EAAeO,EAAOP,aACtBC,EAAiBM,EAAON,eACxBoB,EAAed,EAAOc,aACtB,MAAMC,QAAuBT,QAAQC,IAAI,CAACT,EAAOxB,OAAQyB,EAAQzB,SAC3D8B,EAAYW,EAAe,GAC3BV,EAAaU,EAAe,IAI5B,gBAAEP,EAAe,eAAEQ,IAAmB,QAAwBZ,EAAWC,EAAYb,EAAeC,EAAcC,EAAgBoB,GAOxI,OANIhB,IAAWR,GACXQ,EAAOV,UAEPW,IAAYR,GACZQ,EAAQX,UAEL,CACHoB,iBAAiB,IAAAE,GAASF,EAAiB,SAC3CQ,gBAAgB,IAAAN,GAASM,GAEjC,C,sECpCO,MAAMC,GAAe,E,SAAAjF,IAAG,CAAEkF,cApBjC,SAAuBC,EAAW7C,EAAM8C,EAAGC,GACvC,MAAMC,GAAQ,QAAgBhD,EAAM,OAAQ,gBACtCiD,GAAK,QAAqBH,EAAG,IAAK,gBAClCI,GAAK,QAAqBH,EAAG,IAAK,gBACxC,IAAII,EAAQH,EACZ,MAAMI,EAAY,GAClB,IAAK,IAAI1C,EAAI,EAAGA,EAAImC,EAAU9G,OAAQ2E,IAAK,CACvC,MAAM2C,EAASR,EAAUnC,GAAGyC,EAAOF,EAAGvC,GAAIwC,EAAGxC,IAC7C0C,EAAU3E,KAAK4E,EAAO,IACtBD,EAAU3E,KAAK4E,EAAO,IACtBF,EAAQE,EAAO,EACnB,CACA,MAAMC,EAAO,GACPC,EAAO,GACb,IAAK,IAAI7C,EAAI,EAAGA,EAAI0C,EAAUrH,OAAQ2E,GAAK,EACvC4C,EAAK7E,KAAK2E,EAAU1C,IACpB6C,EAAK9E,KAAK2E,EAAU1C,EAAI,IAE5B,MAAO,CAAC4C,EAAMC,EAClB,G,uGCgBO,MAAMC,GAAM,E,SAAA9F,IAAG,CAAE+F,KAPxB,SAAc5C,EAAGR,GACb,IAAIqD,GAAK,QAAgB7C,EAAG,IAAK,OAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,QAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,GAC9B,MAAMvE,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAAUF,EACtC,G,uGCVO,MAAMyE,GAAY,E,SAAAnG,IAAG,CAAEoG,WAP9B,SAAoBjD,EAAGR,GACnB,MAAMqD,GAAK,QAAgB7C,EAAG,IAAK,YAAa,QAC1C8C,GAAK,QAAgBtD,EAAG,IAAK,YAAa,SAChD,QAA2BqD,EAAG5H,MAAO6H,EAAG7H,OACxC,MAAMsD,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAAWF,EACvC,G,6FCHO,MAAM2E,GAAQ,E,SAAArG,IAAG,CAAEsG,OAL1B,SAAgBrI,GACZ,MACMyD,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,UAEnC,OAAO,KAAO2D,UAAU,KAAOF,EACnC,G,6FCkBO,MAAM6E,GAAO,E,SAAAvG,IAAG,CAAEwG,MANzB,SAAevI,EAAGwI,EAAO,KAAMC,GAAW,GACtC,MACMhF,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,SAE7B0D,EAAQ,CAAE8E,OAAMC,YACtB,OAAO,KAAO9E,UAAU,KAAMF,EAAQC,EAC1C,G,8HCQO,MAAMgF,GAAU,E,SAAA3G,IAAG,CAAE4G,SAZ5B,SAAkBzD,EAAGR,GACjB,IAAIqD,GAAK,QAAgB7C,EAAG,IAAK,WAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,YAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,GACb,SAAbD,EAAGa,QACHb,GAAK,OAAKA,EAAI,SACdC,GAAK,OAAKA,EAAI,WAElB,QAA2BD,EAAG5H,MAAO6H,EAAG7H,OACxC,MAAMsD,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAASF,EACrC,G,oLCUO,MAAMoF,GAAY,IAAA9G,IAAG,CAAE+G,WAf9B,SAAoB9I,EAAGwI,EAAO,KAAMC,GAAW,GAC3C,MAAMM,GAAK,QAAgB/I,EAAG,IAAK,aAC7BgJ,GAAO,QAAeR,EAAMO,EAAG5I,OAC/B8I,GAAO,OAAIF,EAAIC,GAAM,GACrB9D,GAAI,OAAI6D,EAAIE,GACZvE,GAAI,OAAIQ,GACRiC,GAAI,OAAIzC,EAAGsE,GACXE,GAAI,OAAI/B,GACRgC,GAAM,QAAI,OAAQF,EAAMC,EAAE/I,OAAQ+I,GACxC,GAAIT,EAAU,CACV,MAAMW,GAAW,QAAqBD,EAAIhJ,MAAO6I,GACjD,OAAO,OAAQG,EAAKC,EACxB,CACA,OAAOD,CACX,G,mHCzBO,MAAME,GAAe,E,SAAAtH,IAAG,CAAEuH,cARjC,SAAuBpE,EAAGR,GACtB,IAAIqD,GAAK,QAAgB7C,EAAG,IAAK,gBAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,iBAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,IAC9B,QAA2BD,EAAG5H,MAAO6H,EAAG7H,OACxC,MAAMsD,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAAcF,EAC1C,G,wGCDO,MAAM8F,GAAa,E,SAAAxH,IAAG,CAAEyH,YAP/B,SAAqBtE,EAAGR,GACpB,MAAMqD,GAAK,QAAgB7C,EAAG,IAAK,aAAc,QAC3C8C,GAAK,QAAgBtD,EAAG,IAAK,aAAc,SACjD,QAA2BqD,EAAG5H,MAAO6H,EAAG7H,OACxC,MAAMsD,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAAYF,EACxC,G,8HCuCO,MAAMgG,GAAY,IAAA1H,IAAG,CAAE2H,WAxB9B,SAAoB1J,EAAG2J,EAAa,CAAC,EAAG,EAAG,GAAIC,EAASC,EAAKC,EAAiBC,EAAa,SACvF,MAAMhB,GAAK,QAAgB/I,EAAG,IAAK,aACnC,IAAIgK,EAAMjB,EACNkB,GAAe,EACH,IAAZlB,EAAG9G,OACHgI,GAAe,EACfD,GAAM,OAAQjB,EAAI,CAAC,EAAGA,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,MAE1E,KAAyB,IAAb6J,EAAI/H,MAAY,IAAM,qDAAqD+H,EAAI/H,UAC3F,KAA2B,UAAf8H,GAAwB,IAChC,gFAAyBA,MACN,MAAnBD,GACA,KAAY,KAAWD,IAAM,IACzB,0EAAmBC,iBAA+BD,OAE1D,MAAMpG,EAAS,CAAEzD,EAAGgK,GACdtG,EAAQ,CAAEiG,aAAYC,UAASC,MAAKC,kBAAiBC,cAErDZ,EAAM,KAAOxF,UAAU,KAAWF,EAAQC,GAChD,OAAIuG,GACO,OAAQd,EAAK,CAACA,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,KAEtEgJ,CACX,G,8HC3BO,SAASe,EAASlK,EAAGmK,GAAG,SAAEC,EAAW,MAAS,CAAC,GAClD,GAAiB,OAAbA,GAAkC,OAAbA,EACrB,MAAM,IAAIC,UAAU,GAAGD,+CAE3B,QAAUE,IAANtK,EACA,MAAO,GAEX,IAAI+I,GAAK,QAAgB/I,EAAG,IAAK,WAAYA,aAAa,KAASA,EAAE4I,MAAQ,WAC7E,QAAU0B,IAANH,EACA,MAAO,CAACpB,GAEZ,IAAIwB,GAAK,QAAgBJ,EAAG,IAAK,WAAYA,aAAa,KAASA,EAAEvB,MAAQ,WAC7E,MAAMlI,GAAI,QAAcqI,EAAG5I,OACrBiH,GAAI,QAAcmD,EAAGpK,OAC3B,MAAiB,OAAbiK,GACArB,GAAK,OAAQA,EAAI,CAAC,GAAI,IACtBwB,GAAK,OAAQA,EAAI,EAAE,EAAG,IACf,EACH,QAAO,OAAK,CAACnD,EAAG,GAAI2B,EAAGH,OAAQG,IAC/B,OAAOwB,GAAI,OAAK,CAAC,EAAG7J,GAAI6J,EAAG3B,WAGnCG,GAAK,OAAQA,EAAI,EAAE,EAAG,IACtBwB,GAAK,OAAQA,EAAI,CAAC,GAAI,IACf,EACH,OAAOxB,GAAI,OAAK,CAAC,EAAG3B,GAAI2B,EAAGH,SAC3B,QAAO,OAAK,CAAClI,EAAG,GAAI6J,EAAG3B,OAAQ2B,IAEvC,C,6FC5CO,MAAMC,GAAQ,E,SAAAzI,IAAG,CAAE0I,OAL1B,SAAgBzK,GACZ,MACMyD,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,UAEnC,OAAO,KAAO2D,UAAU,KAAOF,EACnC,G,kHCQO,MAAMiH,GAAa,E,SAAA3I,IAAG,CAAE4I,YAP/B,SAAqBzF,EAAGR,GACpB,MAAMqD,GAAK,QAAgB7C,EAAG,IAAK,aAAc,QAC3C8C,GAAK,QAAgBtD,EAAG,IAAK,aAAc,QAGjD,OAFA,QAA2BqD,EAAG5H,MAAO6H,EAAG7H,QAEjC,QAAW,OAAU+E,EAAGR,IAAI,QAAW,OAAWQ,EAAGR,IAChE,G,oLCeO,MAAMkG,GAAU,IAAA7I,IAAG,CAAE8I,SAf5B,SAAkBC,EAAQhH,EAAaiH,EAASC,EAAU,KAAMC,EAAY,IAAUC,wBAClF,MAAMC,GAAU,QAAgBL,EAAQ,SAAU,WAC5C7G,GAAe,QAAgBH,EAAa,cAAe,WACjE,IAAIsH,EAAW,KACA,MAAXL,IACAK,GAAW,QAAgBL,EAAS,UAAW,aAEnD,QAAkBI,EAAQhL,MAAO8D,EAAa9D,MAAO,sBACrD,MAAMkL,GAAM,OAAO,GACbC,GAAgB,OAAON,GACvBO,GAAK,QAAI,OAAIJ,GAAS,QAAI,OAAIlH,EAAcqH,MAC5CE,GAAK,QAAI,OAAIH,EAAKF,IAAU,QAAI,QAAI,OAAIE,EAAKpH,GAAeqH,KAC5DG,GAAS,OAAIF,EAAIC,GACvB,OAAO,OAAoBC,EAAQL,EAAUH,EACjD,G,8HCTO,MAAMS,GAAmB,IAAA3J,IAAG,CAAE4J,kBAXrC,SAA2Bb,EAAQhH,EAAaiH,EAASE,EAAY,IAAUC,wBAC3E,MAAMC,GAAU,QAAgBL,EAAQ,SAAU,oBAC5C7G,GAAe,QAAgBH,EAAa,cAAe,oBACjE,IAAIsH,EAAW,KACA,MAAXL,IACAK,GAAW,QAAgBL,EAAS,UAAW,sBAEnD,QAAkBI,EAAQhL,MAAO8D,EAAa9D,MAAO,+BACrD,MAAMsL,GAAS,OAAkBN,EAASlH,GAC1C,OAAO,OAAoBwH,EAAQL,EAAUH,EACjD,G,wGCYO,MAAMW,GAAc,E,SAAA7J,IAAG,CAAE8J,aAnBhC,SAAsBC,EAAItE,EAAOE,EAAQiC,EAAYC,EAASC,EAAKC,GAC/D,MAAMiC,GAAM,QAAgBD,EAAI,KAAM,eAChCE,GAAS,QAAgBxE,EAAO,QAAS,eACzCyE,GAAU,QAAgBvE,EAAQ,SAAU,eAClD,KAAYsE,EAAO/J,OAAS8J,EAAI9J,MAAM,IAAM,kBAAkB+J,EAAO/J,oCAC7D8J,EAAI9J,UACZ,KAAyB,IAAb8J,EAAI9J,MAAY,IACxB,wDAAG8J,EAAI9J,UACX,KAA4B,IAAhB+J,EAAO/J,MAAY,IAC3B,2DAAG+J,EAAO/J,UACS,MAAnB6H,GACA,KAAY,KAAWD,IAAM,IACzB,4EAAmBC,iBAA+BD,OAE1D,MAAMpG,EAAS,CAAEqI,GAAIC,EAAKvE,MAAOwE,EAAQtE,OAAQuE,GAC3CvI,EAAQ,CAAEiG,aAAYC,UAASC,MAAKC,mBAE1C,OAAO,KAAOnG,UAAU,KAAaF,EAAQC,EACjD,G,wBC1CO,IAAIwI,E,gCACX,SAAWA,GACPA,EAAUA,EAAgB,KAAI,GAAK,OACnCA,EAAUA,EAAgB,KAAI,GAAK,OACnCA,EAAUA,EAAe,IAAI,GAAK,MAClCA,EAAUA,EAAkC,uBAAI,GAAK,wBACxD,CALD,CAKGA,IAAcA,EAAY,CAAC,G,8HC2DvB,MAAMC,GAAgB,IAAApK,IAAG,CAAEqK,eArClC,SAAwBN,EAAItE,EAAOE,EAAQiC,EAAYC,EAASC,EAAKC,GACjE,MAAMiC,GAAM,QAAgBD,EAAI,KAAM,iBAChCE,GAAS,QAAgBxE,EAAO,QAAS,iBACzCyE,GAAU,QAAgBvE,EAAQ,SAAU,iBAClD,IAAI2E,EAAON,EACPO,EAAUN,EACVO,EAAWN,EACXhC,GAAe,EACC,IAAhB+B,EAAO/J,OACPgI,GAAe,EACfoC,GAAO,OAAQN,EAAK,CAAC,EAAGA,EAAI5L,MAAM,GAAI4L,EAAI5L,MAAM,GAAI4L,EAAI5L,MAAM,GAAI4L,EAAI5L,MAAM,KAC5EmM,GAAU,OAAQN,EAAQ,CACtB,EAAGA,EAAO7L,MAAM,GAAI6L,EAAO7L,MAAM,GAAI6L,EAAO7L,MAAM,GAAI6L,EAAO7L,MAAM,KAEvEoM,GAAW,OAAQN,EAAS,CACxB,EAAGA,EAAQ9L,MAAM,GAAI8L,EAAQ9L,MAAM,GAAI8L,EAAQ9L,MAAM,GAAI8L,EAAQ9L,MAAM,MAG/E,KAA0B,IAAdkM,EAAKpK,MAAY,IACzB,0DAAGoK,EAAKpK,UACZ,KAA6B,IAAjBqK,EAAQrK,MAAY,IAC5B,6DAAGqK,EAAQrK,UACf,KAA8B,IAAlBsK,EAAStK,MAAY,IAC7B,8DAAGsK,EAAStK,UACO,MAAnB6H,GACA,KAAY,KAAWD,IAAM,IACzB,8EAA0BC,iBAA+BD,OAEjE,MAAMpG,EAAS,CAAEqI,GAAIO,EAAM7E,MAAO8E,EAAS5E,OAAQ6E,GAC7C7I,EAAQ,CAAEiG,aAAYC,UAASC,MAAKC,mBAEpCX,EAAM,KAAOxF,UAAU,KAAeF,EAAQC,GACpD,OAAIuG,GACO,OAAQd,EAAK,CAACA,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,KAEtEgJ,CACX,G,gMC4EO,MAAMqD,GAAS,IAAAzK,IAAG,CAAE0K,aA3G3B,UAAsB,EAAEvH,EAAC,EAAER,EAAC,WAAEgI,GAAa,EAAK,WAAEC,GAAa,EAAK,KAAEC,EAAI,WAAEC,EAAa,SAAQ,uBAAEC,EAAsB,eAAEC,IACvH,IAA2D,KAAvD,QAAW,KAAOC,MAAMC,cAAeJ,GAAuB,CAC9D,IAAIK,GAAS,OAAchI,EAAGR,EAAGgI,EAAYC,GAI7C,OAHY,MAARC,IACAM,GAAS,OAAIA,EAAQN,KAElB,QAAgBM,EAAQL,EAAYC,EAAwBC,EACvE,CACA,IAAIhF,GAAK,QAAgB7C,EAAG,IAAK,gBAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,iBAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,GAC9B,MAAMmF,EAAcT,EAAa3E,EAAG5H,MAAM4H,EAAG9F,KAAO,GAAK8F,EAAG5H,MAAM4H,EAAG9F,KAAO,GACtEmL,EAAcT,EAAa3E,EAAG7H,MAAM6H,EAAG/F,KAAO,GAAK+F,EAAG7H,MAAM6H,EAAG/F,KAAO,GACtEoL,EAAcX,EAAa3E,EAAG5H,MAAM4H,EAAG9F,KAAO,GAAK8F,EAAG5H,MAAM4H,EAAG9F,KAAO,GACtEqL,EAAcX,EAAa3E,EAAG7H,MAAM6H,EAAG/F,KAAO,GAAK+F,EAAG7H,MAAM6H,EAAG/F,KAAO,GACtEsL,EAAaxF,EAAG5H,MAAMgC,MAAM,GAAI,GAChCqL,EAAaxF,EAAG7H,MAAMgC,MAAM,GAAI,GAChCsL,EAAY,KAAmBF,GAC/BG,EAAY,KAAmBF,GACrC,KAAYzF,EAAG9F,MAAQ,GAAK+F,EAAG/F,MAAQ,GAAK8F,EAAG9F,OAAS+F,EAAG/F,MAAM,IAC7D,kFAAsB8F,EAAG9F,YAAY+F,EAAG/F,UAC5C,KAAY,KAAiBsL,EAAYC,IAAa,IAAM,4CAA4CD,WACjGC,6BAAsCzF,EAAG5H,aACzC6H,EAAG7H,sBACV,KAAYgN,IAAgBC,GAAa,IAAM,wCAAwCD,WAChFC,6BAAuCrF,EAAG5H,aAC1C6H,EAAG7H,wBAAwBuM,oBACXC,kBACvB,MAAMgB,EAAW5F,EAAG5H,MAAMgC,MAAM,GAAI,GAAGyL,OAAO,CAACP,EAAaC,IACtDO,EAAMnB,GACR,OAAQ3E,EAAI,CAAC0F,EAAWN,EAAaE,KACrC,OAAQtF,EAAI,CAAC0F,EAAWJ,EAAaF,IACnCW,EAAMnB,GACR,OAAQ3E,EAAI,CAAC0F,EAAWJ,EAAaF,KACrC,OAAQpF,EAAI,CAAC0F,EAAWN,EAAaE,IACzC,IAAIS,EAMAC,EALQ,MAARpB,IACAmB,GAAQ,QAAgBnB,EAAM,OAAQ,iBACrCmB,IAAS,IAAA9F,gBAAe8F,EAAOhG,GAChC,KAA0C4F,EAAUI,EAAM5N,QAGhC,MAA1B2M,IACAkB,GAA0B,QAAgBlB,EAAwB,gBAAiB,iBAEvF,MAAMmB,EAAO,CAACnC,EAAIoC,KACd,MAAOL,EAAKC,EAAK3D,EAAG4D,GAASG,EAIvBC,GAAe,SAAqB,OAAQrC,EAAI3B,EAAEhK,OAAQgK,EAAG0C,GACnE,IAAIuB,EACAC,EAiBJ,GAhBK3B,GAAeC,GAIVD,GAAcC,GACpByB,GAAO,OAAcD,EAAcL,GAAK,GAAO,GAC/CO,GAAO,OAAcF,EAAcN,GAAK,GAAM,IAEzCnB,IAAeC,GACpByB,GAAO,OAAcN,EAAKK,GAAc,GAAO,GAC/CE,GAAO,OAAcR,EAAKM,GAAc,GAAO,KAG/CC,GAAO,OAAcN,EAAKK,GAAc,GAAM,GAC9CE,GAAO,OAAcF,EAAcN,GAAK,GAAM,KAb9CO,GAAO,OAAcD,EAAcL,GAAK,GAAO,GAC/CO,GAAO,OAAcR,EAAKM,GAAc,GAAM,IActC,MAARvB,EAAc,CAEd,MAAO,CAACwB,EAAMC,GADE,QAAqBN,EAAOI,GAEhD,CAEI,MAAO,CAACC,EAAMC,EAClB,EAEE5K,EAAS,CACXyB,EAAG2I,EACHnJ,EAAGoJ,EACHlB,KAAMmB,EACNjB,uBAAwBkB,GAEtBtK,EAAQ,CAAEgJ,aAAYC,aAAYE,aAAYE,kBAGpD,GAAY,MAARH,EAAc,CACd,MAAM0B,GAAW,SAAW,CAACT,EAAKC,EAAKS,KACnC,MAAMpF,EAEN,KAAOxF,UAAU,KAAcF,EAAQC,GAEvC,OADA6K,EAAK,CAACV,EAAKC,EAAK3E,IACT,CAAE9G,OAAO,OAAQ8G,EAAKwE,GAAWa,SAAUP,EAAM,IAE5D,OAAOK,EAAST,EAAKC,EACzB,CACK,CACD,MAAMW,GAAmB,SAAW,CAACZ,EAAKC,EAAKC,EAAOQ,KAClD,MAAMpF,EAEN,KAAOxF,UAAU,KAAcF,EAAQC,GAEvC,OADA6K,EAAK,CAACV,EAAKC,EAAK3E,EAAK4E,IACd,CAAE1L,OAAO,OAAQ8G,EAAKwE,GAAWa,SAAUP,EAAM,IAE5D,OAAOQ,EAAiBZ,EAAKC,EAAKC,EACtC,CACJ,G,8HC7FO,MAAMW,GAAU,E,SAAA3M,IAAG,CAAE4M,SAZ5B,SAAkBzJ,EAAGR,GACjB,IAAIqD,GAAK,QAAgB7C,EAAG,IAAK,WAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,YAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,GACb,SAAbD,EAAGa,QACHb,GAAK,OAAKA,EAAI,SACdC,GAAK,OAAKA,EAAI,WAElB,QAA2BD,EAAG5H,MAAO6H,EAAG7H,OACxC,MAAMsD,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAASF,EACrC,G,wGCGO,MAAMmL,GAAgB,E,SAAA7M,IAAG,CAAE8M,eApBlC,SAAwB5L,EAAOoC,EAAOyJ,EAAQC,EAAUC,EAAS,WAAYC,EAAqB,GAC9F,MAAM1L,GAAS,QAAgBN,EAAO,QAAS,iBACzC4C,GAAS,QAAgBR,EAAO,QAAS,gBAAiB,WAC1D6J,GAAU,QAAgBJ,EAAQ,SAAU,gBAAiB,SAC7DK,EAAWtJ,EAAO1F,MAAM,GAC9B,KAA4B,IAAhBoD,EAAOtB,MAAY,IAC3B,6DAAgBsB,EAAOtB,UAC3B,KAA4B,IAAhB4D,EAAO5D,MAAkC,IAApB4D,EAAO1F,MAAM,IAAU,IAAM,oDAAoDgP,sBAC7FtJ,EAAO1F,WAC5B,KAA6B,IAAjB+O,EAAQjN,MAAciN,EAAQ/O,MAAM,KAAOgP,GAAU,IAAM,qDAAqDA,oBACvGtJ,EAAO1F,WAC5B,KAAgC,IAApB4O,EAAS3O,QAAc,IAC/B,wEAAU2O,EAAS3O,YACvB,KAAY2O,EAAS,IAAM,GAAKA,EAAS,IAAM,GAAG,IAAM,2CAA2CA,MACnG,KAAuB,aAAXC,GAAoC,YAAXA,GAAsB,IAAM,+CAA+CA,MAChH,MAAMvL,EAAS,CAAER,MAAOM,EAAQ8B,MAAOQ,EAAQiJ,OAAQI,GACjDxL,EAAQ,CAAEsL,SAAQC,qBAAoBF,YAE5C,OADY,KAAOpL,UAAU,KAAeF,EAAQC,EAExD,G,6FC1BO,MAAM0L,GAAQ,E,SAAArN,IAAG,CAAEsN,OAL1B,SAAgBrP,GACZ,MACMyD,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,UAEnC,OAAO,KAAO2D,UAAU,KAAOF,EACnC,G,6FCGO,MAAM6L,GAAO,E,SAAAvN,IAAG,CAAEwN,MALzB,SAAe/H,GACX,MACM/D,EAAS,CAAE+D,OADF,QAAgBA,EAAO,QAAS,SAE/C,OAAO,KAAO7D,UAAU,KAAMF,EAClC,G,sNC2DO,MAAM+L,GAAW,IAAAzN,IAAG,CAAE0N,UA7B7B,SAAmBvK,EAAGwK,EAAUC,IAC5B,QAAOD,EAAW,IAAM,GAAG,IAAM,gDAAgDA,QACjF,QAAOC,EAAW,IAAM,GAAG,IAAM,gDAAgDA,OACjF,MAAM5H,GAAK,QAAgB7C,EAAG,IAAK,aACnC,QAAO6C,EAAG9F,MAAQ,GAAG,IAAM,4CAA4C8F,EAAG9F,UAC1E,MAAM9B,EAAQ4H,EAAG5H,OACVyP,EAAGC,GAAK9H,EAAG5H,MAAMgC,OAAO,GAC/B,KAAMuN,GAAYE,GACd,MAAM,IAAIE,MAAM,yBAAyBJ,mDACYE,OAEzD,KAAMD,GAAYE,GACd,MAAM,IAAIC,MAAM,yBAAyBH,sDACeE,OAExDH,EAAW,IACXA,EAAWE,GAEXD,EAAW,IACXA,EAAWE,GAEf,MAAM9K,GAAI,QAAQ,OAAM,EAAG6K,EAAG,EAAG,SAAU,EAAE,EAAG,IAC1ChP,GAAI,OAAM,EAAGiP,EAAG,EAAG,SACnBE,GAAK,OAAIhL,EAAGnE,GACZoP,GAAS,QAAW,OAAUD,GAAI,QAAQL,EAAU,WAAW,OAAaK,GAAI,QAAQJ,EAAU,WAClGM,GAAO,OAAM,CAACL,EAAGC,GAAI9H,EAAGa,OAC9B,OAAO,QAAQ,QAAM,QAAQ,OAAQb,EAAI,EAAE,EAAG6H,EAAGC,KAC5CK,KAAIC,IAAO,OAAMH,EAAQG,EAAKF,MAAS9P,EAChD,G,8JC3DO,MAAMiQ,GAAiB,IAAArO,IAAG,CAAEsO,gBAZnC,SAAyBvF,EAAQhH,EAAa0E,EAAMuC,EAASE,EAAY,IAAUC,wBAC/E,MAAMC,GAAU,QAAgBL,EAAQ,SAAU,kBAC5C7G,GAAe,QAAgBH,EAAa,cAAe,kBACjE,IAAIsH,EAAW,KACA,MAAXL,IACAK,GAAW,QAAgBL,EAAS,UAAW,oBAEnD,QAAkBI,EAAQhL,MAAO8D,EAAa9D,MAAO,6BACrD,MAAMkL,GAAM,OAAO,GACbI,GAAS,OAAIJ,GAAK,QAAI,OAAIF,EAASlH,GAAeuE,GAAM,IAC9D,OAAO,OAAoBiD,EAAQL,EAAUH,EACjD,G,8LCuBO,MAAMqF,GAAY,IAAAvO,IAAG,CAAEwO,WAf9B,SAAoBzF,EAAQhH,EAAaiH,EAASyF,EAAQ,EAAKvF,EAAY,IAAUC,wBACjF,MAAMC,GAAU,QAAgBL,EAAQ,SAAU,aAC5C7G,GAAe,QAAgBH,EAAa,cAAe,aACjE,IAAIsH,EAAW,KACA,MAAXL,IACAK,GAAW,QAAgBL,EAAS,UAAW,eAEnD,QAAkBI,EAAQhL,MAAO8D,EAAa9D,MAAO,wBACrD,MAAMsQ,GAAc,OAAOD,GACrBE,GAAQ,QAAI,OAAIzM,EAAckH,IAC9BwF,GAAY,OAAQD,EAAOD,GAC3BG,GAAS,OAAIF,EAAOC,GACpBlF,GAAS,QAAI,QAAI,OAAO,KAAM,OAAOkF,KAAa,OAAIF,EAAaG,IACzE,OAAO,OAAoBnF,EAAQL,EAAUH,EACjD,G,6FCHO,MAAM4F,GAAM,E,SAAA9O,IAAG,CAAE+O,KANxB,SAAc9Q,EAAGwI,EAAO,KAAMC,GAAW,GACrC,MACMhF,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,QAE7B0D,EAAQ,CAAEqN,iBAAkBvI,EAAMC,YACxC,OAAO,KAAO9E,UAAU,KAAKF,EAAQC,EACzC,G,6SCWO,MAAMsN,GAAwB,IAAAjP,IAAG,CAAEkP,uBAzB1C,SAAgCC,EAAQ1M,EAAM2M,GAAe,EAAOC,GAAmB,GACnF,MAAMC,GAAU,QAAgBH,EAAQ,SAAU,yBAClD,KAA6B,IAAjBG,EAAQpP,MAA+B,IAAjBoP,EAAQpP,MAAY,IAClD,uEAAQoP,EAAQpP,UACpB,KAA4B,IAAhBuC,EAAKpE,QAAc,IAC3B,oEAAGoE,OACP,KAA8B,YAAlB6M,EAAQzI,OAAyC,UAAlByI,EAAQzI,OAAmB,IAAM,qDAC5E,MAAiC,IAArBwI,IAA+C,IAAjBD,GAAwB,IAAM,6FAExE,IAAIG,EAAcD,EACdE,GAAe,EACE,IAAjBF,EAAQpP,OACRsP,GAAe,EACfD,GAAc,OAAQD,EAAS,CAAC,EAAGA,EAAQlR,MAAM,GAAIkR,EAAQlR,MAAM,GAAIkR,EAAQlR,MAAM,MAEzF,QAAWqE,EACLf,EAAS,CAAEyN,OAAQI,GACnB5N,EAAQ,CAAEyN,eAAcC,mBAAkB5M,QAE1C2E,EAAM,KAAOxF,UAAU,KAAuBF,EAAQC,GAC5D,OAAI6N,GACO,OAAQpI,EAAK,CAACA,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,KAExDgJ,CACX,G,6FC3BO,MAAMqI,GAAM,E,SAAAzP,IAAG,CAAE0P,KALxB,SAAczR,GACV,MACMyD,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,QAEnC,OAAO,KAAO2D,UAAU,KAAKF,EACjC,G,8HC2BO,MAAMiO,GAAiB,IAAA3P,IAAG,CAAE4P,gBAxBnC,SAAyBT,EAAQ1M,EAAM2M,GAAe,EAAOC,GAAmB,GAC5E,MAAMC,GAAU,QAAgBH,EAAQ,SAAU,kBAClD,KAA6B,IAAjBG,EAAQpP,MAA+B,IAAjBoP,EAAQpP,MAAY,IAClD,gEAAQoP,EAAQpP,UACpB,KAA4B,IAAhBuC,EAAKpE,QAAc,IAC3B,6DAAGoE,OACP,MAAiC,IAArB4M,IAA+C,IAAjBD,GAAwB,IAAM,sFAExE,IAAIG,EAAcD,EACdE,GAAe,EACE,IAAjBF,EAAQpP,OACRsP,GAAe,EACfD,GAAc,OAAQD,EAAS,CAAC,EAAGA,EAAQlR,MAAM,GAAIkR,EAAQlR,MAAM,GAAIkR,EAAQlR,MAAM,MAEzF,QAAWqE,EACLf,EAAS,CAAEyN,OAAQI,GACnB5N,EAAQ,CAAEyN,eAAcC,mBAAkB5M,QAE1C2E,EAAM,KAAOxF,UAAU,KAAgBF,EAAQC,GACrD,OAAI6N,GACO,OAAQpI,EAAK,CAACA,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,KAExDgJ,CACX,G,6FCDO,MAAMyI,GAAoB,E,SAAA7P,IAAG,CAAE8P,mBARtC,SAA4B7R,EAAG2J,EAAYC,EAASC,EAAKiI,GAAsB,GAC3E,MACMrO,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,sBAE7B0D,EAAQ,CAAEiG,aAAYC,UAASC,MAAKiI,uBAEpC5E,EAAS,KAAOvJ,UAAU,KAAmBF,EAAQC,GAC3D,MAAO,CAAEwJ,OAAQA,EAAO,GAAI6E,QAAS7E,EAAO,GAChD,G,kFC9BO,SAAS8E,EAASC,EAAOC,EAAMC,GAClC,GAAIA,GAAO,EACP,MAAM,IAAIrC,MAAM,4CAEpB,MAAMpM,EAAQ,CAAEuO,QAAOC,OAAMC,OAC7B,OAAO,KAAOxO,UAAU,KAAU,CAAC,EAAGD,EAC1C,C,6FCiBO,MAAM0O,GAAS,E,SAAArQ,IAAG,CAAEsQ,QAP3B,SAAiBrS,EAAGsS,EAAS9J,EAAO,EAAG+J,EAAY,GAC/C,MAEM9O,EAAS,CAAEzD,GAFN,QAAgBA,EAAG,IAAK,UAEXsS,SADP,QAAgBA,EAAS,UAAW,SAAU,UAEzD5O,EAAQ,CAAE8E,OAAM+J,aACtB,OAAO,KAAO5O,UAAU,KAAUF,EAAQC,EAC9C,G,6FCYO,MAAM8O,GAAW,E,SAAAzQ,IAAG,CAAE0Q,UAN7B,SAAmBzS,EAAGsS,GAClB,MAAMI,GAAW,QAAgBJ,EAAS,UAAW,WAAY,SAE3D7O,EAAS,CAAEsC,QADN,QAAgB/F,EAAG,IAAK,YACNsS,QAASI,GACtC,OAAO,KAAO/O,UAAU,KAAUF,EACtC,G,wGCdO,MAAMkP,GAAoB,E,SAAA5Q,IAAG,CAAE6Q,mBAVtC,SAA4BvN,EAAOC,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,mBAClG,MAAME,GAAS,QAAgBR,EAAO,QAAS,qBACzCS,GAAU,QAAgBR,EAAQ,SAAU,qBAC5C7B,GAAS,OAAsBoC,EAAQC,EAASP,EAAeC,EAAcC,GAI7E/B,EAAQ,CAAE6B,cAHhBA,EAAgB9B,EAAO8B,cAGQC,aAF/BA,EAAe/B,EAAO+B,aAEuBC,eAD7CA,EAAiBhC,EAAOgC,gBAExB,OAAO,KAAO9B,UAAU,KAAqB,CAAE0B,MAAOQ,EAAQP,OAAQQ,GAAWpC,EACrF,G,mHCHO,MAAMmP,GAAY,E,SAAA9Q,IAAG,CAAE+Q,WAR9B,SAAoB5N,EAAGR,GACnB,IAAIqD,GAAK,QAAgB7C,EAAG,IAAK,aAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,cAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,IAC9B,QAA2BD,EAAG5H,MAAO6H,EAAG7H,OACxC,MAAMsD,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAAWF,EACvC,G,6FCYO,MAAMsP,GAAM,E,SAAAhR,IAAG,CAAEiR,KAPxB,SAAchT,EAAGwI,EAAO,KAAMC,GAAW,GACrC,MACMhF,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,QAE7B0D,EAAQ,CAAE8E,OAAMC,YAEtB,OAAO,KAAO9E,UAAU,KAAKF,EAAQC,EACzC,G,mNC2CO,MAAMuP,GAAsB,IAAAlR,IAAG,CAAEmR,qBAlBxC,SAA8BC,EAAkBC,EAAQrI,EAASsI,EAAiB,EAAGpI,EAAY,IAAUC,wBACvG,IAAIoI,GAAoB,QAAgBH,EAAkB,mBAAoB,uBAC9E,MAAMI,GAAU,QAAgBH,EAAQ,SAAU,uBAClD,IAAIhI,EAAW,KAKf,GAJe,MAAXL,IACAK,GAAW,QAAgBL,EAAS,UAAW,yBAEnD,QAAkBuI,EAAkBnT,MAAOoT,EAAQpT,MAAO,kCACtDkT,EAAiB,EAAG,CACpB,MAAMG,GAAuB,OAAOH,GAC9BhI,GAAM,OAAO,GACboI,GAAO,OAAO,IACpBH,GACI,QAAI,OAAIA,GAAmB,OAAIjI,EAAKmI,KAAwB,OAAIC,EAAMD,GAC9E,CACA,MAAM/H,EAjEV,SAAwCX,EAAQsI,GAC5C,MAAMjI,GAAU,QAAgBL,EAAQ,SAAU,iCAC5CyI,GAAU,QAAgBH,EAAQ,SAAU,kCAClD,QAAkBjI,EAAQhL,MAAOoT,EAAQpT,MAAO,4CAqBhD,MAAMuT,GAAY,OAAKH,GACjBI,GAAgB,OAAIJ,EAASpI,GAC7ByI,GAAgB,QAAM,QAAI,QAAI,OAAIL,MACxC,OAAO,QAAI,OAAIG,EAAWC,GAAgBC,EAC9C,CAqCmBC,CAA+BP,EAAmBC,GACjE,OAAO,OAAoB9H,EAAQL,EAAUH,EACjD,G,wGCzCO,MAAM6I,GAAM,E,SAAA/R,IAAG,CAAEgS,KAPxB,SAAc7O,EAAGR,GACb,IAAIqD,GAAK,QAAgB7C,EAAG,IAAK,OAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,QAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,GAC9B,MAAMvE,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAAKF,EACjC,G,mHCXO,MAAMuQ,GAAO,E,SAAAjS,IAAG,CAAEkS,MARzB,SAAe/O,EAAGR,GACd,IAAIqD,GAAK,QAAgB7C,EAAG,IAAK,QAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,SAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,IAC9B,QAA2BD,EAAG5H,MAAO6H,EAAG7H,OACxC,MAAMsD,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAAMF,EAClC,G,mHCEO,MAAMyQ,GAAU,E,SAAAnS,IAAG,CAAEoS,SAR5B,SAAkBjP,EAAGR,GACjB,IAAIqD,GAAK,QAAgB7C,EAAG,IAAK,WAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,YAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,IAC9B,QAA2BD,EAAG5H,MAAO6H,EAAG7H,OACxC,MAAMsD,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAASF,EACrC,G,wGCEO,MAAM2Q,GAAmB,E,SAAArS,IAAG,CAAEsS,kBATrC,SAA2BpR,EAAOqR,EAASjR,EAAY,EAAGkR,EAAS,IAC/D,MAAMhR,GAAS,QAAgBN,EAAO,QAAS,mBAAoB,WACnE,KAA4B,IAAhBM,EAAOtB,MAAY,IAC3B,gEAAgBsB,EAAOtB,UAC3B,MAAMwB,EAAS,CAAER,MAAOM,GAClBG,EAAQ,CAAE4Q,UAASjR,YAAWkR,UAEpC,OADY,KAAO5Q,UAAU,KAAkBF,EAAQC,EAE3D,G,wGCAO,MAAM8I,GAAS,E,SAAAzK,IAAG,CAAEyS,QAR3B,SAAiBtP,EAAGR,EAAGgI,GAAa,EAAOC,GAAa,GACpD,IAAI5E,GAAK,QAAgB7C,EAAG,IAAK,UAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,WAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,GAC9B,MAAMvE,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GACrBtE,EAAQ,CAAEgJ,aAAYC,cAC5B,OAAO,KAAOhJ,UAAU,KAAaF,EAAQC,EACjD,G,yKC0BO,MAAM+Q,GAAgB,IAAA1S,IAAG,CAAE2S,eAhBlC,SAAwBC,EAAG3U,EAAG4U,EAAOC,EAAMC,GAAa,GACpD,MAAMC,GAAK,QAAgBJ,EAAG,IAAK,iBAC7B5L,GAAK,QAAgB/I,EAAG,IAAK,iBAC7BgV,GAAS,QAAgBJ,EAAO,QAAS,kBAC/C,IAAAK,kBAAiBF,EAAIhM,GACrB,KAAY,KAAiBgM,EAAG5U,MAAO4I,EAAG5I,QAAQ,IAAM,8BACxD,MAAMkL,GAAM,OAAO,GACb6J,GAAgB,OAAI7J,EAAK2J,GAC/B,IAAIG,GAAS,QAAI,OAAIpM,EAAIgM,GAAKG,GAC9B,GAAIJ,EAAY,CACZ,KAAoB,MAARD,GAAc,IAAM,mDAChC,MAAMO,GAAQ,QAAgBP,EAAM,OAAQ,iBAC5CM,GAAS,OAAIA,GAAQ,OAAI9J,GAAK,OAAI2J,EAAQI,IAC9C,CACA,OAAO,OAAIL,EAAII,EACnB,G,wGCDO,MAAME,GAAY,E,SAAAtT,IAAG,CAAEuT,WAtB9B,SAAoBtV,EAAGuV,EAAUC,GAC7B,KAAqB,YAATA,GAA+B,cAATA,GAAsB,IACpD,+DAAOA,OACX,MAAMzM,GAAK,QAAgB/I,EAAG,IAAK,aACnC,GAAgB,IAAZ+I,EAAG9G,KACH,MAAM,IAAI6N,MAAM,kEAGpB,KAAYyF,EAASnV,SAAW2I,EAAG9G,MAAM,IAAM,wCAAwC8G,EAAG9G,aAC/EsT,EAASnV,YACpB,MAAMqV,EAAuB,YAATD,EAAqB,EAAI,EAC7C,IAAK,IAAIzQ,EAAI,EAAGA,EAAIgE,EAAG9G,KAAM8C,IACzB,KAAmC,IAAvBwQ,EAASxQ,GAAG3E,QAAc,IAAM,0DAC5C,KAAYmV,EAASxQ,GAAG,IAAM,GAAKwQ,EAASxQ,GAAG,IAAMgE,EAAG5I,MAAM4E,GAAK0Q,GAC/DF,EAASxQ,GAAG,IAAM,GAAKwQ,EAASxQ,GAAG,IAAMgE,EAAG5I,MAAM4E,GAAK0Q,GAAa,IAAM,wBAAwB1Q,wCAC5FgE,EAAG5I,MAAM4E,GAAK0Q,uCACX1M,EAAG5I,UAEpB,MAAMuD,EAAQ,CAAE6R,WAAUC,QACpB/R,EAAS,CAAEzD,EAAG+I,GACpB,OAAO,KAAOpF,UAAU,KAAWF,EAAQC,EAC/C,G,6FCxBO,MAAMgS,GAAY,E,SAAA3T,IAAG,CAAE4T,WAN9B,SAAoB3V,EAAG4V,EAAQ,IAC3B,MACMnS,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,cAE7B0D,EAAQ,CAAEkS,SAChB,OAAO,KAAOjS,UAAU,KAAWF,EAAQC,EAC/C,G,6FCLO,MAAMmS,GAAW,E,SAAA9T,IAAG,CAAE+T,UAL7B,SAAmB9V,GACf,MACMyD,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,aAEnC,OAAO,KAAO2D,UAAU,KAAUF,EACtC,G,6PCRO,SAASsS,EAAqBjK,EAAI3B,EAAG0C,GACxC,GAAkB,MAAdA,GAAqC,WAAfA,EACtB,OAAOf,EAEX,GAAmB,SAAfe,EACA,OAAO,OAAIf,GAAI,OAAK3B,IAExB,MAAM,IAAI2F,MAAM,gDAAgDjD,KACpE,CAEO,SAASmJ,EAAqBpJ,EAAMuB,GACvC,IAAIhF,EAAMgF,EACV,MAAM8H,EAAa,KAAgCrJ,EAAKzM,MAAOgO,EAAahO,OAI5E,OAHI8V,EAAW7V,OAAS,IACpB+I,GAAM,OAAIA,EAAK8M,KAEZ,OAAQ9M,EAAKyD,EAAKzM,MAC7B,CACO,SAAS+V,EAAgBlW,EAAG6M,EAAYC,EAAwBC,GACnE,GAAmB,WAAfF,EACA,OAAO7M,EAEN,GAAmB,SAAf6M,EACL,OAAO,OAAK7M,GAEX,GAAmB,QAAf6M,EACL,OAAO,OAAI7M,GAEV,GAAmB,UAAf6M,EACL,OAAO,OAAM7M,GAEZ,GAAmB,UAAf6M,EACL,OAAO,OAAM7M,EAAG8M,GAEf,GAAmB,cAAfD,EACL,OAAO,OAAU7M,EAAG+M,GAEnB,GAAmB,YAAfF,EACL,OAAO,OAAQ7M,GAEnB,MAAM,IAAI8P,MAAM,4BAA4BjD,KAChD,CAEO,MAAMsJ,EAAa,CAAClJ,EAAeJ,MACjBI,EAAgB,IACE,WAAfJ,C,oLCfrB,MAAMuJ,GAAsB,IAAArU,IAAG,CAAEsU,qBApCxC,SAA8B5K,EAAQV,EAASE,EAAY,IAAUC,wBACjE,MAAMoL,GAAU,QAAgB7K,EAAQ,SAAU,uBAClD,IAAIL,EAAW,KACA,MAAXL,IACAK,GAAW,QAAgBL,EAAS,UAAW,wBAEnD,MAAMwL,EAA4B,MAAZnL,EAAoBkL,GAAU,OAAIA,EAASlL,GACjE,GAAIH,IAAc,IAAUuL,KACxB,OAAOD,EAEX,GAAItL,IAAc,IAAUwL,IACxB,OAAO,OAAIF,GAEf,GAAItL,IAAc,IAAUyL,KAAM,CAC9B,GAAgB,MAAZtL,EACA,OAAO,OAAKmL,GAEX,CACD,MAAMI,EAAkBL,EAAQ9R,KAAO4G,EAAS5G,KAC1C0I,GAAS,QAAI,OAAIqJ,IAAe,OAAInL,IAC1C,OAAOuL,EAAkB,GAAI,OAAIzJ,GAAQ,OAAOyJ,IAC5CzJ,CACR,CACJ,CACA,GAAIjC,IAAc,IAAUC,uBAAwB,CAChD,GAAgB,MAAZE,EACA,OAAO,QAAI,OAAImL,IAAe,OAAOD,EAAQ9R,OAE5C,CACD,MAAMoS,GAAqB,OAAIxL,GAAU,OAAKkL,EAAQnW,QAChD0W,GAAc,QAAK,QAAI,OAASD,GAAoB,OAAO,KAAM,WACvE,OAAO,QAAI,OAAIL,GAAeM,EAClC,CACJ,CACA,MAAM/G,MAAM,sBAAsB7E,IACtC,G,gQCsEO,MAAM6L,GAAsB,IAAA/U,IAAG,CAAEgV,qBAlBxC,SAA8BC,EAAc5D,EAAQrI,EAASsI,EAAiB,EAAGpI,EAAY,IAAUC,wBACnG,IAAI+L,GAAgB,QAAgBD,EAAc,eAAgB,uBAClE,MAAMzD,GAAU,QAAgBH,EAAQ,SAAU,uBAClD,IAAIhI,EAAW,KAKf,GAJe,MAAXL,IACAK,GAAW,QAAgBL,EAAS,UAAW,yBAEnD,QAAkBkM,EAAc9W,MAAOoT,EAAQpT,MAAO,kCAClDkT,EAAiB,EAAG,CACpB,MAAMG,GAAuB,OAAOH,GAC9BhI,GAAM,OAAO,GACb6L,GAAa,OAAOD,EAAc9W,MAAM,IAC9C8W,GACI,QAAI,OAAIA,GAAe,OAAI5L,EAAKmI,KAAwB,OAAIA,EAAsB0D,GAC1F,CACA,MAAMzL,EAlEV,SAAwCX,EAAQsI,EAAQ+D,GAAM,GAI1D,IAHa,IAATA,IACAA,EAAM/D,EAAOnR,KAAO,GAEpBkV,IAAQ/D,EAAOnR,KAAO,EACtB,MAAM6N,MACF,mGAAuCsD,EAAOnR,oBAC/BkV,KAGvB,MAAM7I,GAAW,SAAW,CAACxD,EAAQsI,EAAQ7E,KAIzC,MACM6I,GAAM,OAAUhE,EAAQ,CAAC+D,IADd,GAEXE,GAAY,QAAI,OAAKjE,EAAQ,WAAYgE,GAC/C7I,EAAK,CAACzD,EAAQuM,IACd,MAAMC,GAAa,QAAI,OAAID,EAAWvM,IAUtC,MAAO,CAAEzI,OATK,OAAIiV,EAAY,CAACH,IASf3I,SARC,CAAC1C,EAAIoC,KAClB,MAAOpD,EAAQuM,GAAanJ,EACtBqJ,GAAU,QAAqBzL,EAAG3L,MAAO,CAACgX,IAChD,MAAO,EACH,QAAI,OAAQrL,EAAIyL,IAAU,QAAI,OAAKzM,EAAQ,YAAY,OAAIuM,MAC3D,QAAI,OAAQvL,EAAIyL,IAAU,QAAI,OAAIF,IAAY,OAAKvM,EAAQ,aAC9D,EAEqB,IAE9B,OAAOwD,EAASxD,EAAQsI,EAC5B,CAmCmBoE,CAA+BP,EAAe1D,GAC7D,OAAO,OAAoB9H,EAAQL,EAAUH,EACjD,G,uGChEO,MAAMwM,EAtBb5T,eAAuCwB,EAAOC,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,mBAC7G,MAAME,GAAS,QAAgBR,EAAO,QAAS,0BACzCS,GAAU,QAAgBR,EAAQ,SAAU,0BAC5C7B,GAAS,OAAsBoC,EAAQC,EAASP,EAAeC,EAAcC,GACnFF,EAAgB9B,EAAO8B,cACvBC,EAAe/B,EAAO+B,aACtBC,EAAiBhC,EAAOgC,eACxB,MAAMqB,QAAuBT,QAAQC,IAAI,CAACT,EAAOxB,OAAQyB,EAAQzB,SAC3D8B,EAAYW,EAAe,GAC3BV,EAAaU,EAAe,IAI5B,gBAAEP,IAAoB,QAAwBJ,EAAWC,EAAYb,EAAeC,EAAcC,GAOxG,OANII,IAAWR,GACXQ,EAAOV,UAEPW,IAAYR,GACZQ,EAAQX,WAEL,IAAAsB,GAASF,EAAiB,QACrC,C,8JCvBO,MAAMmR,GAAY,IAAA3V,IAAG,CAAE4V,WAd9B,SAAoB7M,EAAQhH,EAAaiH,EAASE,EAAY,IAAUC,wBACpE,IAAIC,GAAU,QAAgBL,EAAQ,SAAU,aAChD,MAAM7G,GAAe,QAAgBH,EAAa,cAAe,aACjE,IAAIsH,EAAW,KACA,MAAXL,IACAK,GAAW,QAAgBL,EAAS,UAAW,eAEnD,QAAkBI,EAAQhL,MAAO8D,EAAa9D,MAAO,wBACrD,MAAMkL,GAAM,OAAO,GAEnBF,GAAU,QAAI,QAAI,OAAO,GAAIA,GAAUE,GACvC,MAAMI,GAAS,QAAK,OAAIJ,GAAK,OAAIF,EAASlH,KAC1C,OAAO,OAAoBwH,EAAQL,EAAUH,EACjD,G,mHC2BO,MAAM2M,GAAc,IAAA7V,IAAG,CAAE8V,aAvBhC,SAAsBzE,EAAQ0E,EAAYC,EAAMC,GAAa,GACzD,MAAMzE,GAAU,QAAgBH,EAAQ,SAAU,eAC5C6E,EAAc1E,EAAQ/O,KACtB0T,EAAW3E,EAAQtR,KACzB,GAAIgW,EAAc,EACd,MAAM,IAAInI,MACN,+DAAGmI,MAEX,GAAIC,EAAW,EACX,MAAM,IAAIpI,MAAM,gDAAgDoI,KAIpEH,EAAOA,GAAQI,KAAKC,SAEpB,MACM3U,EAAS,CAAE2P,OADa,IAAb8E,GAAiB,OAAQ3E,EAAS,CAAC,GAAI,IAAMA,GAExD7P,EAAQ,CAAEoU,aAAYC,OAAMC,cAE5B7O,EAAM,KAAOxF,UAAU,KAAaF,EAAQC,GAElD,OAAoB,IAAbwU,GAAiB,OAAQ/O,EAAK,CAACA,EAAI3E,OAAS2E,CACvD,G,kFCvCO,MAAMkP,GAAqC,E,SAAAtW,IAAG,CAAEuW,oCALvD,SAA6CtY,EAAGmK,EAAG2B,EAAIyM,EAAc,EAAG3L,EAAO,EAAGgJ,EAAQ,EAAG4C,EAAO,IAChG,MAAM/U,EAAS,CAAEzD,IAAGmK,IAAG2B,MACjBpI,EAAQ,CAAE6U,cAAa3L,OAAMgJ,QAAO4C,QAC1C,OAAO,KAAO7U,UAAU,KAASF,EAAQC,EAC7C,G,wGCaO,MAAM+U,GAAgB,E,SAAA1W,IAAG,CAAE2W,eARlC,SAAwBzV,GACpB,MAAMM,GAAS,QAAgBN,EAAO,QAAS,gBAAiB,WAChE,KAA4B,IAAhBM,EAAOtB,MAAY,IAC3B,6DAAgBsB,EAAOtB,UAC3B,MAAMwB,EAAS,CAAER,MAAOM,GAExB,OADY,KAAOI,UAAU,KAAeF,EAAQ,CAAC,EAEzD,G,mLC2DO,MAAMkV,GAAc,IAAA5W,IAAG,CAAE6W,aAvChC,SAAsBC,GAClB,IAAIC,EACJ,GAAIC,MAAMC,QAAQH,GAAK,CACnBC,GAAkB,GAClB,QAAa,MAAND,GAAcA,EAAGzY,OAAS,GAAG,IAAM,sEAE1C,MAAM+W,EAAM0B,EAAG,GAAG1Y,MAAM,GACxB,IAAK,IAAI4E,EAAI,EAAGA,EAAI8T,EAAGzY,SAAU2E,GAC7B,QAAO8T,EAAG9T,GAAG5E,MAAM,KAAOgX,GAAK,IAC3B,iEAAI0B,EAAG9T,GAAG5E,MAAM,UAAUgX,MAEtC,MAEI2B,GAAkB,EAClBD,GAAK,OAAMA,EAAIA,EAAG1Y,MAAM,GAAI,GAAG+P,KAAIlQ,IAAK,OAAQA,EAAG,CAAC,OAExD,QAAO6Y,EAAGzY,QAAUyY,EAAG,GAAG1Y,MAAM,IAAI,IAAM,oCAAoC0Y,EAAGzY,yCACpDyY,EAAG,GAAG1Y,MAAM,SACzC,MAAM8Y,EAAK,GACLC,EAAOL,EACb,IAAK,IAAI9T,EAAI,EAAGA,EAAI8T,EAAGzY,SAAU2E,EAC7BkU,EAAGnW,KAAK,KAAO5C,MAAK,KAChB,IAAIF,EAAIkZ,EAAKnU,GACb,GAAIA,EAAI,EACJ,IAAK,IAAInE,EAAI,EAAGA,EAAImE,IAAKnE,EAAG,CACxB,MAAMuY,GAAO,QAAI,QAAI,OAAIF,EAAGrY,GAAIZ,IAAKiZ,EAAGrY,IACxCZ,GAAI,OAAIA,EAAGmZ,EACf,CAEJ,OAAO,OAAInZ,GAAG,OAAKA,EAAG,aAAa,KAG3C,OAAI8Y,GACO,OAAMG,EAAI,GAGVA,CAEf,G,uRCqBO,MAAMG,GAAY,IAAArX,IAAG,CAAEsX,WA/D9B,SAAoBpW,EAAO+L,EAAS,SAAUsK,GAAW,EAAOC,EAAc,IAC1E,MAAMhW,GAAS,QAAgBN,EAAO,QAAS,aAMzCuW,EAAqBjW,EAAOpD,MAAM,GAAKoD,EAAOpD,MAAM,GAC1D,IACIK,EAAGiZ,EAAG/U,EAAGgV,EADTC,GAAa,QAAI,IAAAlT,GAAS,CAAC8S,IAAe,KAU9C,GARA,KAA4B,IAAhBhW,EAAOtB,MAAY,IAC3B,yDAAgBsB,EAAOtB,UAC3B,KAAgC,IAApBsB,EAAOpD,MAAM,IAAgC,IAApBoD,EAAOpD,MAAM,IAAU,IAExD,0EAAWoD,EAAOpD,MAAM,QAC5B,KAA6B,UAAjBoD,EAAOqF,OAAsC,YAAjBrF,EAAOqF,OAAqB,IAChE,sEAAiBrF,EAAOqF,WAC5B,KAAuB,SAAXoG,GAAgC,WAAXA,GAAqB,IAAM,0CAA0CA,MAC9E,IAApBzL,EAAOpD,MAAM,GAAU,EACtBK,EAAGiZ,EAAG/U,IAAK,OAAMnB,EAAQ,CAAC,EAAG,EAAG,IAAK,GACtC,MAAMqW,GAAK,OAAIpZ,EAhBQ,OAiBjBqZ,GAAK,OAAIJ,EAhBU,MAiBnBzR,GAAK,OAAItD,EAhBS,MAiBxBgV,GAAY,QAAI,OAAIE,EAAIC,GAAK7R,EACjC,MAEI0R,EAAYzW,EAEhB,GAAe,SAAX+L,EAAmB,CAEnB2K,EAOR,SAAcG,EAAWC,GACrB,IAGIC,EAAYC,EAAaC,EAAWC,EAASC,EAAkBC,EAH/DC,GAAa,IAAA7T,GAAS,EAAE,IACxB8T,GAAe,IAAA9T,GAAS,CAAC,IACzB+T,GAAY,IAAA/T,GAAS,CAAC,IAE1B,IAAK,IAAIzB,EAAQ,EAAGA,EAAQ8U,EAAUtV,KAAO,EAAGQ,IAAS,CACrDgV,GAAa,OAAMF,EAAW,EAAG9U,EAAQ,GACzCiV,GAAc,OAAMH,EAAW9U,EAAQ,GACvCoV,GAAmB,QAAI,OAAIJ,GAAaD,GACxCM,GAAa,QAAI,OAAIJ,GAAcF,GACnC,MAAMU,GAAgB,QAAI,OAAIT,GAAY,OAAM,EAAGA,EAAWxV,QAC9D0V,GAAY,OAAIO,GAAe,OAAIT,IACnC,MAAMU,GAAc,OAAKT,EAAY9Z,MAAO6Z,EAAWxV,MACjDmW,GAAa,QAAI,OAAM,EAAGV,EAAYzV,MAAOkW,GAC7CE,GAAa,OAAIX,EAAa,GACpCE,GAAU,QAAI,OAAIS,IAAa,OAAIX,IACnC,MAAMY,GAAgB,OAAIX,EAAWC,GAC/BW,GAAgB,OAAIZ,EAAWC,GAC/BY,GAAe,OAAIX,EAAkBC,GAC3CG,GAAY,QAAI,OAAIO,EAAcF,GAAgBC,GAClD,MAAME,GAAY,OAAQR,EAAWD,GACrCA,GAAe,OAAMS,EAAWR,EAAWD,GAC3CD,GAAa,OAAMU,GAAW,IAAAvU,GAAS,CAACzB,IAASsV,EACrD,CACA,OAAOA,CACX,CAhCqBW,EADM,QAAS,QAAK,OAAMvB,GAAY,UAAU,OAAO,IAAK,KAC3CF,EAClC,CACA,MAAM0B,EAAe5B,GACjB,OAAUI,EAAWC,IAAc,OAAQD,EAAWC,GAE1D,OADe,QAAK,OAAIuB,EAAc,KAAM,QAEhD,G,+FC9EO,SAASC,EAAmBC,EAAQ9I,GACvC,MAAM+I,EAAaD,EAAOjb,MAAMC,OAC1Bkb,EAAchJ,EAAQnS,MAAMC,OAClC,GAAIib,EAAa,EACb,MAAM,IAAIvL,MACN,4EAAqBuL,MAE7B,GAAIC,EAAc,EACd,MAAM,IAAIxL,MACN,8EAAqBwL,MAE7B,GAAsB,UAAlBhJ,EAAQ1J,MACR,MAAM,IAAIkH,MACN,yEAAsBwC,EAAQ1J,UAEtC,GAAI0J,EAAQnS,MAAMmb,EAAc,GAAKD,EACjC,MAAM,IAAIvL,MACN,iEAAGwC,EAAQnS,MAAMmb,EAAc,UAAUD,KAEjD,GAAoC,KAAhC,QAAcD,EAAOjb,OACrB,MAAM,IAAI2P,MACN,mEAAiBsL,EAAOjb,UAEhC,MAAMob,EAAejJ,EAAQnS,MACvBqb,EAAYD,EAAaA,EAAanb,OAAS,GAGrD,IAAIqb,EAAU,EACd,IAAK,IAAI1W,EAAI,EAAGA,EAAIwW,EAAanb,OAAS,IAAK2E,EAC3C0W,GAAWF,EAAaxW,GAE5B,MAAM2W,EAAaN,EAAOjb,MACpBwb,EAAcJ,EAAapZ,QACjCwZ,EAAYC,MACZ,IAAIC,EAAY,EAChB,IAAK,IAAI9W,EAAIyW,EAAWzW,EAAIsW,IAActW,EACtC8W,GAAaH,EAAW3W,GACxB4W,EAAY7Y,KAAK4Y,EAAW3W,IAEhC,MAAM6E,EAAU,KAAI,QAAewR,EAAOjb,OAAO+P,KAAI4L,GAAUA,EAASD,IACpE,GAAG1Z,MAAM,EAAGqZ,GAChB,MAAO,CAACG,EAAaF,EAASI,EAAWjS,EAC7C,C,wGCWO,MAAMmS,GAA0B,E,SAAAha,IAAG,CAAEia,yBAlB5C,SAAkC3W,EAAOC,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBC,GAAqB,GAChJ,MAAMC,GAAS,QAAgBR,EAAO,QAAS,qBACzCS,GAAU,QAAgBR,EAAQ,SAAU,qBAC5CS,GAAS,OAAsBF,EAAQC,EAASP,EAAeC,EAAcC,EAAgB,MAI7FhC,EAAS,CAAE4B,MAAOQ,EAAQP,OAAQQ,GAClCpC,EAAQ,CACV6B,cALmBQ,EAAOR,cAM1BC,aALkBO,EAAOP,aAMzBC,eALoBM,EAAON,eAM3BG,sBAGEsH,EAAS,KAAOvJ,UAAU,KAAqBF,EAAQC,GAC7D,MAAO,CAAE6C,gBAAiB2G,EAAO,GAAI1G,aAAc0G,EAAO,GAC9D,G,yICYO,MAAM+O,GAAU,IAAAla,IAAG,CAAEma,SAzB5B,SAAkBlc,EAAG2J,EAAYC,EAASC,EAAKC,GAC3C,MAAMf,GAAK,QAAgB/I,EAAG,IAAK,WAEnC,IAAImc,EAAMpT,EACNwI,GAAe,EACH,IAAZxI,EAAG9G,OACHsP,GAAe,EACf4K,GAAM,OAAQpT,EAAI,CAAC,EAAGA,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,MAE7D,KAAyB,IAAbgc,EAAIla,MAAY,IAAM,uDAAuDka,EAAIla,UAC7F,KAAY,KAAyC2H,EARnC,IAQwD,IACtE,wEAAeA,wBACI,MAAnBE,GACA,KAAY,KAAWD,IAAM,IACzB,wEAAmBC,iBAA+BD,OAE1D,MAAMpG,EAAS,CAAEzD,EAAGmc,GACdzY,EAAQ,CAAEiG,aAAYC,UAASC,MAAKC,mBAEpCX,EAAM,KAAOxF,UAAU,KAASF,EAAQC,GAC9C,OAAI6N,GACO,OAAQpI,EAAK,CAACA,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,KAExDgJ,CACX,G,wGClBO,MAAMiT,GAAW,E,SAAAra,IAAG,CAAEsa,UAP7B,SAAmBnX,EAAGR,GAClB,IAAIqD,GAAK,QAAgB7C,EAAG,IAAK,YAC7B8C,GAAK,QAAgBtD,EAAG,IAAK,aAChCqD,EAAIC,IAAM,IAAAC,gBAAeF,EAAIC,GAC9B,MAAMvE,EAAS,CAAEyB,EAAG6C,EAAIrD,EAAGsD,GAC3B,OAAO,KAAOrE,UAAU,KAAUF,EACtC,G,wICJO,MAAM6Y,GAAqB,IAAAva,IAAG,CAAEwa,oBAXvC,SAA6BzR,EAAQhH,EAAaiH,EAASE,EAAY,IAAUC,wBAC7E,MAAMC,GAAU,QAAgBL,EAAQ,SAAU,sBAC5C7G,GAAe,QAAgBH,EAAa,cAAe,sBACjE,IAAIsH,EAAW,KACA,MAAXL,IACAK,GAAW,QAAgBL,EAAS,UAAW,wBAEnD,QAAkBI,EAAQhL,MAAO8D,EAAa9D,MAAO,iCACrD,MAAMsL,GAAS,QAAI,OAAIN,EAASlH,IAChC,OAAO,OAAoBwH,EAAQL,EAAUH,EACjD,G,6FCVO,MAAMuR,GAAa,E,SAAAza,IAAG,CAAE0a,YAL/B,SAAqBzc,GACjB,MACMyD,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,aAAc,SAEjD,OAAO,KAAO2D,UAAU,KAAYF,EACxC,G,+JCcO,MAAMiZ,GAAU,IAAA3a,IAAG,CAAE4a,SAZ5B,SAAkB3c,EAAGwI,EAAO,KAAMC,GAAW,GACzCzI,GAAI,QAAgBA,EAAG,IAAK,WAC5B,MAAMgJ,GAAO,QAAeR,EAAMxI,EAAEG,OAC9Byc,GAAQ,OAAK5c,EAAGgJ,EAAMP,GAC5B,IAAIoU,EAAgBD,EAAMzc,MACrBsI,IACDoU,GAAgB,QAAqBD,EAAMzc,MAAO6I,IAEtD,MAAM8T,GAAa,QAAO,QAAI,OAAK9c,EAAG,YAAY,OAAQ4c,EAAOC,KAEjE,MAAO,CAAEvU,KAAMsU,EAAOG,UADL,OAAKD,EAAY9T,EAAMP,GAE5C,G,iOCqIO,MAAMuU,GAAkB,IAAAjb,IAAG,CAAEkb,sBArGpC,UAA+B,EAAEjd,EAAC,OAAEkd,EAAM,QAAEtT,EAAO,IAAEC,EAAG,WAAEE,EAAa,OAAM,UAAEoT,EAAY,CAAC,EAAG,GAAE,gBAAErT,EAAe,KAAE8C,EAAI,WAAEC,EAAa,SAAQ,uBAAEC,EAAsB,eAAEC,IACrK,IAA2D,KAAvD,QAAW,KAAOC,MAAMC,cAAeJ,GAAuB,CAC9D,IAAIK,GAAS,OAAuBlN,EAAGkd,EAAQtT,EAASC,EAAKE,EAAYoT,EAAWrT,GAIpF,OAHY,MAAR8C,IACAM,GAAS,OAAIA,EAAQN,KAElB,QAAgBM,EAAQL,EAAYC,EAAwBC,EACvE,CACA,MAAMhE,GAAK,QAAgB/I,EAAG,IAAK,mBAC7Bod,GAAU,QAAgBF,EAAQ,SAAU,mBAClD,IAAIf,EAAMpT,EACNwI,GAAe,EACH,IAAZxI,EAAG9G,OACHsP,GAAe,EACf4K,GAAM,OAAQpT,EAAI,CAAC,EAAGA,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,MAE7D,KAAyB,IAAbgc,EAAIla,MAAY,IACxB,sEAAQka,EAAIla,UAChB,KAA6B,IAAjBmb,EAAQnb,MAAY,IAC5B,uEAAgBmb,EAAQnb,UAC5B,KAAYka,EAAIhc,MAAM,KAAOid,EAAQjd,MAAM,IAAI,IAC3C,6DAAIgc,EAAIhc,MAAM,qDACJid,EAAQjd,MAAM,QACX,MAAbgd,IACAA,EAAY,CAAC,EAAG,IAEpB,KAAY,KAAyCvT,EAASuT,IAAY,IACtE,sFAAqBvT,oBAA0BuT,OAC5B,MAAnBrT,GACA,KAAY,KAAWD,IAAM,IACzB,qFAAyBC,iBAA+BD,OAEhE,MAAMwT,EAAW,KAA4BlB,EAAIhc,MAAOid,EAAQjd,MAAOyJ,EAASuT,EAAWtT,EAAKC,GAAiB,GACjH,IAAIiE,EAMAC,EALQ,MAARpB,IACAmB,GAAQ,QAAgBnB,EAAM,OAAQ,iBACrCmB,IAAS,IAAA9F,gBAAe8F,EAAOhF,GAChC,KAA0CsU,EAAS1P,SAAUI,EAAM5N,QAGzC,MAA1B2M,IACAkB,GAA0B,QAAgBlB,EAAwB,gBAAiB,0BAEvF,MAAMmB,EAAO,CAACnC,EAAIoC,KACd,KAAY,KAA4BiP,IAAY,IAEhD,mHAAIA,OACR,MAAOC,EAASjB,EAAKhS,EAAGyC,GAAQsB,EAC1BC,GAAe,QAAqBrC,EAAI3B,EAAG0C,GAC3CyQ,GAAO,OAAmCnB,EAAIhc,MAAOgO,EAAciP,EAASxT,EAASC,EAAKsT,EAAWrT,GACrGyT,GAAY,OAAoCpB,EAAKhO,EAAciP,EAAQjd,MAAOyJ,EAASC,EAAKsT,EAAWrT,GACjH,GAAY,MAAR8C,EAAc,CAEd,MAAO,CAAC0Q,EAAMC,GADE,QAAqBxP,EAAOI,GAEhD,CACA,MAAO,CAACmP,EAAMC,EAAU,EAEtB9Z,EAAS,CACXzD,EAAGmc,EACHe,OAAQE,EACRxQ,KAAMmB,EACNjB,uBAAwBkB,GAEtBtK,EAAQ,CACVkG,UACAC,MACAE,aACAoT,YACArT,kBACA+C,aACAE,kBAIJ,GAAY,MAARH,EAAc,CACd,MAAM0B,GAAW,SAAW,CAAC6N,EAAKe,EAAQ3O,KAEtC,IAAIpF,EAAM,KAAOxF,UAAU,KAAsBF,EAAQC,GAMzD,OALA6K,EAAK,CAAC2O,EAAQf,EAAKhT,IACfoI,IAEApI,GAAM,OAAQA,EAAK,CAACA,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,MAEvD,CAAEkC,MAAO8G,EAAKqF,SAAUP,EAAM,IAEzC,OAAOK,EAAS6N,EAAKiB,EACzB,CACK,CACD,MAAM3O,GAAmB,SAAW,CAAC0N,EAAKe,EAAQtQ,EAAM2B,KAEpD,IAAIpF,EAAM,KAAOxF,UAAU,KAAsBF,EAAQC,GAMzD,OALA6K,EAAK,CAAC2O,EAAQf,EAAKhT,EAAKyD,IACpB2E,IAEApI,GAAM,OAAQA,EAAK,CAACA,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,MAEvD,CAAEkC,MAAO8G,EAAKqF,SAAUP,EAAM,IAEzC,OAAOQ,EAAiB0N,EAAKiB,EAASrP,EAC1C,CACJ,G,iOCMO,MAAMyP,GAAS,IAAAzb,IAAG,CAAE0b,aApG3B,UAAsB,EAAEzd,EAAC,OAAEkd,EAAM,QAAEtT,EAAO,IAAEC,EAAG,WAAEE,EAAa,OAAM,UAAEoT,EAAY,CAAC,EAAG,GAAE,gBAAErT,EAAe,KAAE8C,EAAI,WAAEC,EAAa,SAAQ,uBAAEC,EAAsB,eAAEC,IAE5J,GADAF,EAAaA,GAAc,UACgC,KAAvD,QAAW,KAAOG,MAAMC,cAAeJ,GAAuB,CAC9D,IAAIK,GAAS,OAAclN,EAAGkd,EAAQtT,EAASC,EAAKE,EAAYoT,EAAWrT,GAI3E,OAHY,MAAR8C,IACAM,GAAS,OAAIA,EAAQN,KAElB,QAAgBM,EAAQL,EAAYC,EAAwBC,EACvE,CACA,MAAMhE,GAAK,QAAgB/I,EAAG,IAAK,UAC7Bod,GAAU,QAAgBF,EAAQ,SAAU,UAClD,IAAIf,EAAMpT,EACNwI,GAAe,EACH,IAAZxI,EAAG9G,OACHsP,GAAe,EACf4K,GAAM,OAAQpT,EAAI,CAAC,EAAGA,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,MAE7D,KAAyB,IAAbgc,EAAIla,MAAY,IACxB,6DAAGka,EAAIla,UACX,KAA6B,IAAjBmb,EAAQnb,MAAY,IAC5B,8DAAGmb,EAAQnb,UACQ,MAAnB6H,GACA,KAAY,KAAWD,IAAM,IACzB,6EAAmBC,iBAA+BD,OAE1D,KAAYsS,EAAIhc,MAAM,KAAOid,EAAQjd,MAAM,IAAI,IAAM,oCAAoCgc,EAAIhc,MAAM,yCACrEid,EAAQjd,MAAM,QAC5C,KAAY,KAAyCyJ,EAASuT,IAAY,IACtE,uEAAevT,oBAA0BuT,OAC7C,KAA2B,SAAfpT,GAAuB,IAAM,sCAAsCA,4CAC/E,MAAMsT,EAAW,KAA4BlB,EAAIhc,MAAOid,EAAQjd,MAAOyJ,EAASuT,EAAWtT,EAAKC,GAChG,IAAIiE,EAMAC,EALQ,MAARpB,IACAmB,GAAQ,QAAgBnB,EAAM,OAAQ,iBACrCmB,IAAS,IAAA9F,gBAAe8F,EAAOhF,GAChC,KAA0CsU,EAAS1P,SAAUI,EAAM5N,QAGzC,MAA1B2M,IACAkB,GAA0B,QAAgBlB,EAAwB,gBAAiB,iBAEvF,MAAMmB,EAAO,CAACnC,EAAIoC,KACd,MAAOkP,EAASjB,EAAKhS,EAAG4D,GAASG,EAC3BC,GAAe,QAAqBrC,EAAI3B,EAAG0C,GACjD,KAAY,KAA4BsQ,IAAY,IAEhD,uHAAsDA,OAC1D,MAEMO,EAAM,EAFC,OAAoBvB,EAAIhc,MAAOgO,EAAciP,EAASxT,EAASC,IAC1D,OAAqBsS,EAAKhO,EAAciP,EAAQjd,MAAOyJ,EAASC,IAElF,GAAa,MAATkE,EAAe,CACf,MAAM4P,GAAU,QAAqB5P,EAAOI,GAC5CuP,EAAI5a,KAAK6a,EACb,CACA,OAAOD,CAAG,EAERja,EAAS,CACXzD,EAAGmc,EACHe,OAAQE,EACRxQ,KAAMmB,EACNjB,uBAAwBkB,GAEtBtK,EAAQ,CACVkG,UACAC,MACAE,aACAoT,YACArT,kBACA+C,aACAE,kBAIJ,GAAY,MAARH,EAAc,CACd,MAAM0B,GAAW,SAAW,CAAC6N,EAAKe,EAAQ3O,KACtC,IAAIpF,EAEJ,KAAOxF,UAAU,KAAaF,EAAQC,GAMtC,OALA6K,EAAK,CAAC2O,EAAQf,EAAKhT,IACfoI,IAEApI,GAAM,OAAQA,EAAK,CAACA,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,MAEvD,CAAEkC,MAAO8G,EAAKqF,SAAUP,EAAM,IAEzC,OAAOK,EAAS6N,EAAKiB,EACzB,CACK,CACD,MAAM3O,GAAmB,SAAW,CAAC0N,EAAKe,EAAQtQ,EAAM2B,KACpD,IAAIpF,EAAM,KAAOxF,UAAU,KAAaF,EAAQC,GAMhD,OALA6K,EAAK,CAAC2O,EAAQf,EAAKhT,EAAKyD,IACpB2E,IAEApI,GAAM,OAAQA,EAAK,CAACA,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,MAEvD,CAAEkC,MAAO8G,EAAKqF,SAAUP,EAAM,IAEzC,OAAOQ,EAAiB0N,EAAKiB,EAASrP,EAC1C,CACJ,G,8HC/HO,MAAM6P,GAA6B,IAAA7b,IAAG,CAAE8b,4BAvB/C,SAAqC7d,EAAGuY,EAAc,EAAG3L,EAAO,EAAGgJ,EAAQ,EAAG4C,EAAO,IACjF,MAAMzP,GAAK,QAAgB/I,EAAG,IAAK,8BACnC,KAAwB,IAAZ+I,EAAG9G,MAA0B,IAAZ8G,EAAG9G,MAAY,IAAM,2FAChC8G,EAAG9G,UACrB,KAAY,KAAWsW,IAAc,IACjC,2FAA+BA,OACnC,IAAI4D,EAAMpT,EACNwI,GAAe,EACH,IAAZxI,EAAG9G,OACHsP,GAAe,EACf4K,GAAM,OAAQpT,EAAI,CAAC,EAAGA,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,GAAI4I,EAAG5I,MAAM,MAE7D,MAAMsD,EAAS,CAAEzD,EAAGmc,GACdzY,EAAQ,CAAE6U,cAAa3L,OAAMgJ,QAAO4C,QAEpCrP,EAAM,KAAOxF,UAAU,KAAKF,EAAQC,GAC1C,OAAI6N,GACO,OAAQpI,EAAK,CAACA,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,GAAIgJ,EAAIhJ,MAAM,KAGpDgJ,CAEf,G,wGCKO,MAAM2U,GAA6B,E,SAAA/b,IAAG,CAAEgc,4BAd/C,SAAqC1Y,EAAOC,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBkB,EAAe,GAC7I,MAAMhB,GAAS,QAAgBR,EAAO,QAAS,qBACzCS,GAAU,QAAgBR,EAAQ,SAAU,qBAC5CS,GAAS,OAAsBF,EAAQC,EAASP,EAAeC,EAAcC,EAAgBoB,GAK7FpD,EAAS,CAAE4B,MAAOQ,EAAQP,OAAQQ,GAClCpC,EAAQ,CAAE6B,cALhBA,EAAgBQ,EAAOR,cAKQC,aAJ/BA,EAAeO,EAAOP,aAIuBC,eAH7CA,EAAiBM,EAAON,eAGqCoB,aAF7DA,EAAed,EAAOc,cAIhBqG,EAAS,KAAOvJ,UAAU,KAAqBF,EAAQC,GAC7D,MAAO,CAAE6C,gBAAiB2G,EAAO,GAAInG,eAAgBmG,EAAO,GAChE,G,wICTO,MAAM8Q,GAAa,IAAAjc,IAAG,CAAEkc,YAlB/B,SAAqBje,GACjB,MAAM+I,GAAK,QAAgB/I,EAAG,IAAK,cAenC,OAXiB,SAAYA,IASlB,CAAEqC,OALK,QAAI,QAAS,OAAIrC,KAKfwO,SAJE1C,IACD,OAAIA,GAAI,QAAQ,OAAI9L,QAKlCsO,CAASvF,EACpB,G,kFCrBA,SAASmV,EAAK/d,EAAOkC,EAAOuG,GACxB,MAAMlF,EAAQ,CAAEvD,QAAOkC,QAAOuG,SAC9B,OAAO,KAAOjF,UAAU,KAAM,CAAC,EAAGD,EACtC,C,6FCGO,MAAMya,GAAM,E,SAAApc,IAAG,CAAEqc,KALxB,SAAcpe,GACV,MACMyD,EAAS,CAAEzD,GADN,QAAgBA,EAAG,IAAK,QAEnC,OAAO,KAAO2D,UAAU,KAAKF,EACjC,G,wKCoDO,MAAM4a,GAAa,IAAAtc,IAAG,CAAEuc,YA1C/B,SAAqBlL,EAAQ5K,GAAO,GAChC,MAAM+K,GAAU,QAAgBH,EAAQ,SAAU,cAIlD,IAHc,IAAV5K,IACAA,EAAO+K,EAAQtR,KAAO,GAEtBuG,IAAS+K,EAAQtR,KAAO,EACxB,MAAM6N,MACF,gFAAmByD,EAAQtR,qBAAqBuG,KA2BxD,OAdiB,SAAW,CAAC4K,EAAQ7E,KACjC,MACMtF,GAAO,OAAImK,EAAQ5K,GAAM,GACzB+V,GAAU,OAAInL,EAAQnK,GACtB5G,GAAQ,QAAI,OAAKkc,EAAS,YAAY,QAAI,QAAI,OAAIA,GAAU/V,GAHjD,KAIjB+F,EAAK,CAAClM,IAON,MAAO,CAAEA,QAAOmM,SANC,CAAC1C,EAAIoC,KAClB,MAAO7L,GAAS6L,EAEVsQ,GAAU,OAAInc,GACpB,OAAO,OAAIyJ,GAAI,QAAI,OAAIA,EAAItD,GAFV,GAE2BgW,GAAS,EAE/B,GAEvBlQ,CAASiF,EAOpB,G","sources":["webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/linalg/qr.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/transform.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/in_top_k.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_padded_async.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/is_nan.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_with_score_async.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/multi_rnn_cell.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/mul.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/logical_or.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/log1p.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/mean.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/minimum.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/log_sum_exp.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/greater_equal.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/logical_and.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_3d.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/meshgrid.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/floor.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/logical_xor.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/losses/log_loss.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/losses/mean_squared_error.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_grad.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/loss_ops_utils.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_3d_grad.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/fused/mat_mul.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/maximum.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/crop_and_resize.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/is_inf.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/imag.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/linalg/band_part.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/losses/cosine_distance.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/losses/huber_loss.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/max.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/resize_nearest_neighbor.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/log.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/resize_bilinear.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_with_argmax.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/linspace.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/gather.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/gather_nd.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/less_equal.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/min.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/losses/sigmoid_cross_entropy.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/mod.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/less.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/greater.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/rotate_with_offset.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/mat_mul.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/moving_average.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/mirror_pad.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/leaky_relu.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/is_finite.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/fused_util.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/losses/compute_weighted_loss.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/losses/softmax_cross_entropy.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_async.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/losses/hinge_loss.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/multinomial.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/local_response_normalization_backprop.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/flip_left_right.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/linalg/gram_schmidt.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/threshold.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/gather_nd_util.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_padded.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/floorDiv.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/losses/absolute_difference.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/logical_not.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/moments.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/fused/depthwise_conv2d.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/fused/conv2d.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/local_response_normalization.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_with_score.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/log_sigmoid.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/fill.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/neg.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-core/dist/ops/log_softmax.js"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { dispose } from '../../globals';\nimport { assert } from '../../util';\nimport { clone } from '../clone';\nimport { concat } from '../concat';\nimport { div } from '../div';\nimport { eye } from '../eye';\nimport { greater } from '../greater';\nimport { matMul } from '../mat_mul';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { norm } from '../norm';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { slice } from '../slice';\nimport { stack } from '../stack';\nimport { sub } from '../sub';\nimport { tensor2d } from '../tensor2d';\nimport { transpose } from '../transpose';\nimport { unstack } from '../unstack';\nimport { where } from '../where';\n/**\n * Compute QR decomposition of m-by-n matrix using Householder transformation.\n *\n * Implementation based on\n *   [http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf]\n * (http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf)\n *\n * ```js\n * const a = tf.tensor2d([[1, 2], [3, 4]]);\n * let [q, r] = tf.linalg.qr(a);\n * console.log('Q');\n * q.print();\n * console.log('R');\n * r.print();\n * console.log('Orthogonalized');\n * q.dot(q.transpose()).print()  // should be nearly the identity matrix.\n * console.log('Reconstructed');\n * q.dot(r).print(); // should be nearly [[1, 2], [3, 4]];\n * ```\n *\n * @param x The `tf.Tensor` to be QR-decomposed. Must have rank >= 2. Suppose\n *   it has the shape `[..., M, N]`.\n * @param fullMatrices An optional boolean parameter. Defaults to `false`.\n *   If `true`, compute full-sized `Q`. If `false` (the default),\n *   compute only the leading N columns of `Q` and `R`.\n * @returns An `Array` of two `tf.Tensor`s: `[Q, R]`. `Q` is a unitary matrix,\n *   i.e., its columns all have unit norm and are mutually orthogonal.\n *   If `M >= N`,\n *     If `fullMatrices` is `false` (default),\n *       - `Q` has a shape of `[..., M, N]`,\n *       - `R` has a shape of `[..., N, N]`.\n *     If `fullMatrices` is `true` (default),\n *       - `Q` has a shape of `[..., M, M]`,\n *       - `R` has a shape of `[..., M, N]`.\n *   If `M < N`,\n *     - `Q` has a shape of `[..., M, M]`,\n *     - `R` has a shape of `[..., M, N]`.\n * @throws If the rank of `x` is less than 2.\n *\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\nfunction qr_(x, fullMatrices = false) {\n    assert(x.rank >= 2, () => `qr() requires input tensor to have a rank >= 2, but got rank ${x.rank}`);\n    if (x.rank === 2) {\n        return qr2d(x, fullMatrices);\n    }\n    else {\n        // Rank > 2.\n        // TODO(cais): Below we split the input into individual 2D tensors,\n        //   perform QR decomposition on them and then stack the results back\n        //   together. We should explore whether this can be parallelized.\n        const outerDimsProd = x.shape.slice(0, x.shape.length - 2)\n            .reduce((value, prev) => value * prev);\n        const x2ds = unstack(reshape(x, [\n            outerDimsProd, x.shape[x.shape.length - 2],\n            x.shape[x.shape.length - 1]\n        ]), 0);\n        const q2ds = [];\n        const r2ds = [];\n        x2ds.forEach(x2d => {\n            const [q2d, r2d] = qr2d(x2d, fullMatrices);\n            q2ds.push(q2d);\n            r2ds.push(r2d);\n        });\n        const q = reshape(stack(q2ds, 0), x.shape);\n        const r = reshape(stack(r2ds, 0), x.shape);\n        return [q, r];\n    }\n}\nfunction qr2d(x, fullMatrices = false) {\n    return ENGINE.tidy(() => {\n        assert(x.shape.length === 2, () => `qr2d() requires a 2D Tensor, but got a ${x.shape.length}D Tensor.`);\n        const m = x.shape[0];\n        const n = x.shape[1];\n        let q = eye(m); // Orthogonal transform so far.\n        let r = clone(x); // Transformed matrix so far.\n        const one2D = tensor2d([[1]], [1, 1]);\n        let w = clone(one2D);\n        const iters = m >= n ? n : m;\n        for (let j = 0; j < iters; ++j) {\n            // This tidy within the for-loop ensures we clean up temporary\n            // tensors as soon as they are no longer needed.\n            const rTemp = r;\n            const wTemp = w;\n            const qTemp = q;\n            [w, r, q] = ENGINE.tidy(() => {\n                // Find H = I - tau * w * w', to put zeros below R(j, j).\n                const rjEnd1 = slice(r, [j, j], [m - j, 1]);\n                const normX = norm(rjEnd1);\n                const rjj = slice(r, [j, j], [1, 1]);\n                // The sign() function returns 0 on 0, which causes division by zero.\n                const s = where(greater(rjj, 0), tensor2d([[-1]]), tensor2d([[1]]));\n                const u1 = sub(rjj, mul(s, normX));\n                const wPre = div(rjEnd1, u1);\n                if (wPre.shape[0] === 1) {\n                    w = clone(one2D);\n                }\n                else {\n                    w = concat([\n                        one2D,\n                        slice(wPre, [1, 0], [wPre.shape[0] - 1, wPre.shape[1]])\n                    ], 0);\n                }\n                const tau = neg(div(matMul(s, u1), normX));\n                // -- R := HR, Q := QH.\n                const rjEndAll = slice(r, [j, 0], [m - j, n]);\n                const tauTimesW = mul(tau, w);\n                const wT = transpose(w);\n                if (j === 0) {\n                    r = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));\n                }\n                else {\n                    const rTimesTau = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));\n                    r = concat([slice(r, [0, 0], [j, n]), rTimesTau], 0);\n                }\n                const tawTimesWT = transpose(tauTimesW);\n                const qAllJEnd = slice(q, [0, j], [m, q.shape[1] - j]);\n                if (j === 0) {\n                    q = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));\n                }\n                else {\n                    const qTimesTau = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));\n                    q = concat([slice(q, [0, 0], [m, j]), qTimesTau], 1);\n                }\n                return [w, r, q];\n            });\n            dispose([rTemp, wTemp, qTemp]);\n        }\n        if (!fullMatrices && m > n) {\n            q = slice(q, [0, 0], [m, n]);\n            r = slice(r, [0, 0], [n, n]);\n        }\n        return [q, r];\n    });\n}\nexport const qr = op({ qr_ });\n//# sourceMappingURL=qr.js.map","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { Transform } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\n/**\n * Applies the given transform(s) to the image(s).\n *\n * @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.\n * @param transforms Projective transform matrix/matrices. A tensor1d of length\n *     8 or tensor of size N x 8. If one row of transforms is [a0, a1, a2, b0\n *     b1, b2, c0, c1], then it maps the output point (x, y) to a transformed\n *     input point (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k),\n *     where k = c0 x + c1 y + 1. The transforms are inverted compared to the\n *     transform mapping input points to output points.\n * @param interpolation Interpolation mode.\n *     Supported values: 'nearest', 'bilinear'. Default to 'nearest'.\n * @param fillMode Points outside the boundaries of the input are filled\n *     according to the given mode, one of 'constant', 'reflect', 'wrap',\n *     'nearest'. Default to 'constant'.\n *     'reflect': (d c b a | a b c d | d c b a ) The input is extended by\n *     reflecting about the edge of the last pixel.\n *     'constant': (k k k k | a b c d | k k k k) The input is extended by\n *     filling all values beyond the edge with the same constant value k.\n *     'wrap': (a b c d | a b c d | a b c d) The input is extended by\n *     wrapping around to the opposite edge.\n *     'nearest': (a a a a | a b c d | d d d d) The input is extended by\n *     the nearest pixel.\n * @param fillValue A float represents the value to be filled outside the\n *     boundaries when fillMode is 'constant'.\n * @param Output dimension after the transform, [height, width]. If undefined,\n *     output is the same size as input image.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction transform_(image, transforms, interpolation = 'nearest', fillMode = 'constant', fillValue = 0, outputShape) {\n    const $image = convertToTensor(image, 'image', 'transform', 'float32');\n    const $transforms = convertToTensor(transforms, 'transforms', 'transform', 'float32');\n    util.assert($image.rank === 4, () => 'Error in transform: image must be rank 4,' +\n        `but got rank ${$image.rank}.`);\n    util.assert($transforms.rank === 2 &&\n        ($transforms.shape[0] === $image.shape[0] ||\n            $transforms.shape[0] === 1) &&\n        $transforms.shape[1] === 8, () => `Error in transform: Input transform should be batch x 8 or 1 x 8`);\n    util.assert(outputShape == null || outputShape.length === 2, () => 'Error in transform: outputShape must be [height, width] or null, ' +\n        `but got ${outputShape}.`);\n    const inputs = { image: $image, transforms: $transforms };\n    const attrs = { interpolation, fillMode, fillValue, outputShape };\n    return ENGINE.runKernel(Transform, inputs, attrs);\n}\nexport const transform = op({ transform_ });\n//# sourceMappingURL=transform.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../tensor_util_env';\nimport { assert, assertShapesMatch, getTypedArrayFromDType } from '../util';\nimport { tensor } from './tensor';\n/**\n * Returns whether the targets are in the top K predictions.\n *\n * ```js\n * const predictions = tf.tensor2d([[20, 10, 40, 30], [30, 50, -20, 10]]);\n * const targets = tf.tensor1d([2, 0]);\n * const precision = await tf.inTopKAsync(predictions, targets);\n * precision.print();\n * ```\n * @param predictions 2-D or higher `tf.Tensor` with last dimension being\n *     at least `k`.\n * @param targets 1-D or higher `tf.Tensor`.\n * @param k Optional Number of top elements to look at for computing precision,\n *     default to 1.\n *\n * @doc {heading: 'Operations', subheading: 'Evaluation'}\n */\nasync function inTopKAsync_(predictions, targets, k = 1) {\n    const $predictions = convertToTensor(predictions, 'predictions', 'inTopK');\n    const $targets = convertToTensor(targets, 'targets', 'inTopK');\n    assert($predictions.rank > 1, () => 'inTopK() expects the predictions to be of rank 2 or higher, ' +\n        `but got ${$predictions.rank}`);\n    assert($predictions.rank - 1 === $targets.rank, () => `predictions rank should be 1 larger than ` +\n        `targets rank, but got predictions rank ` +\n        `${$predictions.rank} and targets rank ${$targets.rank}`);\n    assertShapesMatch($predictions.shape.slice(0, $predictions.shape.length - 1), $targets.shape, `predictions's shape should be align with the targets' shape, ` +\n        'except the last dimension.');\n    const lastDim = $predictions.shape[$predictions.shape.length - 1];\n    assert(k > 0 && k <= lastDim, () => `'k' passed to inTopK() must be > 0 && <= the predictions last ` +\n        `dimension (${lastDim}), but got ${k}`);\n    const predictionsVals = await $predictions.data();\n    const targetsVals = await $targets.data();\n    // Reshape predictionsVals into a 2d tensor [batch, lastDim]\n    // and look up topK along lastDim.\n    const [batch, size] = [predictionsVals.length / lastDim, lastDim];\n    const precision = getTypedArrayFromDType('bool', batch);\n    for (let b = 0; b < batch; b++) {\n        const offset = b * size;\n        const vals = predictionsVals.subarray(offset, offset + size);\n        const valAndInd = [];\n        for (let i = 0; i < vals.length; i++) {\n            valAndInd.push({ value: vals[i], index: i });\n        }\n        valAndInd.sort((a, b) => b.value - a.value);\n        precision[b] = 0;\n        for (let i = 0; i < k; i++) {\n            if (valAndInd[i].index === targetsVals[b]) {\n                precision[b] = 1;\n                break;\n            }\n        }\n    }\n    if (predictions !== $predictions) {\n        $predictions.dispose();\n    }\n    if (targets !== $targets) {\n        $targets.dispose();\n    }\n    // Output precision has the same shape as targets.\n    return tensor(precision, $targets.shape, 'bool');\n}\nexport const inTopKAsync = inTopKAsync_;\n//# sourceMappingURL=in_top_k.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { nonMaxSuppressionV4Impl } from '../../backends/non_max_suppression_impl';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { scalar } from '../scalar';\nimport { tensor1d } from '../tensor1d';\n/**\n * Asynchronously performs non maximum suppression of bounding boxes based on\n * iou (intersection over union), with an option to pad results.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param padToMaxOutputSize Defalts to false. If true, size of output\n *     `selectedIndices` is padded to maxOutputSize.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - validOutputs: A scalar denoting how many elements in `selectedIndices`\n *       are valid. Valid elements occur first, then padding.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nasync function nonMaxSuppressionPaddedAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, padToMaxOutputSize = false) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, null /* softNmsSigma */);\n    const $maxOutputSize = params.maxOutputSize;\n    const $iouThreshold = params.iouThreshold;\n    const $scoreThreshold = params.scoreThreshold;\n    const [boxesVals, scoresVals] = await Promise.all([$boxes.data(), $scores.data()]);\n    // We call a cpu based impl directly with the typedarray data here rather\n    // than a kernel because all kernels are synchronous (and thus cannot await\n    // .data()).\n    const { selectedIndices, validOutputs } = nonMaxSuppressionV4Impl(boxesVals, scoresVals, $maxOutputSize, $iouThreshold, $scoreThreshold, padToMaxOutputSize);\n    if ($boxes !== boxes) {\n        $boxes.dispose();\n    }\n    if ($scores !== scores) {\n        $scores.dispose();\n    }\n    return {\n        selectedIndices: tensor1d(selectedIndices, 'int32'),\n        validOutputs: scalar(validOutputs, 'int32')\n    };\n}\nexport const nonMaxSuppressionPaddedAsync = nonMaxSuppressionPaddedAsync_;\n//# sourceMappingURL=non_max_suppression_padded_async.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { IsNan } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * RReturns which elements of x are NaN.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isNaN().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction isNaN_(x) {\n    const $x = convertToTensor(x, 'x', 'isNaN');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(IsNan, inputs);\n}\nexport const isNaN = op({ isNaN_ });\n//# sourceMappingURL=is_nan.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { nonMaxSuppressionV5Impl } from '../../backends/non_max_suppression_impl';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { tensor1d } from '../tensor1d';\n/**\n * Asynchronously performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * This op also supports a Soft-NMS mode (c.f.\n * Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score\n * of other overlapping boxes, therefore favoring different regions of the image\n * with high scores. To enable this Soft-NMS mode, set the `softNmsSigma`\n * parameter to be larger than 0.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param softNmsSigma A float representing the sigma parameter for Soft NMS.\n *     When sigma is 0, it falls back to nonMaxSuppression.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - selectedScores: A 1D tensor with the corresponding scores for each\n *       selected box.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nasync function nonMaxSuppressionWithScoreAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, softNmsSigma = 0.0) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);\n    maxOutputSize = params.maxOutputSize;\n    iouThreshold = params.iouThreshold;\n    scoreThreshold = params.scoreThreshold;\n    softNmsSigma = params.softNmsSigma;\n    const boxesAndScores = await Promise.all([$boxes.data(), $scores.data()]);\n    const boxesVals = boxesAndScores[0];\n    const scoresVals = boxesAndScores[1];\n    // We call a cpu based impl directly with the typedarray data  here rather\n    // than a kernel because all kernels are synchronous (and thus cannot await\n    // .data()).\n    const { selectedIndices, selectedScores } = nonMaxSuppressionV5Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);\n    if ($boxes !== boxes) {\n        $boxes.dispose();\n    }\n    if ($scores !== scores) {\n        $scores.dispose();\n    }\n    return {\n        selectedIndices: tensor1d(selectedIndices, 'int32'),\n        selectedScores: tensor1d(selectedScores)\n    };\n}\nexport const nonMaxSuppressionWithScoreAsync = nonMaxSuppressionWithScoreAsync_;\n//# sourceMappingURL=non_max_suppression_with_score_async.js.map","import { convertToTensor, convertToTensorArray } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the next states and outputs of a stack of LSTMCells.\n *\n * Each cell output is used as input to the next cell.\n *\n * Returns `[cellState, cellOutput]`.\n *\n * Derived from tf.contrib.rn.MultiRNNCell.\n *\n * @param lstmCells Array of LSTMCell functions.\n * @param data The input to the cell.\n * @param c Array of previous cell states.\n * @param h Array of previous cell outputs.\n *\n * @doc {heading: 'Operations', subheading: 'RNN'}\n */\nfunction multiRNNCell_(lstmCells, data, c, h) {\n    const $data = convertToTensor(data, 'data', 'multiRNNCell');\n    const $c = convertToTensorArray(c, 'c', 'multiRNNCell');\n    const $h = convertToTensorArray(h, 'h', 'multiRNNCell');\n    let input = $data;\n    const newStates = [];\n    for (let i = 0; i < lstmCells.length; i++) {\n        const output = lstmCells[i](input, $c[i], $h[i]);\n        newStates.push(output[0]);\n        newStates.push(output[1]);\n        input = output[1];\n    }\n    const newC = [];\n    const newH = [];\n    for (let i = 0; i < newStates.length; i += 2) {\n        newC.push(newStates[i]);\n        newH.push(newStates[i + 1]);\n    }\n    return [newC, newH];\n}\nexport const multiRNNCell = op({ multiRNNCell_ });\n//# sourceMappingURL=multi_rnn_cell.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Multiply } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Multiplies two `tf.Tensor`s element-wise, A * B. Supports broadcasting.\n *\n * We also expose `tf.mulStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3, 4]);\n * const b = tf.tensor1d([2, 3, 4, 5]);\n *\n * a.mul(b).print();  // or tf.mul(a, b)\n * ```\n *\n * ```js\n * // Broadcast mul a with b.\n * const a = tf.tensor1d([1, 2, 3, 4]);\n * const b = tf.scalar(5);\n *\n * a.mul(b).print();  // or tf.mul(a, b)\n * ```\n * @param a The first tensor to multiply.\n * @param b The second tensor to multiply. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction mul_(a, b) {\n    let $a = convertToTensor(a, 'a', 'mul');\n    let $b = convertToTensor(b, 'b', 'mul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Multiply, inputs);\n}\nexport const mul = op({ mul_ });\n//# sourceMappingURL=mul.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LogicalOr } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of `a OR b` element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([false, false, true, true], 'bool');\n * const b = tf.tensor1d([false, true, false, true], 'bool');\n *\n * a.logicalOr(b).print();\n * ```\n * @param a The first input tensor. Must be of dtype bool.\n * @param b The second input tensor. Must be of dtype bool.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalOr_(a, b) {\n    const $a = convertToTensor(a, 'a', 'logicalOr', 'bool');\n    const $b = convertToTensor(b, 'b', 'logicalOr', 'bool');\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(LogicalOr, inputs);\n}\nexport const logicalOr = op({ logicalOr_ });\n//# sourceMappingURL=logical_or.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Log1p } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes natural logarithm of the input `tf.Tensor` plus one\n * element-wise: `ln(1 + x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, Math.E - 1]);\n *\n * x.log1p().print();  // or tf.log1p(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction log1p_(x) {\n    const $x = convertToTensor(x, 'x', 'log1p');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(Log1p, inputs);\n}\nexport const log1p = op({ log1p_ });\n//# sourceMappingURL=log1p.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Mean } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the mean of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces `x` along the dimensions given in `axis`. Unless `keepDims` is\n * true, the rank of the `tf.Tensor` is reduced by 1 for each entry in `axis`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axis` has no entries, all dimensions are reduced, and a `tf.Tensor` with\n * a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.mean().print();  // or tf.mean(a)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.mean(axis).print();  // or tf.mean(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction mean_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'mean');\n    const inputs = { x: $x };\n    const attrs = { axis, keepDims };\n    return ENGINE.runKernel(Mean, inputs, attrs);\n}\nexport const mean = op({ mean_ });\n//# sourceMappingURL=mean.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Minimum } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { cast } from './cast';\nimport { op } from './operation';\n/**\n * Returns the min of a and b (`a < b ? a : b`) element-wise.\n * Supports broadcasting.\n *\n * We also expose `minimumStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.minimum(b).print();  // or tf.minimum(a, b)\n * ```\n *\n * ```js\n * // Broadcast minimum a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.minimum(b).print();  // or tf.minimum(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction minimum_(a, b) {\n    let $a = convertToTensor(a, 'a', 'minimum');\n    let $b = convertToTensor(b, 'b', 'minimum');\n    [$a, $b] = makeTypesMatch($a, $b);\n    if ($a.dtype === 'bool') {\n        $a = cast($a, 'int32');\n        $b = cast($b, 'int32');\n    }\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Minimum, inputs);\n}\nexport const minimum = op({ minimum_ });\n//# sourceMappingURL=minimum.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../tensor_util_env';\nimport { parseAxisParam } from '../util';\nimport { add } from './add';\nimport { expandShapeToKeepDim } from './axis_util';\nimport { exp } from './exp';\nimport { log } from './log';\nimport { max } from './max';\nimport { op } from './operation';\nimport { reshape } from './reshape';\nimport { sub } from './sub';\nimport { sum } from './sum';\n/**\n * Computes the log(sum(exp(elements across the reduction dimensions)).\n *\n * Reduces the input along the dimensions given in `axis`. Unless `keepDims`\n * is true, the rank of the array is reduced by 1 for each entry in `axis`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axis` has no entries, all dimensions are reduced, and an array with a\n * single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.logSumExp().print();  // or tf.logSumExp(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.logSumExp(axis).print();  // or tf.logSumExp(a, axis)\n * ```\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. If null (the default),\n *     reduces all dimensions.\n * @param keepDims If true, retains reduced dimensions with length\n *     of 1. Defaults to false.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction logSumExp_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'logSumExp');\n    const axes = parseAxisParam(axis, $x.shape);\n    const xMax = max($x, axes, true /* keepDims */);\n    const a = sub($x, xMax);\n    const b = exp(a);\n    const c = sum(b, axes);\n    const d = log(c);\n    const res = add(reshape(xMax, d.shape), d);\n    if (keepDims) {\n        const newShape = expandShapeToKeepDim(res.shape, axes);\n        return reshape(res, newShape);\n    }\n    return res;\n}\nexport const logSumExp = op({ logSumExp_ });\n//# sourceMappingURL=log_sum_exp.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { GreaterEqual } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a >= b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.greaterEqual(b).print();\n * ```\n *\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction greaterEqual_(a, b) {\n    let $a = convertToTensor(a, 'a', 'greaterEqual');\n    let $b = convertToTensor(b, 'b', 'greaterEqual');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(GreaterEqual, inputs);\n}\nexport const greaterEqual = op({ greaterEqual_ });\n//# sourceMappingURL=greater_equal.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LogicalAnd } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of `a AND b` element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([false, false, true, true], 'bool');\n * const b = tf.tensor1d([false, true, false, true], 'bool');\n *\n * a.logicalAnd(b).print();\n * ```\n *\n * @param a The first input tensor. Must be of dtype bool.\n * @param b The second input tensor. Must be of dtype bool.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalAnd_(a, b) {\n    const $a = convertToTensor(a, 'a', 'logicalAnd', 'bool');\n    const $b = convertToTensor(b, 'b', 'logicalAnd', 'bool');\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(LogicalAnd, inputs);\n}\nexport const logicalAnd = op({ logicalAnd_ });\n//# sourceMappingURL=logical_and.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPool3D } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Computes the 3D max pooling.\n *\n * ```js\n * const x = tf.tensor5d([1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 2, 2, 1]);\n * const result = tf.maxPool3d(x, 2, 1, 'valid');\n * result.print();\n * ```\n *\n * @param x The input tensor, of rank 5 or rank 4 of shape\n *     `[batch, depth, height, width, inChannels]`.\n * @param filterSize The filter size:\n *     `[filterDepth, filterHeight, filterWidth]`.\n *     If `filterSize` is a single number,\n *     then `filterDepth == filterHeight == filterWidth`.\n * @param strides The strides of the pooling:\n *     `[strideDepth, strideHeight, strideWidth]`.\n *     If `strides` is a single number,\n *     then `strideDepth == strideHeight == strideWidth`.\n * @param pad The type of padding algorithm.\n *    - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *    - `valid`: output will be smaller than input if filter is larger\n *       than 1*1x1.\n *    - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param dataFormat An optional string from: \"NDHWC\", \"NCDHW\". Defaults to\n *     \"NDHWC\". Specify the data format of the input and output data. With the\n *     default format \"NDHWC\", the data is stored in the order of: [batch,\n *     depth, height, width, channels]. Only \"NDHWC\" is currently supported.\n * @doc {heading: 'Operations', subheading: 'Convolution'}\n */\nfunction maxPool3d_(x, filterSize = [1, 1, 1], strides, pad, dimRoundingMode, dataFormat = 'NDHWC') {\n    const $x = convertToTensor(x, 'x', 'maxPool3d');\n    let x5D = $x;\n    let reshapedTo5D = false;\n    if ($x.rank === 4) {\n        reshapedTo5D = true;\n        x5D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2], $x.shape[3]]);\n    }\n    util.assert(x5D.rank === 5, () => `Error in maxPool3d: x must be rank 5 but got rank ${x5D.rank}.`);\n    util.assert(dataFormat === 'NDHWC', () => `Error in maxPool3d: Only NDHWC is currently supported, ` +\n        `but got dataFormat of ${dataFormat}`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in maxPool3d: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const inputs = { x: x5D };\n    const attrs = { filterSize, strides, pad, dimRoundingMode, dataFormat };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(MaxPool3D, inputs, attrs);\n    if (reshapedTo5D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3], res.shape[4]]);\n    }\n    return res;\n}\nexport const maxPool3d = op({ maxPool3d_ });\n//# sourceMappingURL=max_pool_3d.js.map","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { matMul } from './mat_mul';\nimport { ones } from './ones';\nimport { reshape } from './reshape';\nimport { Tensor } from '../tensor';\nimport { convertToTensor } from '../tensor_util_env';\nimport { sizeFromShape } from '../util_base';\n/**\n * Broadcasts parameters for evaluation on an N-D grid.\n *\n * Given N one-dimensional coordinate arrays `*args`, returns a list `outputs`\n * of N-D coordinate arrays for evaluating expressions on an N-D grid.\n *\n * Notes:\n * `meshgrid` supports cartesian ('xy') and matrix ('ij') indexing conventions.\n * When the `indexing` argument is set to 'xy' (the default), the broadcasting\n * instructions for the first two dimensions are swapped.\n * Examples:\n * Calling `const [X, Y] = meshgrid(x, y)` with the tensors\n *\n * ```javascript\n * const x = [1, 2, 3];\n * const y = [4, 5, 6];\n * const [X, Y] = tf.meshgrid(x, y);\n * // X = [[1, 2, 3],\n * //      [1, 2, 3],\n * //      [1, 2, 3]]\n * // Y = [[4, 4, 4],\n * //      [5, 5, 5],\n * //      [6, 6, 6]]\n * ```\n *\n * @param x Tensor with rank geq 1.\n * @param y Tensor with rank geq 1.\n * @param indexing\n *\n * @doc {heading: 'Operations', subheading: 'Slicing and Joining'}\n */\nexport function meshgrid(x, y, { indexing = 'xy' } = {}) {\n    if (indexing !== 'xy' && indexing !== 'ij') {\n        throw new TypeError(`${indexing} is not a valid third argument to meshgrid`);\n    }\n    if (x === undefined) {\n        return [];\n    }\n    let $x = convertToTensor(x, 'x', 'meshgrid', x instanceof Tensor ? x.dtype : 'float32');\n    if (y === undefined) {\n        return [$x];\n    }\n    let $y = convertToTensor(y, 'y', 'meshgrid', y instanceof Tensor ? y.dtype : 'float32');\n    const w = sizeFromShape($x.shape);\n    const h = sizeFromShape($y.shape);\n    if (indexing === 'xy') {\n        $x = reshape($x, [1, -1]);\n        $y = reshape($y, [-1, 1]);\n        return [\n            matMul(ones([h, 1], $x.dtype), $x),\n            matMul($y, ones([1, w], $y.dtype)),\n        ];\n    }\n    $x = reshape($x, [-1, 1]);\n    $y = reshape($y, [1, -1]);\n    return [\n        matMul($x, ones([1, h], $x.dtype)),\n        matMul(ones([w, 1], $y.dtype), $y),\n    ];\n}\n//# sourceMappingURL=meshgrid.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Floor } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes floor of input `tf.Tensor` element-wise: `floor(x)`.\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3]);\n *\n * x.floor().print();  // or tf.floor(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction floor_(x) {\n    const $x = convertToTensor(x, 'x', 'floor');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(Floor, inputs);\n}\nexport const floor = op({ floor_ });\n//# sourceMappingURL=floor.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { logicalAnd } from './logical_and';\nimport { logicalNot } from './logical_not';\nimport { logicalOr } from './logical_or';\nimport { op } from './operation';\n/**\n * Returns the truth value of `a XOR b` element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([false, false, true, true], 'bool');\n * const b = tf.tensor1d([false, true, false, true], 'bool');\n *\n * a.logicalXor(b).print();\n * ```\n *\n * @param a The first input tensor. Must be of dtype bool.\n * @param b The second input tensor. Must be of dtype bool.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalXor_(a, b) {\n    const $a = convertToTensor(a, 'a', 'logicalXor', 'bool');\n    const $b = convertToTensor(b, 'b', 'logicalXor', 'bool');\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    // x ^ y = (x | y) & ~(x & y)\n    return logicalAnd(logicalOr(a, b), logicalNot(logicalAnd(a, b)));\n}\nexport const logicalXor = op({ logicalXor_ });\n//# sourceMappingURL=logical_xor.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { add } from '../add';\nimport { log } from '../log';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the log loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param epsilon A small increment to avoid taking log of zero\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction logLoss_(labels, predictions, weights, epsilon = 1e-7, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'logLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'logLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'logLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in logLoss: ');\n    const one = scalar(1);\n    const epsilonScalar = scalar(epsilon);\n    const l1 = neg(mul($labels, log(add($predictions, epsilonScalar))));\n    const l2 = mul(sub(one, $labels), log(add(sub(one, $predictions), epsilonScalar)));\n    const losses = sub(l1, l2);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const logLoss = op({ logLoss_ });\n//# sourceMappingURL=log_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { Reduction } from '../loss_ops_utils';\nimport { op } from '../operation';\nimport { squaredDifference } from '../squared_difference';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the mean squared error between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction meanSquaredError_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'meanSquaredError');\n    const $predictions = convertToTensor(predictions, 'predictions', 'meanSquaredError');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'meanSquaredError');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in meanSquaredError: ');\n    const losses = squaredDifference($labels, $predictions);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const meanSquaredError = op({ meanSquaredError_ });\n//# sourceMappingURL=mean_squared_error.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPoolGrad } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\n/**\n * Computes the backprop of a 2D max pool.\n *\n * @param dy The dy error, of rank 4 or rank 3 of shape\n *     [batchSize, height, width, channels]. If rank 3, batch of 1 is\n * assumed.\n * @param input The original input image, of rank 4, of shape\n *     [batchSize, height, width, channels].\n * @param output The original output image, of rank 4, of shape\n *     [batchSize, outHeight, outWidth, channels].\n * @param filterSize The filter size: `[filterHeight, filterWidth]`. If\n *     `filterSize` is a single number, then `filterHeight == filterWidth`.\n * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If\n *     `strides` is a single number, then `strideHeight == strideWidth`.\n * @param pad A string from: 'same', 'valid'. The type of padding algorithm\n *     used in the forward prop of the op.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n */\nfunction maxPoolGrad_(dy, input, output, filterSize, strides, pad, dimRoundingMode) {\n    const $dy = convertToTensor(dy, 'dy', 'maxPoolGrad');\n    const $input = convertToTensor(input, 'input', 'maxPoolGrad');\n    const $output = convertToTensor(output, 'output', 'maxPoolGrad');\n    util.assert($input.rank === $dy.rank, () => `Rank of input (${$input.rank}) does not match rank of dy ` +\n        `(${$dy.rank})`);\n    util.assert($dy.rank === 4, () => `Error in maxPoolGrad: dy must be rank 4 but got rank ` +\n        `${$dy.rank}.`);\n    util.assert($input.rank === 4, () => `Error in maxPoolGrad: input must be rank 4 but got rank ` +\n        `${$input.rank}.`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in maxPoolGrad: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const inputs = { dy: $dy, input: $input, output: $output };\n    const attrs = { filterSize, strides, pad, dimRoundingMode };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    return ENGINE.runKernel(MaxPoolGrad, inputs, attrs);\n}\nexport const maxPoolGrad = op({ maxPoolGrad_ });\n//# sourceMappingURL=max_pool_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nexport var Reduction;\n(function (Reduction) {\n    Reduction[Reduction[\"NONE\"] = 0] = \"NONE\";\n    Reduction[Reduction[\"MEAN\"] = 1] = \"MEAN\";\n    Reduction[Reduction[\"SUM\"] = 2] = \"SUM\";\n    Reduction[Reduction[\"SUM_BY_NONZERO_WEIGHTS\"] = 3] = \"SUM_BY_NONZERO_WEIGHTS\";\n})(Reduction || (Reduction = {}));\n//# sourceMappingURL=loss_ops_utils.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPool3DGrad } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Computes the backprop of a 3d max pool.\n *\n * @param dy The dy error, of rank 5 of shape\n *     [batchSize, depth, height, width, channels].\n * assumed.\n * @param input The original input image, of rank 5 or rank 4 of shape\n *     [batchSize, depth, height, width, channels].\n * @param output The original output image, of rank 5 of shape\n *     [batchSize, outDepth, outHeight, outWidth, channels].\n * @param filterSize The filter size:\n *     `[filterDepth, filterHeight, filterWidth]`.\n *     `filterSize` is a single number,\n *     then `filterDepth == filterHeight == filterWidth`.\n * @param strides The strides of the pooling:\n *     `[strideDepth, strideHeight, strideWidth]`. If\n *     `strides` is a single number, then `strideHeight == strideWidth`.\n * @param pad A string from: 'same', 'valid'. The type of padding algorithm\n *     used in the forward prop of the op.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n */\nfunction maxPool3dGrad_(dy, input, output, filterSize, strides, pad, dimRoundingMode) {\n    const $dy = convertToTensor(dy, 'dy', 'maxPool3dGrad');\n    const $input = convertToTensor(input, 'input', 'maxPool3dGrad');\n    const $output = convertToTensor(output, 'output', 'maxPool3dGrad');\n    let dy5D = $dy;\n    let input5D = $input;\n    let output5D = $output;\n    let reshapedTo5D = false;\n    if ($input.rank === 4) {\n        reshapedTo5D = true;\n        dy5D = reshape($dy, [1, $dy.shape[0], $dy.shape[1], $dy.shape[2], $dy.shape[3]]);\n        input5D = reshape($input, [\n            1, $input.shape[0], $input.shape[1], $input.shape[2], $input.shape[3]\n        ]);\n        output5D = reshape($output, [\n            1, $output.shape[0], $output.shape[1], $output.shape[2], $output.shape[3]\n        ]);\n    }\n    util.assert(dy5D.rank === 5, () => `Error in maxPool3dGrad: dy must be rank 5 but got rank ` +\n        `${dy5D.rank}.`);\n    util.assert(input5D.rank === 5, () => `Error in maxPool3dGrad: input must be rank 5 but got rank ` +\n        `${input5D.rank}.`);\n    util.assert(output5D.rank === 5, () => `Error in maxPool3dGrad: output must be rank 5 but got rank ` +\n        `${output5D.rank}.`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in maxPool3dGrad: pad must be an integer when ` +\n            `using, dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const inputs = { dy: dy5D, input: input5D, output: output5D };\n    const attrs = { filterSize, strides, pad, dimRoundingMode };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(MaxPool3DGrad, inputs, attrs);\n    if (reshapedTo5D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3], res.shape[4]]);\n    }\n    return res;\n}\nexport const maxPool3dGrad = op({ maxPool3dGrad_ });\n//# sourceMappingURL=max_pool_3d_grad.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\nfunction fusedMatMul_({ a, b, transposeA = false, transposeB = false, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha, }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedMatMul(a, b, transposeA, transposeB);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    let $a = convertToTensor(a, 'a', 'fused matMul');\n    let $b = convertToTensor(b, 'b', 'fused matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n    const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n    util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at ` +\n        `least 2, got ranks ${$a.rank} and ${$b.rank}.`);\n    util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` +\n        `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} must match.`);\n    util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\n        `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} and transposeA=${transposeA}` +\n        ` and transposeB=${transposeB} must match.`);\n    const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n    const a3D = transposeA ?\n        reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n    const b3D = transposeB ?\n        reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused matMul');\n        [$bias] = makeTypesMatch($bias, $a);\n        broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n    }\n    const grad = (dy, saved) => {\n        const [a3D, b3D, y, $bias] = saved;\n        // we reshape dy because the result of the forward is not\n        // necessarily going to be a 3d tensor due to a reshape done at the end of\n        // the customOp.\n        const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n        let aDer;\n        let bDer;\n        if (!transposeA && !transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, true, false);\n        }\n        else if (!transposeA && transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, false);\n            bDer = unfusedMatMul(dyActivation, a3D, true, false);\n        }\n        else if (transposeA && !transposeB) {\n            aDer = unfusedMatMul(b3D, dyActivation, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, false, false);\n        }\n        else {\n            aDer = unfusedMatMul(b3D, dyActivation, true, true);\n            bDer = unfusedMatMul(dyActivation, a3D, true, true);\n        }\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [aDer, bDer, biasDer];\n        }\n        else {\n            return [aDer, bDer];\n        }\n    };\n    const inputs = {\n        a: a3D,\n        b: b3D,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = { transposeA, transposeB, activation, leakyreluAlpha };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((a3D, b3D, save) => {\n            const res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n            save([a3D, b3D, res]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOp(a3D, b3D);\n    }\n    else {\n        const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\n            const res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n            save([a3D, b3D, res, $bias]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOpWithBias(a3D, b3D, $bias);\n    }\n}\nexport const matMul = op({ fusedMatMul_ });\n//# sourceMappingURL=mat_mul.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Maximum } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { cast } from './cast';\nimport { op } from './operation';\n/**\n * Returns the max of a and b (`a > b ? a : b`) element-wise.\n * Supports broadcasting.\n *\n * We also expose `tf.maximumStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.maximum(b).print();  // or tf.maximum(a, b)\n * ```\n *\n * ```js\n * // Broadcast maximum a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.maximum(b).print();  // or tf.maximum(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction maximum_(a, b) {\n    let $a = convertToTensor(a, 'a', 'maximum');\n    let $b = convertToTensor(b, 'b', 'maximum');\n    [$a, $b] = makeTypesMatch($a, $b);\n    if ($a.dtype === 'bool') {\n        $a = cast($a, 'int32');\n        $b = cast($b, 'int32');\n    }\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Maximum, inputs);\n}\nexport const maximum = op({ maximum_ });\n//# sourceMappingURL=maximum.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { CropAndResize } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\n/**\n * Extracts crops from the input image tensor and resizes them using bilinear\n * sampling or nearest neighbor sampling (possibly with aspect ratio change)\n * to a common output size specified by cropSize.\n *\n * @param image 4d tensor of shape `[batch,imageHeight,imageWidth, depth]`,\n *     where imageHeight and imageWidth must be positive, specifying the\n *     batch of images from which to take crops\n * @param boxes 2d float32 tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the normalized\n *     coordinates of the box in the boxInd[i]'th image in the batch\n * @param boxInd 1d int32 tensor of shape `[numBoxes]` with values in range\n *     `[0, batch)` that specifies the image that the `i`-th box refers to.\n * @param cropSize 1d int32 tensor of 2 elements `[cropHeigh, cropWidth]`\n *     specifying the size to which all crops are resized to.\n * @param method Optional string from `'bilinear' | 'nearest'`,\n *     defaults to bilinear, which specifies the sampling method for resizing\n * @param extrapolationValue A threshold for deciding when to remove boxes based\n *     on score. Defaults to 0.\n * @return A 4D tensor of the shape `[numBoxes,cropHeight,cropWidth,depth]`\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction cropAndResize_(image, boxes, boxInd, cropSize, method = 'bilinear', extrapolationValue = 0) {\n    const $image = convertToTensor(image, 'image', 'cropAndResize');\n    const $boxes = convertToTensor(boxes, 'boxes', 'cropAndResize', 'float32');\n    const $boxInd = convertToTensor(boxInd, 'boxInd', 'cropAndResize', 'int32');\n    const numBoxes = $boxes.shape[0];\n    util.assert($image.rank === 4, () => 'Error in cropAndResize: image must be rank 4,' +\n        `but got rank ${$image.rank}.`);\n    util.assert($boxes.rank === 2 && $boxes.shape[1] === 4, () => `Error in cropAndResize: boxes must be have size [${numBoxes},4] ` +\n        `but had shape ${$boxes.shape}.`);\n    util.assert($boxInd.rank === 1 && $boxInd.shape[0] === numBoxes, () => `Error in cropAndResize: boxInd must be have size [${numBoxes}] ` +\n        `but had shape ${$boxes.shape}.`);\n    util.assert(cropSize.length === 2, () => `Error in cropAndResize: cropSize must be of length 2, but got ` +\n        `length ${cropSize.length}.`);\n    util.assert(cropSize[0] >= 1 && cropSize[1] >= 1, () => `cropSize must be atleast [1,1], but was ${cropSize}`);\n    util.assert(method === 'bilinear' || method === 'nearest', () => `method must be bilinear or nearest, but was ${method}`);\n    const inputs = { image: $image, boxes: $boxes, boxInd: $boxInd };\n    const attrs = { method, extrapolationValue, cropSize };\n    const res = ENGINE.runKernel(CropAndResize, inputs, attrs);\n    return res;\n}\nexport const cropAndResize = op({ cropAndResize_ });\n//# sourceMappingURL=crop_and_resize.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { IsInf } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns which elements of x are Infinity or -Infinity.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isInf().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction isInf_(x) {\n    const $x = convertToTensor(x, 'x', 'isInf');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(IsInf, inputs);\n}\nexport const isInf = op({ isInf_ });\n//# sourceMappingURL=is_inf.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Imag } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns the imaginary part of a complex (or real) tensor.\n *\n * Given a tensor input, this operation returns a tensor of type float that is\n * the imaginary part of each element in input considered as a complex number.\n * If input is real, a tensor of all zeros is returned.\n *\n * ```js\n * const x = tf.complex([-2.25, 3.25], [4.75, 5.75]);\n * tf.imag(x).print();\n * ```\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nfunction imag_(input) {\n    const $input = convertToTensor(input, 'input', 'imag');\n    const inputs = { input: $input };\n    return ENGINE.runKernel(Imag, inputs);\n}\nexport const imag = op({ imag_ });\n//# sourceMappingURL=imag.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assert } from '../../util';\nimport { greaterEqual } from '../greater_equal';\nimport { lessEqual } from '../less_equal';\nimport { logicalAnd } from '../logical_and';\nimport { op } from '../operation';\nimport { range } from '../range';\nimport { reshape } from '../reshape';\nimport { scalar } from '../scalar';\nimport { stack } from '../stack';\nimport { sub } from '../sub';\nimport { unstack } from '../unstack';\nimport { where } from '../where';\nimport { zeros } from '../zeros';\n/**\n * Copy a tensor setting everything outside a central band in each innermost\n * matrix to zero.\n *\n * The band part is computed as follows: Assume input has `k` dimensions\n * `[I, J, K, ..., M, N]`, then the output is a tensor with the same shape where\n * `band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]`.\n * The indicator function\n * `in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower))`\n * `&& (num_upper < 0 || (n-m) <= num_upper)`\n *\n * ```js\n * const x = tf.tensor2d([[ 0,  1,  2, 3],\n *                        [-1,  0,  1, 2],\n *                        [-2, -1,  0, 1],\n *                        [-3, -2, -1, 0]]);\n * let y = tf.linalg.bandPart(x, 1, -1);\n * y.print(); // [[ 0,  1,  2, 3],\n *            //  [-1,  0,  1, 2],\n *            //  [ 0, -1,  0, 1],\n *            //  [ 0, 0 , -1, 0]]\n * let z = tf.linalg.bandPart(x, 2, 1);\n * z.print(); // [[ 0,  1,  0, 0],\n *            //  [-1,  0,  1, 0],\n *            //  [-2, -1,  0, 1],\n *            //  [ 0, -2, -1, 0]]\n * ```\n *\n * @param x Rank `k` tensor\n * @param numLower Number of subdiagonals to keep.\n *   If negative, keep entire lower triangle.\n * @param numUpper Number of subdiagonals to keep.\n *   If negative, keep entire upper triangle.\n * @returns Rank `k` tensor of the same shape as input.\n *   The extracted banded tensor.\n *\n * @doc {heading:'Operations', subheading:'Linear Algebra', namespace:'linalg'}\n */\nfunction bandPart_(a, numLower, numUpper) {\n    assert(numLower % 1 === 0, () => `bandPart(): numLower must be an integer, got ${numLower}.`);\n    assert(numUpper % 1 === 0, () => `bandPart(): numUpper must be an integer, got ${numUpper}.`);\n    const $a = convertToTensor(a, 'a', 'bandPart');\n    assert($a.rank >= 2, () => `bandPart(): Rank must be at least 2, got ${$a.rank}.`);\n    const shape = $a.shape;\n    const [M, N] = $a.shape.slice(-2);\n    if (!(numLower <= M)) {\n        throw new Error(`bandPart(): numLower (${numLower})` +\n            ` must not be greater than the number of rows (${M}).`);\n    }\n    if (!(numUpper <= N)) {\n        throw new Error(`bandPart(): numUpper (${numUpper})` +\n            ` must not be greater than the number of columns (${N}).`);\n    }\n    if (numLower < 0) {\n        numLower = M;\n    }\n    if (numUpper < 0) {\n        numUpper = N;\n    }\n    const i = reshape(range(0, M, 1, 'int32'), [-1, 1]);\n    const j = range(0, N, 1, 'int32');\n    const ij = sub(i, j);\n    const inBand = logicalAnd(lessEqual(ij, scalar(+numLower, 'int32')), greaterEqual(ij, scalar(-numUpper, 'int32')));\n    const zero = zeros([M, N], $a.dtype);\n    return reshape(stack(unstack(reshape($a, [-1, M, N]))\n        .map(mat => where(inBand, mat, zero))), shape);\n}\nexport const bandPart = op({ bandPart_ });\n//# sourceMappingURL=band_part.js.map","import { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the cosine distance loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param axis The dimension along which the cosine distance is computed.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction cosineDistance_(labels, predictions, axis, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'cosineDistance');\n    const $predictions = convertToTensor(predictions, 'predictions', 'cosineDistance');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'cosineDistance');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in cosineDistance: ');\n    const one = scalar(1);\n    const losses = sub(one, sum(mul($labels, $predictions), axis, true));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const cosineDistance = op({ cosineDistance_ });\n//# sourceMappingURL=cosine_distance.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { add } from '../add';\nimport { Reduction } from '../loss_ops_utils';\nimport { minimum } from '../minimum';\nimport { mul } from '../mul';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { square } from '../square';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the huber loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param delta Point where huber loss changes from quadratic to linear.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`.\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction huberLoss_(labels, predictions, weights, delta = 1.0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'huberLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'huberLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'huberLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in huberLoss: ');\n    const deltaScalar = scalar(delta);\n    const error = abs(sub($predictions, $labels));\n    const quadratic = minimum(error, deltaScalar);\n    const linear = sub(error, quadratic);\n    const losses = add(mul(scalar(0.5), square(quadratic)), mul(deltaScalar, linear));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const huberLoss = op({ huberLoss_ });\n//# sourceMappingURL=huber_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Max } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the maximum of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If `axes` has no entries, all dimensions are reduced, and an\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.max().print();  // or tf.max(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.max(axis).print();  // or tf.max(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction max_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'max');\n    const inputs = { x: $x };\n    const attrs = { reductionIndices: axis, keepDims };\n    return ENGINE.runKernel(Max, inputs, attrs);\n}\nexport const max = op({ max_ });\n//# sourceMappingURL=max.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { ResizeNearestNeighbor } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * NearestNeighbor resize a batch of 3D images to a new shape.\n *\n * @param images The images, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.\n * @param size The new shape `[newHeight, newWidth]` to resize the\n *     images to. Each channel is resized individually.\n * @param alignCorners Defaults to False. If true, rescale\n *     input by `(new_height - 1) / (height - 1)`, which exactly aligns the 4\n *     corners of images and resized images. If false, rescale by\n *     `new_height / height`. Treat similarly the width dimension.\n * @param halfPixelCenters Defaults to `false`. Whether to assumes pixels are of\n *      half the actual dimensions, and yields more accurate resizes. This flag\n *      would also make the floating point coordinates of the top left pixel\n *      0.5, 0.5.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction resizeNearestNeighbor_(images, size, alignCorners = false, halfPixelCenters = false) {\n    const $images = convertToTensor(images, 'images', 'resizeNearestNeighbor');\n    util.assert($images.rank === 3 || $images.rank === 4, () => `Error in resizeNearestNeighbor: x must be rank 3 or 4, but got ` +\n        `rank ${$images.rank}.`);\n    util.assert(size.length === 2, () => `Error in resizeNearestNeighbor: new shape must 2D, but got shape ` +\n        `${size}.`);\n    util.assert($images.dtype === 'float32' || $images.dtype === 'int32', () => '`images` must have `int32` or `float32` as dtype');\n    util.assert(halfPixelCenters === false || alignCorners === false, () => `Error in resizeNearestNeighbor: If halfPixelCenters is true, ` +\n        `alignCorners must be false.`);\n    let batchImages = $images;\n    let reshapedTo4D = false;\n    if ($images.rank === 3) {\n        reshapedTo4D = true;\n        batchImages = reshape($images, [1, $images.shape[0], $images.shape[1], $images.shape[2]]);\n    }\n    const [] = size;\n    const inputs = { images: batchImages };\n    const attrs = { alignCorners, halfPixelCenters, size };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(ResizeNearestNeighbor, inputs, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    return res;\n}\nexport const resizeNearestNeighbor = op({ resizeNearestNeighbor_ });\n//# sourceMappingURL=resize_nearest_neighbor.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Log } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes natural logarithm of the input `tf.Tensor` element-wise: `ln(x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, Math.E]);\n *\n * x.log().print();  // or tf.log(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction log_(x) {\n    const $x = convertToTensor(x, 'x', 'log');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(Log, inputs);\n}\nexport const log = op({ log_ });\n//# sourceMappingURL=log.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { ResizeBilinear } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Bilinear resize a single 3D image or a batch of 3D images to a new shape.\n *\n * @param images The images, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.\n * @param size The new shape `[newHeight, newWidth]` to resize the\n *     images to. Each channel is resized individually.\n * @param alignCorners Defaults to `false`. If true, rescale\n *     input by `(new_height - 1) / (height - 1)`, which exactly aligns the 4\n *     corners of images and resized images. If false, rescale by\n *     `new_height / height`. Treat similarly the width dimension.\n * @param halfPixelCenters Defaults to `false`. Whether to assume pixel centers\n *     are at 0.5, which would make the floating point coordinates of the top\n *     left pixel 0.5, 0.5.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction resizeBilinear_(images, size, alignCorners = false, halfPixelCenters = false) {\n    const $images = convertToTensor(images, 'images', 'resizeBilinear');\n    util.assert($images.rank === 3 || $images.rank === 4, () => `Error in resizeBilinear: x must be rank 3 or 4, but got ` +\n        `rank ${$images.rank}.`);\n    util.assert(size.length === 2, () => `Error in resizeBilinear: new shape must 2D, but got shape ` +\n        `${size}.`);\n    util.assert(halfPixelCenters === false || alignCorners === false, () => `Error in resizeBilinear: If halfPixelCenters is true, ` +\n        `alignCorners must be false.`);\n    let batchImages = $images;\n    let reshapedTo4D = false;\n    if ($images.rank === 3) {\n        reshapedTo4D = true;\n        batchImages = reshape($images, [1, $images.shape[0], $images.shape[1], $images.shape[2]]);\n    }\n    const [] = size;\n    const inputs = { images: batchImages };\n    const attrs = { alignCorners, halfPixelCenters, size };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(ResizeBilinear, inputs, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    return res;\n}\nexport const resizeBilinear = op({ resizeBilinear_ });\n//# sourceMappingURL=resize_bilinear.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPoolWithArgmax } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the 2D max pooling of an image with Argmax index.\n * The indices in argmax are flattened, so that a maximum value at position `[b,\n * y, x, c]` becomes flattened index: `(y * width + x) * channels + c` if\n * include_batch_in_index is False; `((b * height + y) * width + x) * channels\n * +c` if include_batch_in_index is True.\n *\n * The indices returned are always in `[0, height) x [0, width)` before\n * flattening.\n *\n * @param x The input tensor, of rank 4 or rank 3 of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.\n * @param filterSize The filter size: `[filterHeight, filterWidth]`. If\n *     `filterSize` is a single number, then `filterHeight == filterWidth`.\n * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If\n *     `strides` is a single number, then `strideHeight == strideWidth`.\n * @param dataFormat An optional string from: \"NDHWC\", \"NCDHW\". Defaults to\n *     \"NDHWC\". Specify the data format of the input and output data. With the\n *     default format \"NDHWC\", the data is stored in the order of: [batch,\n *     depth, height, width, channels]. Only \"NDHWC\" is currently supported.\n * @param pad The type of padding algorithm.\n *    - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *    - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *    - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param includeBatchIndex Defaults to False. Whether to include batch\n *    dimension in flattened index of argmax.\n *\n * @doc {heading: 'Operations', subheading: 'Convolution'}\n */\nfunction maxPoolWithArgmax_(x, filterSize, strides, pad, includeBatchInIndex = false) {\n    const $x = convertToTensor(x, 'x', 'maxPoolWithArgmax');\n    const inputs = { x: $x };\n    const attrs = { filterSize, strides, pad, includeBatchInIndex };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const result = ENGINE.runKernel(MaxPoolWithArgmax, inputs, attrs);\n    return { result: result[0], indexes: result[1] };\n}\nexport const maxPoolWithArgmax = op({ maxPoolWithArgmax_ });\n//# sourceMappingURL=max_pool_with_argmax.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LinSpace } from '../kernel_names';\n/**\n * Return an evenly spaced sequence of numbers over the given interval.\n *\n * ```js\n * tf.linspace(0, 9, 10).print();\n * ```\n * @param start The start value of the sequence.\n * @param stop The end value of the sequence.\n * @param num The number of values to generate.\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nexport function linspace(start, stop, num) {\n    if (num <= 0) {\n        throw new Error('The number of values should be positive.');\n    }\n    const attrs = { start, stop, num };\n    return ENGINE.runKernel(LinSpace, {}, attrs);\n}\n//# sourceMappingURL=linspace.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { GatherV2 } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Gather slices from tensor `x`'s axis `axis` according to `indices`.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3, 4]);\n * const indices = tf.tensor1d([1, 3, 3], 'int32');\n *\n * x.gather(indices).print();\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const indices = tf.tensor1d([1, 1, 0], 'int32');\n *\n * x.gather(indices).print();\n * ```\n * @param x The input tensor whose slices to be gathered.\n * @param indices The indices of the values to extract.\n * @param axis The axis over which to select values. Defaults to 0.\n * @param batchDims Optional. The number of batch dimensions. It must be less\n *     than or equal to rank(indices). Defaults to 0.\n *     The output tensor will have shape of\n *     `x.shape[:axis] + indices.shape[batchDims:] + x.shape[axis + 1:]`\n *\n * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}\n */\nfunction gather_(x, indices, axis = 0, batchDims = 0) {\n    const $x = convertToTensor(x, 'x', 'gather');\n    const $indices = convertToTensor(indices, 'indices', 'gather', 'int32');\n    const inputs = { x: $x, indices: $indices };\n    const attrs = { axis, batchDims };\n    return ENGINE.runKernel(GatherV2, inputs, attrs);\n}\nexport const gather = op({ gather_ });\n//# sourceMappingURL=gather.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { GatherNd } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Gather slices from input tensor into a Tensor with shape specified by\n * `indices`.\n *\n * `indices` is an K-dimensional integer tensor, best thought of as a\n * (K-1)-dimensional tensor of indices into input, where each element defines a\n * slice of input:\n * output[\\\\(i_0, ..., i_{K-2}\\\\)] = input[indices[\\\\(i_0, ..., i_{K-2}\\\\)]]\n *\n * Whereas in `tf.gather`, `indices` defines slices into the first dimension of\n * input, in `tf.gatherND`, `indices` defines slices into the first N dimensions\n * of input, where N = indices.shape[-1].\n *\n * The last dimension of indices can be at most the rank of input:\n * indices.shape[-1] <= input.rank\n *\n * The last dimension of `indices` corresponds to elements\n * (if indices.shape[-1] == input.rank) or slices\n * (if indices.shape[-1] < input.rank) along dimension indices.shape[-1] of\n * input.\n * The output tensor has shape\n * indices.shape[:-1] + input.shape[indices.shape[-1]:]\n *\n * Note that on CPU, if an out of bound index is found, an error is returned. On\n * GPU, if an out of bound index is found, a 0 is stored in the corresponding\n * output value.\n *\n * ```js\n * const indices = tf.tensor2d([0, 1, 1, 0], [2,2], 'int32');\n * const input = tf.tensor2d([9, 10, 11, 12], [2, 2]);\n * tf.gatherND(input, indices).print() // [10, 11]\n * ```\n *\n * @param x The tensor from which to gather values.\n * @param indices Index tensor, must be of type int32.\n *\n * @doc {heading: 'Operations', subheading: 'Slicing and Joining'}\n */\nfunction gatherND_(x, indices) {\n    const $indices = convertToTensor(indices, 'indices', 'gatherND', 'int32');\n    const $x = convertToTensor(x, 'x', 'gatherND');\n    const inputs = { params: $x, indices: $indices };\n    return ENGINE.runKernel(GatherNd, inputs);\n}\nexport const gatherND = op({ gatherND_ });\n//# sourceMappingURL=gather_nd.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { NonMaxSuppressionV3 } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { op } from '../operation';\n/**\n * Performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @return A 1D tensor with the selected box indices.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction nonMaxSuppression_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');\n    const inputs = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold);\n    maxOutputSize = inputs.maxOutputSize;\n    iouThreshold = inputs.iouThreshold;\n    scoreThreshold = inputs.scoreThreshold;\n    const attrs = { maxOutputSize, iouThreshold, scoreThreshold };\n    return ENGINE.runKernel(NonMaxSuppressionV3, { boxes: $boxes, scores: $scores }, attrs);\n}\nexport const nonMaxSuppression = op({ nonMaxSuppression_ });\n//# sourceMappingURL=non_max_suppression.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LessEqual } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a <= b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.lessEqual(b).print();\n * ```\n *\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction lessEqual_(a, b) {\n    let $a = convertToTensor(a, 'a', 'lessEqual');\n    let $b = convertToTensor(b, 'b', 'lessEqual');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(LessEqual, inputs);\n}\nexport const lessEqual = op({ lessEqual_ });\n//# sourceMappingURL=less_equal.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Min } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the minimum value from the input.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the array is reduced by 1 for each entry in `axes`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axes` has no entries, all dimensions are reduced, and an array with a\n * single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.min().print();  // or tf.min(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.min(axis).print();  // or tf.min(x, axis)\n * ```\n *\n * @param x The input Tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction min_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'min');\n    const inputs = { x: $x };\n    const attrs = { axis, keepDims };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    return ENGINE.runKernel(Min, inputs, attrs);\n}\nexport const min = op({ min_ });\n//# sourceMappingURL=min.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { add } from '../add';\nimport { exp } from '../exp';\nimport { log1p } from '../log1p';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { relu } from '../relu';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\nfunction sigmoidCrossEntropyWithLogits_(labels, logits) {\n    const $labels = convertToTensor(labels, 'labels', 'sigmoidCrossEntropyWithLogits');\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropyWithLogits');\n    assertShapesMatch($labels.shape, $logits.shape, 'Error in sigmoidCrossEntropyWithLogits: ');\n    /**\n     * Implementation Details:\n     *\n     * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n     *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n     *   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n     *   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n     *   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n     *   = (1 - z) * x + log(1 + exp(-x))\n     *   = x - x * z + log(1 + exp(-x))\n     *\n     *   For x < 0, to avoid overflow in exp(-x), we reformulate the above\n     *     x - x * z + log(1 + exp(-x))\n     *   = log(exp(x)) - x * z + log(1 + exp(-x))\n     *   = - x * z + log(1 + exp(x))\n     *\n     * Hence, to ensure stability and avoid overflow, the implementation uses\n     * this equivalent formulation:\n     *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n     */\n    const maxOutput = relu($logits);\n    const outputXTarget = mul($logits, $labels);\n    const sigmoidOutput = log1p(exp(neg(abs($logits))));\n    return add(sub(maxOutput, outputXTarget), sigmoidOutput);\n}\n/**\n * Computes the sigmoid cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)\n *                         + 0.5 * labelSmoothing\n *\n * @param multiClassLabels The ground truth output tensor of shape\n * [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\nfunction sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $multiClassLabels = convertToTensor(multiClassLabels, 'multiClassLabels', 'sigmoidCrossEntropy');\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropy');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'sigmoidCrossEntropy');\n    }\n    assertShapesMatch($multiClassLabels.shape, $logits.shape, 'Error in sigmoidCrossEntropy: ');\n    if (labelSmoothing > 0) {\n        const labelSmoothingScalar = scalar(labelSmoothing);\n        const one = scalar(1);\n        const half = scalar(0.5);\n        $multiClassLabels =\n            add(mul($multiClassLabels, sub(one, labelSmoothingScalar)), mul(half, labelSmoothingScalar));\n    }\n    const losses = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const sigmoidCrossEntropy = op({ sigmoidCrossEntropy_ });\n//# sourceMappingURL=sigmoid_cross_entropy.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Mod } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns the mod of a and b element-wise.\n * `floor(x / y) * y + mod(x, y) = x`\n * Supports broadcasting.\n *\n * We also expose `tf.modStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.mod(b).print();  // or tf.mod(a, b)\n * ```\n *\n * ```js\n * // Broadcast a mod b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.mod(b).print();  // or tf.mod(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction mod_(a, b) {\n    let $a = convertToTensor(a, 'a', 'mod');\n    let $b = convertToTensor(b, 'b', 'mod');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Mod, inputs);\n}\nexport const mod = op({ mod_ });\n//# sourceMappingURL=mod.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Less } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a < b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.less(b).print();\n * ```\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction less_(a, b) {\n    let $a = convertToTensor(a, 'a', 'less');\n    let $b = convertToTensor(b, 'b', 'less');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Less, inputs);\n}\nexport const less = op({ less_ });\n//# sourceMappingURL=less.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Greater } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a > b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.greater(b).print();\n * ```\n *\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction greater_(a, b) {\n    let $a = convertToTensor(a, 'a', 'greater');\n    let $b = convertToTensor(b, 'b', 'greater');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Greater, inputs);\n}\nexport const greater = op({ greater_ });\n//# sourceMappingURL=greater.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { RotateWithOffset } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\n/**\n * Rotates the input image tensor counter-clockwise with an optional offset\n * center of rotation. Currently available in the CPU, WebGL, and WASM backends.\n *\n * @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.\n * @param radians The amount of rotation.\n * @param fillValue The value to fill in the empty space leftover\n *     after rotation. Can be either a single grayscale value (0-255), or an\n *     array of three numbers `[red, green, blue]` specifying the red, green,\n *     and blue channels. Defaults to `0` (black).\n * @param center The center of rotation. Can be either a single value (0-1), or\n *     an array of two numbers `[centerX, centerY]`. Defaults to `0.5` (rotates\n *     the image around its center).\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction rotateWithOffset_(image, radians, fillValue = 0, center = 0.5) {\n    const $image = convertToTensor(image, 'image', 'rotateWithOffset', 'float32');\n    util.assert($image.rank === 4, () => 'Error in rotateWithOffset: image must be rank 4,' +\n        `but got rank ${$image.rank}.`);\n    const inputs = { image: $image };\n    const attrs = { radians, fillValue, center };\n    const res = ENGINE.runKernel(RotateWithOffset, inputs, attrs);\n    return res;\n}\nexport const rotateWithOffset = op({ rotateWithOffset_ });\n//# sourceMappingURL=rotate_with_offset.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { BatchMatMul } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the dot product of two matrices, A * B. These must be matrices.\n *\n * ```js\n * const a = tf.tensor2d([1, 2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * a.matMul(b).print();  // or tf.matMul(a, b)\n * ```\n * @param a First matrix in dot product operation.\n * @param b Second matrix in dot product operation.\n * @param transposeA If true, `a` is transposed before multiplication.\n * @param transposeB If true, `b` is transposed before multiplication.\n *\n * @doc {heading: 'Operations', subheading: 'Matrices'}\n */\nfunction matMul_(a, b, transposeA = false, transposeB = false) {\n    let $a = convertToTensor(a, 'a', 'matMul');\n    let $b = convertToTensor(b, 'b', 'matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const inputs = { a: $a, b: $b };\n    const attrs = { transposeA, transposeB };\n    return ENGINE.runKernel(BatchMatMul, inputs, attrs);\n}\nexport const matMul = op({ matMul_ });\n//# sourceMappingURL=mat_mul.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { assertTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { add } from './add';\nimport { div } from './div';\nimport { mul } from './mul';\nimport { op } from './operation';\nimport { pow } from './pow';\nimport { scalar } from './scalar';\nimport { sub } from './sub';\n/**\n * Compute the moving average of a variable.\n *\n * Without zeroDebias, the moving average operation is defined by:\n *   `v += delta`\n * where\n *   `delta = (1 - decay) * (x - v)`\n *\n * With zeroDebias (default), the `delta` term is scaled to debias the\n * effect of the (assumed) zero-initialization of `v`.\n *   `delta /= (1 - decay ^ step)`\n *\n * For more details on the zero-debiasing algorithm, see:\n *   https://arxiv.org/abs/1412.6980\n *\n * Note that this function is completely stateless and does not keep track of\n * step count. The step count needs to be maintained by the caller and passed\n * in as `step`.\n *\n * @param v The current moving average value.\n * @param x New input value, must have the same shape and dtype as `v`.\n * @param decay The decay factor. Typical values are 0.95 and 0.99.\n * @param step Step count.\n * @param zeroDebias: Whether zeroDebias is to be performed (default: `true`).\n * @returns The new moving average value.\n *\n * @doc {heading: 'Operations', subheading: 'Moving Average'}\n */\nfunction movingAverage_(v, x, decay, step, zeroDebias = true) {\n    const $v = convertToTensor(v, 'v', 'movingAverage');\n    const $x = convertToTensor(x, 'x', 'movingAverage');\n    const $decay = convertToTensor(decay, 'decay', 'movingAverage');\n    assertTypesMatch($v, $x);\n    util.assert(util.arraysEqual($v.shape, $x.shape), () => 'Shape mismatch in v and x');\n    const one = scalar(1);\n    const oneMinusDecay = sub(one, $decay);\n    let update = mul(sub($x, $v), oneMinusDecay);\n    if (zeroDebias) {\n        util.assert(step != null, () => 'When using zeroDebias: true, step is required.');\n        const $step = convertToTensor(step, 'step', 'movingAverage');\n        update = div(update, sub(one, pow($decay, $step)));\n    }\n    return add($v, update);\n}\nexport const movingAverage = op({ movingAverage_ });\n//# sourceMappingURL=moving_average.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MirrorPad } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\n/**\n * Pads a `tf.Tensor` using mirror padding.\n *\n * This operation implements the `REFLECT` and `SYMMETRIC` modes of pad.\n *\n * ```js\n * const x = tf.range(0, 9).reshape([1, 1, 3, 3]);\n * x.mirrorPad([[0, 0], [0, 0], [2, 2], [2, 2]], 'reflect').print();\n * ```\n * @param x The tensor to pad.\n * @param paddings An array of length `R` (the rank of the tensor), where\n * each element is a length-2 tuple of ints `[padBefore, padAfter]`,\n * specifying how much to pad along each dimension of the tensor.\n * In \"reflect\" mode, the padded regions do not include the borders,\n * while in \"symmetric\" mode the padded regions do include the borders.\n * For example, if the input is `[1, 2, 3]` and paddings is `[0, 2]`,\n * then the output is `[1, 2, 3, 2, 1]` in \"reflect\" mode, and\n * `[1, 2, 3, 3, 2]` in \"symmetric\" mode.\n * If `mode` is \"reflect\" then both `paddings[D, 0]` and `paddings[D, 1]`\n * must be no greater than `x.shape[D] - 1`. If mode is \"symmetric\"\n * then both `paddings[D, 0]` and `paddings[D, 1]` must be no greater than\n * `x.shape[D]`\n * @param mode String to specify padding mode. Can be `'reflect' | 'symmetric'`\n */\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\nfunction mirrorPad_(x, paddings, mode) {\n    util.assert(mode === 'reflect' || mode === 'symmetric', () => `Invalid mode. Mode must be either reflect or symmetric. ` +\n        `Got ${mode}.`);\n    const $x = convertToTensor(x, 'x', 'mirrorPad');\n    if ($x.rank === 0) {\n        throw new Error('mirrorPad(scalar) is not defined. ' +\n            'Pass non-scalar to mirrorPad');\n    }\n    util.assert(paddings.length === $x.rank, () => `Padding doesn't match input. Must be ${$x.rank}. ` +\n        `Got ${paddings.length}.`);\n    const shapeOffset = mode === 'reflect' ? 1 : 0;\n    for (let i = 0; i < $x.rank; i++) {\n        util.assert(paddings[i].length === 2, () => `Invalid number of paddings. Must be length of 2 each.`);\n        util.assert(paddings[i][0] >= 0 && paddings[i][0] <= $x.shape[i] - shapeOffset &&\n            paddings[i][1] >= 0 && paddings[i][1] <= $x.shape[i] - shapeOffset, () => `Padding in dimension ${i} cannot be greater than or equal ` +\n            `to ${$x.shape[i] - shapeOffset} or less than 0 for input of ` +\n            `shape ${$x.shape}`);\n    }\n    const attrs = { paddings, mode };\n    const inputs = { x: $x };\n    return ENGINE.runKernel(MirrorPad, inputs, attrs);\n}\nexport const mirrorPad = op({ mirrorPad_ });\n//# sourceMappingURL=mirror_pad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LeakyRelu } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes leaky rectified linear element-wise.\n *\n * See\n * [http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf](\n *     http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.leakyRelu(0.1).print();  // or tf.leakyRelu(x, 0.1)\n * ```\n * @param x The input tensor.\n * @param alpha The scaling factor for negative values, defaults to 0.2.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction leakyRelu_(x, alpha = 0.2) {\n    const $x = convertToTensor(x, 'x', 'leakyRelu');\n    const inputs = { x: $x };\n    const attrs = { alpha };\n    return ENGINE.runKernel(LeakyRelu, inputs, attrs);\n}\nexport const leakyRelu = op({ leakyRelu_ });\n//# sourceMappingURL=leaky_relu.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { IsFinite } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns which elements of x are finite.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isFinite().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction isFinite_(x) {\n    const $x = convertToTensor(x, 'x', 'isFinite');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(IsFinite, inputs);\n}\nexport const isFinite = op({ isFinite_ });\n//# sourceMappingURL=is_finite.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as broadcast_util from './broadcast_util';\nimport { elu } from './elu';\nimport { leakyRelu } from './leaky_relu';\nimport { mul } from './mul';\nimport { prelu } from './prelu';\nimport { relu } from './relu';\nimport { relu6 } from './relu6';\nimport { reshape } from './reshape';\nimport { sigmoid } from './sigmoid';\nimport { step } from './step';\nimport { sum } from './sum';\n// Returns gradient for fused activation.\nexport function getFusedDyActivation(dy, y, activation) {\n    if (activation == null || activation === 'linear') {\n        return dy;\n    }\n    if (activation === 'relu') {\n        return mul(dy, step(y));\n    }\n    throw new Error(`Cannot compute gradient for fused activation ${activation}.`);\n}\n// Returns gradient for fused bias.\nexport function getFusedBiasGradient(bias, dyActivation) {\n    let res = dyActivation;\n    const reduceAxes = broadcast_util.getReductionAxes(bias.shape, dyActivation.shape);\n    if (reduceAxes.length > 0) {\n        res = sum(res, reduceAxes);\n    }\n    return reshape(res, bias.shape);\n}\nexport function applyActivation(x, activation, preluActivationWeights, leakyreluAlpha) {\n    if (activation === 'linear') {\n        return x;\n    }\n    else if (activation === 'relu') {\n        return relu(x);\n    }\n    else if (activation === 'elu') {\n        return elu(x);\n    }\n    else if (activation === 'relu6') {\n        return relu6(x);\n    }\n    else if (activation === 'prelu') {\n        return prelu(x, preluActivationWeights);\n    }\n    else if (activation === 'leakyrelu') {\n        return leakyRelu(x, leakyreluAlpha);\n    }\n    else if (activation === 'sigmoid') {\n        return sigmoid(x);\n    }\n    throw new Error(`Unknown fused activation ${activation}.`);\n}\n// Whether we should call fused ops.\nexport const shouldFuse = (gradientDepth, activation) => {\n    const gradientMode = gradientDepth > 0;\n    return !gradientMode || activation === 'linear';\n};\n//# sourceMappingURL=fused_util.js.map","import { convertToTensor } from '../../tensor_util_env';\nimport { cast } from '../cast';\nimport { div } from '../div';\nimport { Reduction } from '../loss_ops_utils';\nimport { mean } from '../mean';\nimport { mul } from '../mul';\nimport { notEqual } from '../not_equal';\nimport { ones } from '../ones';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { sum } from '../sum';\n/**\n * Computes the weighted loss between two tensors.\n *\n * @param losses Tensor of shape `[batch_size, d1, ... dN]`.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `losses`, and must be broadcastable to `losses` (i.e., all\n *    dimensions must be either `1`, or the same as the corresponding\n *    `losses` dimension).\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction computeWeightedLoss_(losses, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $losses = convertToTensor(losses, 'losses', 'computeWeightedLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'computeWeightedLoss');\n    }\n    const weightedLoss = ($weights == null) ? $losses : mul($losses, $weights);\n    if (reduction === Reduction.NONE) {\n        return weightedLoss;\n    }\n    if (reduction === Reduction.SUM) {\n        return sum(weightedLoss);\n    }\n    if (reduction === Reduction.MEAN) {\n        if ($weights == null) {\n            return mean(weightedLoss);\n        }\n        else {\n            const broadcastFactor = $losses.size / $weights.size;\n            const result = div(sum(weightedLoss), sum($weights));\n            return broadcastFactor > 1 ? div(result, scalar(broadcastFactor)) :\n                result;\n        }\n    }\n    if (reduction === Reduction.SUM_BY_NONZERO_WEIGHTS) {\n        if ($weights == null) {\n            return div(sum(weightedLoss), scalar($losses.size));\n        }\n        else {\n            const broadcastedWeights = mul($weights, ones($losses.shape));\n            const numNonZeros = cast(sum(notEqual(broadcastedWeights, scalar(0))), 'float32');\n            return div(sum(weightedLoss), numNonZeros);\n        }\n    }\n    throw Error(`Unknown reduction: ${reduction}`);\n}\nexport const computeWeightedLoss = op({ computeWeightedLoss_ });\n//# sourceMappingURL=compute_weighted_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../../gradients';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { add } from '../add';\nimport { expandShapeToKeepDim } from '../axis_util';\nimport { cast } from '../cast';\nimport { div } from '../div';\nimport { exp } from '../exp';\nimport { logSumExp } from '../log_sum_exp';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes softmax cross entropy between logits and labels.\n *\n * Measures the probability error in discrete classification tasks in which\n * the classes are mutually exclusive (each entry is in exactly one class).\n * For example, each CIFAR-10 image is labeled with one and only one label: an\n * image can be a dog or a truck, but not both.\n *\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\n * not be. All that is required is that each row of labels is a valid\n * probability distribution. If they are not, the computation of the gradient\n * will be incorrect.\n *\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\n * logits internally for efficiency. Do not call this op with the output of\n * softmax, as it will produce incorrect results.\n *\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\n * and the same dtype.\n * @param labels The labels array.\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\nfunction softmaxCrossEntropyWithLogits_(labels, logits, dim = -1) {\n    if (dim === -1) {\n        dim = logits.rank - 1;\n    }\n    if (dim !== logits.rank - 1) {\n        throw Error(`Softmax cross entropy along a non-last dimension is not yet ` +\n            `supported. Labels / logits was rank ${logits.rank} ` +\n            `and dim was ${dim}`);\n    }\n    // Use a custom gradient for numerical stability.\n    const customOp = customGrad((labels, logits, save) => {\n        // Reference:\n        //   1. http://cs231n.github.io/linear-classify/#softmax\n        //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n        const keepDims = true;\n        const lse = logSumExp(logits, [dim], keepDims);\n        const logResult = sub(cast(logits, 'float32'), lse);\n        save([labels, logResult]);\n        const costVector = neg(mul(logResult, labels));\n        const value = sum(costVector, [dim]);\n        const gradFunc = (dy, saved) => {\n            const [labels, logResult] = saved;\n            const dyShape = expandShapeToKeepDim(dy.shape, [dim]);\n            return [\n                mul(reshape(dy, dyShape), sub(cast(labels, 'float32'), exp(logResult))),\n                mul(reshape(dy, dyShape), sub(exp(logResult), cast(labels, 'float32'))),\n            ];\n        };\n        return { value, gradFunc };\n    });\n    return customOp(labels, logits);\n}\n/**\n * Computes the softmax cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\n *                         + labelSmoothing / numClasses\n *\n * @param onehotLabels One hot encoded labels\n *    [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or 1, and must be\n *    broadcastable to `loss`  of shape [batch_size]\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\nfunction softmaxCrossEntropy_(onehotLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\n    const $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\n    }\n    assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\n    if (labelSmoothing > 0) {\n        const labelSmoothingScalar = scalar(labelSmoothing);\n        const one = scalar(1);\n        const numClasses = scalar($onehotLabels.shape[1]);\n        $onehotLabels =\n            add(mul($onehotLabels, sub(one, labelSmoothingScalar)), div(labelSmoothingScalar, numClasses));\n    }\n    const losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const softmaxCrossEntropy = op({ softmaxCrossEntropy_ });\n//# sourceMappingURL=softmax_cross_entropy.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { nonMaxSuppressionV3Impl } from '../../backends/non_max_suppression_impl';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { tensor1d } from '../tensor1d';\n/**\n * Performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * This is the async version of `nonMaxSuppression`\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @return A 1D tensor with the selected box indices.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nasync function nonMaxSuppressionAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');\n    const inputs = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold);\n    maxOutputSize = inputs.maxOutputSize;\n    iouThreshold = inputs.iouThreshold;\n    scoreThreshold = inputs.scoreThreshold;\n    const boxesAndScores = await Promise.all([$boxes.data(), $scores.data()]);\n    const boxesVals = boxesAndScores[0];\n    const scoresVals = boxesAndScores[1];\n    // We call a cpu based impl directly with the typedarray data  here rather\n    // than a kernel because all kernels are synchronous (and thus cannot await\n    // .data()).\n    const { selectedIndices } = nonMaxSuppressionV3Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);\n    if ($boxes !== boxes) {\n        $boxes.dispose();\n    }\n    if ($scores !== scores) {\n        $scores.dispose();\n    }\n    return tensor1d(selectedIndices, 'int32');\n}\nexport const nonMaxSuppressionAsync = nonMaxSuppressionAsync_;\n//# sourceMappingURL=non_max_suppression_async.js.map","import { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { op } from '../operation';\nimport { relu } from '../relu';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the Hinge loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction hingeLoss_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $labels = convertToTensor(labels, 'labels', 'hingeLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'hingeLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'hingeLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in hingeLoss: ');\n    const one = scalar(1);\n    // Convert binary labels to (-1, 1)\n    $labels = sub(mul(scalar(2), $labels), one);\n    const losses = relu(sub(one, mul($labels, $predictions)));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const hingeLoss = op({ hingeLoss_ });\n//# sourceMappingURL=hinge_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Multinomial } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Creates a `tf.Tensor` with values drawn from a multinomial distribution.\n *\n * ```js\n * const probs = tf.tensor([.75, .25]);\n * tf.multinomial(probs, 3).print();\n * ```\n *\n * @param logits 1D array with unnormalized log-probabilities, or\n *     2D array of shape `[batchSize, numOutcomes]`. See the `normalized`\n *     parameter.\n * @param numSamples Number of samples to draw for each row slice.\n * @param seed The seed number.\n * @param normalized Whether the provided `logits` are normalized true\n *     probabilities (sum to 1). Defaults to false.\n * @return 1D array of shape `[numSamples]`, or 2D array of shape\n *     `[batchSize, numSamples]`, depending on the rank of the input.\n *\n * @doc {heading: 'Tensors', subheading: 'Random'}\n */\nfunction multinomial_(logits, numSamples, seed, normalized = false) {\n    const $logits = convertToTensor(logits, 'logits', 'multinomial');\n    const numOutcomes = $logits.size;\n    const origRank = $logits.rank;\n    if (numOutcomes < 2) {\n        throw new Error(`Error in multinomial: you need at least 2 outcomes, but got ` +\n            `${numOutcomes}.`);\n    }\n    if (origRank > 2) {\n        throw new Error(`Rank of probabilities must be 1 or 2, but is ${origRank}`);\n    }\n    // TODO(lina128): Investigate correct seed behavior. The code seems not allow\n    // setting see to 0.\n    seed = seed || Math.random();\n    // The kernel only accepts (and returns) rank 2 tensors.\n    const logits2D = origRank === 1 ? reshape($logits, [1, -1]) : $logits;\n    const inputs = { logits: logits2D };\n    const attrs = { numSamples, seed, normalized };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(Multinomial, inputs, attrs);\n    // tslint:disable-next-line:no-unnecessary-type-assertion\n    return origRank === 1 ? reshape(res, [res.size]) : res;\n}\nexport const multinomial = op({ multinomial_ });\n//# sourceMappingURL=multinomial.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LRNGrad } from '../kernel_names';\nimport { op } from './operation';\nfunction localResponseNormalizationBackprop_(x, y, dy, depthRadius = 5, bias = 1, alpha = 1, beta = 0.5) {\n    const inputs = { x, y, dy };\n    const attrs = { depthRadius, bias, alpha, beta };\n    return ENGINE.runKernel(LRNGrad, inputs, attrs);\n}\nexport const localResponseNormalizationBackprop = op({ localResponseNormalizationBackprop_ });\n//# sourceMappingURL=local_response_normalization_backprop.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { FlipLeftRight } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\n/**\n * Flips the image left to right. Currently available in the CPU, WebGL, and\n * WASM backends.\n *\n * @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.\n */\n/** @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'} */\nfunction flipLeftRight_(image) {\n    const $image = convertToTensor(image, 'image', 'flipLeftRight', 'float32');\n    util.assert($image.rank === 4, () => 'Error in flipLeftRight: image must be rank 4,' +\n        `but got rank ${$image.rank}.`);\n    const inputs = { image: $image };\n    const res = ENGINE.runKernel(FlipLeftRight, inputs, {});\n    return res;\n}\nexport const flipLeftRight = op({ flipLeftRight_ });\n//# sourceMappingURL=flip_left_right.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { assert } from '../../util';\nimport { div } from '../div';\nimport { mul } from '../mul';\nimport { norm } from '../norm';\nimport { op } from '../operation';\nimport { split } from '../split';\nimport { squeeze } from '../squeeze';\nimport { stack } from '../stack';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\n/**\n * Gram-Schmidt orthogonalization.\n *\n * ```js\n * const x = tf.tensor2d([[1, 2], [3, 4]]);\n * let y = tf.linalg.gramSchmidt(x);\n * y.print();\n * console.log('Othogonalized:');\n * y.dot(y.transpose()).print();  // should be nearly the identity matrix.\n * console.log('First row direction maintained:');\n * const data = await y.array();\n * console.log(data[0][1] / data[0][0]);  // should be nearly 2.\n * ```\n *\n * @param xs The vectors to be orthogonalized, in one of the two following\n *   formats:\n *   - An Array of `tf.Tensor1D`.\n *   - A `tf.Tensor2D`, i.e., a matrix, in which case the vectors are the rows\n *     of `xs`.\n *   In each case, all the vectors must have the same length and the length\n *   must be greater than or equal to the number of vectors.\n * @returns The orthogonalized and normalized vectors or matrix.\n *   Orthogonalization means that the vectors or the rows of the matrix\n *   are orthogonal (zero inner products). Normalization means that each\n *   vector or each row of the matrix has an L2 norm that equals `1`.\n *\n * @doc {heading:'Operations', subheading:'Linear Algebra', namespace:'linalg'}\n */\nfunction gramSchmidt_(xs) {\n    let inputIsTensor2D;\n    if (Array.isArray(xs)) {\n        inputIsTensor2D = false;\n        assert(xs != null && xs.length > 0, () => 'Gram-Schmidt process: input must not be null, undefined, or ' +\n            'empty');\n        const dim = xs[0].shape[0];\n        for (let i = 1; i < xs.length; ++i) {\n            assert(xs[i].shape[0] === dim, () => 'Gram-Schmidt: Non-unique lengths found in the input vectors: ' +\n                `(${xs[i].shape[0]} vs. ${dim})`);\n        }\n    }\n    else {\n        inputIsTensor2D = true;\n        xs = split(xs, xs.shape[0], 0).map(x => squeeze(x, [0]));\n    }\n    assert(xs.length <= xs[0].shape[0], () => `Gram-Schmidt: Number of vectors (${xs.length}) exceeds ` +\n        `number of dimensions (${xs[0].shape[0]}).`);\n    const ys = [];\n    const xs1d = xs;\n    for (let i = 0; i < xs.length; ++i) {\n        ys.push(ENGINE.tidy(() => {\n            let x = xs1d[i];\n            if (i > 0) {\n                for (let j = 0; j < i; ++j) {\n                    const proj = mul(sum(mul(ys[j], x)), ys[j]);\n                    x = sub(x, proj);\n                }\n            }\n            return div(x, norm(x, 'euclidean'));\n        }));\n    }\n    if (inputIsTensor2D) {\n        return stack(ys, 0);\n    }\n    else {\n        return ys;\n    }\n}\nexport const gramSchmidt = op({ gramSchmidt_ });\n//# sourceMappingURL=gram_schmidt.js.map","/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { tensor1d } from '../tensor1d';\nimport { op } from '../operation';\nimport { cast } from '../cast';\nimport { split } from '../split';\nimport { bincount } from '../bincount';\nimport { lessEqual } from '../less_equal';\nimport { greater } from '../greater';\nimport { sum } from '../sum';\nimport { add } from '../add';\nimport { mul } from '../mul';\nimport { div } from '../div';\nimport { sub } from '../sub';\nimport { round } from '../round';\nimport { where } from '../where';\nimport { fill } from '../fill';\nimport { slice } from '../slice';\nimport { range } from '../range';\nimport { tensor } from '../tensor';\nimport * as util from '../../util';\nimport { convertToTensor } from '../../tensor_util_env';\n/**\n * Performs image binarization with corresponding threshold\n * (depends on the method)value, which creates a binary image from a grayscale.\n * @param image 3d tensor of shape [imageHeight,imageWidth, depth],\n * where imageHeight and imageWidth must be positive.The image color\n * range should be [0, 255].\n * @param method Optional string from `'binary' | 'otsu'`\n * which specifies the method for thresholding. Defaults to 'binary'.\n * @param inverted Optional boolean whichspecifies\n * if colours should be inverted. Defaults to false.\n * @param threshValue Optional number which defines threshold value from 0 to 1.\n * Defaults to 0.5.\n * @return A 3d tensor of shape [imageHeight,imageWidth, depth], which\n * contains binarized image.\n */\nfunction threshold_(image, method = 'binary', inverted = false, threshValue = 0.5) {\n    const $image = convertToTensor(image, 'image', 'threshold');\n    /* 0.2989, 0.5870, 0.1140 are represent luma coefficients in CCIR601.\n    Reference for converting between RGB and grayscale: https://en.wikipedia.org/wiki/Luma_%28video%29  */\n    const RED_INTENCITY_COEF = 0.2989;\n    const GREEN_INTENCITY_COEF = 0.5870;\n    const BLUE_INTENCITY_COEF = 0.1140;\n    const totalPixelsInImage = $image.shape[0] * $image.shape[1];\n    let $threshold = mul(tensor1d([threshValue]), 255);\n    let r, g, b, grayscale;\n    util.assert($image.rank === 3, () => 'Error in threshold: image must be rank 3,' +\n        `but got rank ${$image.rank}.`);\n    util.assert($image.shape[2] === 3 || $image.shape[2] === 1, () => 'Error in threshold: ' +\n        'image color channel must be equal to 3 or 1' +\n        `but got ${$image.shape[2]}.`);\n    util.assert($image.dtype === 'int32' || $image.dtype === 'float32', () => 'Error in dtype: image dtype must be int32 or float32,' +\n        `but got dtype ${$image.dtype}.`);\n    util.assert(method === 'otsu' || method === 'binary', () => `Method must be binary or otsu, but was ${method}`);\n    if ($image.shape[2] === 3) {\n        [r, g, b] = split($image, [1, 1, 1], -1);\n        const $r = mul(r, RED_INTENCITY_COEF);\n        const $g = mul(g, GREEN_INTENCITY_COEF);\n        const $b = mul(b, BLUE_INTENCITY_COEF);\n        grayscale = add(add($r, $g), $b);\n    }\n    else {\n        grayscale = image;\n    }\n    if (method === 'otsu') {\n        const $histogram = bincount(cast(round(grayscale), 'int32'), tensor([]), 256);\n        $threshold = otsu($histogram, totalPixelsInImage);\n    }\n    const invCondition = inverted ?\n        lessEqual(grayscale, $threshold) : greater(grayscale, $threshold);\n    const result = cast(mul(invCondition, 255), 'int32');\n    return result;\n}\nfunction otsu(histogram, total) {\n    let bestThresh = tensor1d([-1]);\n    let bestInBetVar = tensor1d([0]);\n    let cInBetVar = tensor1d([0]);\n    let classFirst, classSecond, meanFirst, meanSec, weightForeground, weightBack;\n    for (let index = 0; index < histogram.size - 1; index++) {\n        classFirst = slice(histogram, 0, index + 1);\n        classSecond = slice(histogram, index + 1);\n        weightForeground = div(sum(classFirst), total);\n        weightBack = div(sum(classSecond), total);\n        const meanFirstDivA = sum(mul(classFirst, range(0, classFirst.size)));\n        meanFirst = div(meanFirstDivA, sum(classFirst));\n        const meanSecFill = fill(classSecond.shape, classFirst.size);\n        const meanSecAdd = add(range(0, classSecond.size), meanSecFill);\n        const meanSecMul = mul(classSecond, (meanSecAdd));\n        meanSec = div(sum(meanSecMul), sum(classSecond));\n        const cInBetVarSubA = sub(meanFirst, meanSec);\n        const cInBetVarSubB = sub(meanFirst, meanSec);\n        const cInBetVarMul = mul(weightForeground, weightBack);\n        cInBetVar = mul(mul(cInBetVarMul, cInBetVarSubA), cInBetVarSubB);\n        const condition = greater(cInBetVar, bestInBetVar);\n        bestInBetVar = where(condition, cInBetVar, bestInBetVar);\n        bestThresh = where(condition, tensor1d([index]), bestThresh);\n    }\n    return bestThresh;\n}\nexport const threshold = op({ threshold_ });\n//# sourceMappingURL=threshold.js.map","import { computeStrides, sizeFromShape } from '../util';\n/**\n * Validate gather nd inputs.\n *\n * @param tensor The tensor contains the source values.\n * @param indices The tensor contains the indices to slice the source.\n *\n * @returns [resultShape, numUpdates, sliceSize, strides]\n */\nexport function prepareAndValidate(tensor, indices) {\n    const tensorRank = tensor.shape.length;\n    const indicesRank = indices.shape.length;\n    if (tensorRank < 1) {\n        throw new Error('tf.gatherND() expects the input to be rank 1 or higher,' +\n            ` but the rank was ${tensorRank}.`);\n    }\n    if (indicesRank < 1) {\n        throw new Error('tf.gatherND() expects the indices to be rank 1 or higher,' +\n            ` but the rank was ${indicesRank}.`);\n    }\n    if (indices.dtype !== 'int32') {\n        throw new Error('tf.gatherND() expects the indices to be int32 type,' +\n            ` but the dtype was ${indices.dtype}.`);\n    }\n    if (indices.shape[indicesRank - 1] > tensorRank) {\n        throw new Error('index innermost dimension length must be <= tensor rank; saw: ' +\n            `${indices.shape[indicesRank - 1]} vs. ${tensorRank}`);\n    }\n    if (sizeFromShape(tensor.shape) === 0) {\n        throw new Error('Requested more than 0 entries, but input is empty.' +\n            ` Input shape: ${tensor.shape}.`);\n    }\n    const indicesShape = indices.shape;\n    const sliceRank = indicesShape[indicesShape.length - 1];\n    // The result shape is\n    //   indices.shape[:-1] + params.shape[indices.shape[-1]:]\n    let nResult = 1;\n    for (let i = 0; i < indicesShape.length - 1; ++i) {\n        nResult *= indicesShape[i];\n    }\n    const inputShape = tensor.shape;\n    const resultShape = indicesShape.slice();\n    resultShape.pop();\n    let sliceSize = 1;\n    for (let i = sliceRank; i < tensorRank; ++i) {\n        sliceSize *= inputShape[i];\n        resultShape.push(inputShape[i]);\n    }\n    const strides = [...computeStrides(tensor.shape).map(stride => stride / sliceSize),\n        1].slice(0, sliceRank);\n    return [resultShape, nResult, sliceSize, strides];\n}\n//# sourceMappingURL=gather_nd_util.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { NonMaxSuppressionV4 } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { op } from '../operation';\n/**\n * Asynchronously performs non maximum suppression of bounding boxes based on\n * iou (intersection over union), with an option to pad results.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param padToMaxOutputSize Defalts to false. If true, size of output\n *     `selectedIndices` is padded to maxOutputSize.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - validOutputs: A scalar denoting how many elements in `selectedIndices`\n *       are valid. Valid elements occur first, then padding.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction nonMaxSuppressionPadded_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, padToMaxOutputSize = false) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, null /* softNmsSigma */);\n    const $maxOutputSize = params.maxOutputSize;\n    const $iouThreshold = params.iouThreshold;\n    const $scoreThreshold = params.scoreThreshold;\n    const inputs = { boxes: $boxes, scores: $scores };\n    const attrs = {\n        maxOutputSize: $maxOutputSize,\n        iouThreshold: $iouThreshold,\n        scoreThreshold: $scoreThreshold,\n        padToMaxOutputSize\n    };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const result = ENGINE.runKernel(NonMaxSuppressionV4, inputs, attrs);\n    return { selectedIndices: result[0], validOutputs: result[1] };\n}\nexport const nonMaxSuppressionPadded = op({ nonMaxSuppressionPadded_ });\n//# sourceMappingURL=non_max_suppression_padded.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPool } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport * as conv_util from './conv_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Computes the 2D max pooling of an image.\n *\n * @param x The input tensor, of rank 4 or rank 3 of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.\n * @param filterSize The filter size: `[filterHeight, filterWidth]`. If\n *     `filterSize` is a single number, then `filterHeight == filterWidth`.\n * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If\n *     `strides` is a single number, then `strideHeight == strideWidth`.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in dilated pooling. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param pad The type of padding algorithm.\n *    - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *    - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *    - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n */\nfunction maxPool_(x, filterSize, strides, pad, dimRoundingMode) {\n    const $x = convertToTensor(x, 'x', 'maxPool');\n    const dilations = 1;\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in maxPool: input must be rank 4 but got rank ${x4D.rank}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in maxPool: Either strides or dilations must be 1. ' +\n        `Got strides ${strides} and dilations '${dilations}'`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in maxPool: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const inputs = { x: x4D };\n    const attrs = { filterSize, strides, pad, dimRoundingMode };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(MaxPool, inputs, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    return res;\n}\nexport const maxPool = op({ maxPool_ });\n//# sourceMappingURL=max_pool.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { FloorDiv } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting.\n * The result is rounded with floor function.\n *\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 9, 16]);\n * const b = tf.tensor1d([1, 2, 3, 4]);\n *\n * a.floorDiv(b).print();  // or tf.div(a, b)\n * ```\n *\n * ```js\n * // Broadcast div a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(2);\n *\n * a.floorDiv(b).print();  // or tf.floorDiv(a, b)\n * ```\n *\n * @param a The first tensor as the numerator.\n * @param b The second tensor as the denominator. Must have the same dtype as\n * `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction floorDiv_(a, b) {\n    let $a = convertToTensor(a, 'a', 'floorDiv');\n    let $b = convertToTensor(b, 'b', 'floorDiv');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(FloorDiv, inputs);\n}\nexport const floorDiv = op({ floorDiv_ });\n//# sourceMappingURL=floorDiv.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { Reduction } from '../loss_ops_utils';\nimport { op } from '../operation';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the absolute difference loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction absoluteDifference_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'absoluteDifference');\n    const $predictions = convertToTensor(predictions, 'predictions', 'absoluteDifference');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'absoluteDifference');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in absoluteDifference: ');\n    const losses = abs(sub($labels, $predictions));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const absoluteDifference = op({ absoluteDifference_ });\n//# sourceMappingURL=absolute_difference.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LogicalNot } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns the truth value of `NOT x` element-wise.\n *\n * ```js\n * const a = tf.tensor1d([false, true], 'bool');\n *\n * a.logicalNot().print();\n * ```\n *\n * @param x The input tensor. Must be of dtype 'bool'.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalNot_(x) {\n    const $x = convertToTensor(x, 'x', 'logicalNot', 'bool');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(LogicalNot, inputs);\n}\nexport const logicalNot = op({ logicalNot_ });\n//# sourceMappingURL=logical_not.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../tensor_util_env';\nimport { parseAxisParam } from '../util';\nimport { expandShapeToKeepDim } from './axis_util';\nimport { cast } from './cast';\nimport { mean } from './mean';\nimport { op } from './operation';\nimport { reshape } from './reshape';\nimport { square } from './square';\nimport { sub } from './sub';\n/**\n * Calculates the mean and variance of `x`. The mean and variance are\n * calculated by aggregating the contents of `x` across `axes`. If `x` is\n * 1-D and `axes = [0]` this is just the mean and variance of a vector.\n *\n * @param x The input tensor.\n * @param axis The dimension(s) along with to compute mean and\n *     variance. By default it reduces all dimensions.\n * @param keepDims If true, the moments have the same dimensionality as the\n *     input.\n * @return An object with two keys: `mean` and `variance`.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction moments_(x, axis = null, keepDims = false) {\n    x = convertToTensor(x, 'x', 'moments');\n    const axes = parseAxisParam(axis, x.shape);\n    const xMean = mean(x, axes, keepDims);\n    let keepDimsShape = xMean.shape;\n    if (!keepDims) {\n        keepDimsShape = expandShapeToKeepDim(xMean.shape, axes);\n    }\n    const devSquared = square(sub(cast(x, 'float32'), reshape(xMean, keepDimsShape)));\n    const variance = mean(devSquared, axes, keepDims);\n    return { mean: xMean, variance };\n}\nexport const moments = op({ moments_ });\n//# sourceMappingURL=moments.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedDepthwiseConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from '../depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from '../depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../depthwise_conv2d_native_backprop_input';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedDepthwiseConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    const $x = convertToTensor(x, 'x', 'depthwiseConv2d');\n    const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` +\n        `rank ${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` +\n        `but got rank ${$filter.rank}.`);\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` +\n        `(${x4D.shape[3]}) must match the inChannels dimension in ` +\n        `filter ${$filter.shape[2]}.`);\n    if (dilations == null) {\n        dilations = [1, 1];\n    }\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' +\n        `be 1. Got strides ${strides} and dilations '${dilations}'`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in fused depthwiseConv2d: pad must be an integer when ` +\n            `using dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true /* depthwise */);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n    }\n    const grad = (dy, saved) => {\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${dilations}'`);\n        const [$filter, x4D, y, bias] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        const xDer = depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, strides, pad, dilations, dimRoundingMode);\n        const filterDer = depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad, dilations, dimRoundingMode);\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [xDer, filterDer, biasDer];\n        }\n        return [xDer, filterDer];\n    };\n    const inputs = {\n        x: x4D,\n        filter: $filter,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = {\n        strides,\n        pad,\n        dataFormat,\n        dilations,\n        dimRoundingMode,\n        activation,\n        leakyreluAlpha\n    };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((x4D, filter, save) => {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n            save([filter, x4D, res]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOp(x4D, $filter);\n    }\n    else {\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n            save([filter, x4D, res, bias]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOpWithBias(x4D, $filter, $bias);\n    }\n}\nexport const depthwiseConv2d = op({ fusedDepthwiseConv2d_ });\n//# sourceMappingURL=depthwise_conv2d.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { conv2d as unfusedConv2d } from '../conv2d';\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha }) {\n    activation = activation || 'linear';\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    const $x = convertToTensor(x, 'x', 'conv2d');\n    const $filter = convertToTensor(filter, 'filter', 'conv2d');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` +\n        `${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` +\n        `${$filter.rank}.`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in fused conv2d: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match ` +\n        `input depth for filter ${$filter.shape[2]}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' +\n        `Got strides ${strides} and dilations '${dilations}'`);\n    util.assert(dataFormat === 'NHWC', () => `Error in conv2d: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n    }\n    const grad = (dy, saved) => {\n        const [$filter, x4D, y, $bias] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' +\n            `dilation rates greater than 1 ` +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n        const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n        const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n        const der = [xDer, filterDer];\n        if ($bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            der.push(biasDer);\n        }\n        return der;\n    };\n    const inputs = {\n        x: x4D,\n        filter: $filter,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = {\n        strides,\n        pad,\n        dataFormat,\n        dilations,\n        dimRoundingMode,\n        activation,\n        leakyreluAlpha\n    };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((x4D, filter, save) => {\n            let res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(FusedConv2D, inputs, attrs);\n            save([filter, x4D, res]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOp(x4D, $filter);\n    }\n    else {\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n            let res = ENGINE.runKernel(FusedConv2D, inputs, attrs);\n            save([filter, x4D, res, bias]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOpWithBias(x4D, $filter, $bias);\n    }\n}\nexport const conv2d = op({ fusedConv2d_ });\n//# sourceMappingURL=conv2d.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LRN } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Normalizes the activation of a local neighborhood across or within\n * channels.\n *\n * @param x The input tensor. The 4-D input tensor is treated as a 3-D array\n *     of 1D vectors (along the last dimension), and each vector is\n *     normalized independently.\n * @param depthRadius The number of adjacent channels in the 1D normalization\n *     window.\n * @param bias A constant bias term for the basis.\n * @param alpha A scale factor, usually positive.\n * @param beta An exponent.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction localResponseNormalization_(x, depthRadius = 5, bias = 1, alpha = 1, beta = 0.5) {\n    const $x = convertToTensor(x, 'x', 'localResponseNormalization');\n    util.assert($x.rank === 4 || $x.rank === 3, () => `Error in localResponseNormalization: x must be rank 3 or 4 but got\n               rank ${$x.rank}.`);\n    util.assert(util.isInt(depthRadius), () => `Error in localResponseNormalization: depthRadius must be an ` +\n        `integer but got depthRadius ${depthRadius}.`);\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    const inputs = { x: x4D };\n    const attrs = { depthRadius, bias, alpha, beta };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(LRN, inputs, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    else {\n        return res;\n    }\n}\nexport const localResponseNormalization = op({ localResponseNormalization_ });\n//# sourceMappingURL=local_response_normalization.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { NonMaxSuppressionV5 } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { op } from '../operation';\n/**\n * Performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * This op also supports a Soft-NMS mode (c.f.\n * Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score\n * of other overlapping boxes, therefore favoring different regions of the image\n * with high scores. To enable this Soft-NMS mode, set the `softNmsSigma`\n * parameter to be larger than 0.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param softNmsSigma A float representing the sigma parameter for Soft NMS.\n *     When sigma is 0, it falls back to nonMaxSuppression.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - selectedScores: A 1D tensor with the corresponding scores for each\n *       selected box.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction nonMaxSuppressionWithScore_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, softNmsSigma = 0.0) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);\n    maxOutputSize = params.maxOutputSize;\n    iouThreshold = params.iouThreshold;\n    scoreThreshold = params.scoreThreshold;\n    softNmsSigma = params.softNmsSigma;\n    const inputs = { boxes: $boxes, scores: $scores };\n    const attrs = { maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const result = ENGINE.runKernel(NonMaxSuppressionV5, inputs, attrs);\n    return { selectedIndices: result[0], selectedScores: result[1] };\n}\nexport const nonMaxSuppressionWithScore = op({ nonMaxSuppressionWithScore_ });\n//# sourceMappingURL=non_max_suppression_with_score.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { mul } from './mul';\nimport { neg } from './neg';\nimport { op } from './operation';\nimport { sigmoid } from './sigmoid';\nimport { softplus } from './softplus';\n/**\n * Computes log sigmoid of the input `tf.Tensor` element-wise:\n * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.logSigmoid().print();  // or tf.logSigmoid(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction logSigmoid_(x) {\n    const $x = convertToTensor(x, 'x', 'logSigmoid');\n    // Use a custom gradient to maintain previous implementation.\n    // There is no LogSigmoid kernel in TF so we can't use engine.runKernel\n    // directly\n    const customOp = customGrad((x) => {\n        // TODO(yassogba) we can remove the chained softplus call here only\n        // after backends have modualrized softplus at which point we can call\n        // engine runKernel(..., Sotfplus, ...) directly.\n        const value = neg(softplus(neg(x)));\n        const gradFunc = (dy) => {\n            const derX = mul(dy, sigmoid(neg(x)));\n            return derX;\n        };\n        return { value, gradFunc };\n    });\n    return customOp($x);\n}\nexport const logSigmoid = op({ logSigmoid_ });\n//# sourceMappingURL=log_sigmoid.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Fill } from '../kernel_names';\n/**\n * Creates a `tf.Tensor` filled with a scalar value.\n *\n * ```js\n * tf.fill([2, 2], 4).print();\n * ```\n *\n * @param shape An array of integers defining the output tensor shape.\n * @param value The scalar value to fill the tensor with.\n * @param dtype The type of an element in the resulting tensor. Defaults to\n * 'float'.\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nfunction fill(shape, value, dtype) {\n    const attrs = { shape, value, dtype };\n    return ENGINE.runKernel(Fill, {}, attrs);\n}\nexport { fill };\n//# sourceMappingURL=fill.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Neg } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes `-1 * x` element-wise.\n *\n * ```js\n * const x = tf.tensor2d([1, 2, -2, 0], [2, 2]);\n *\n * x.neg().print();  // or tf.neg(x)\n * ```\n *\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction neg_(x) {\n    const $x = convertToTensor(x, 'x', 'neg');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(Neg, inputs);\n}\nexport const neg = op({ neg_ });\n//# sourceMappingURL=neg.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { cast } from './cast';\nimport { exp } from './exp';\nimport { log } from './log';\nimport { max } from './max';\nimport { mul } from './mul';\nimport { op } from './operation';\nimport { sub } from './sub';\nimport { sum } from './sum';\n/**\n * Computes the log softmax.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param axis The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction logSoftmax_(logits, axis = -1) {\n    const $logits = convertToTensor(logits, 'logits', 'logSoftmax');\n    if (axis === -1) {\n        axis = $logits.rank - 1;\n    }\n    if (axis !== $logits.rank - 1) {\n        throw Error('Log Softmax along a non-last dimension is not yet supported. ' +\n            `Logits was rank ${$logits.rank} and axis was ${axis}`);\n    }\n    // const forward: ForwardFunc<Tensor> = (backend, save) => {\n    //   const keepDims = true;\n    //   const xMax = max(logits, axis, true);\n    //   const shifted = sub(logits, xMax);\n    //   const value =\n    //       sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis,\n    //       keepDims)));\n    //   save([value]);\n    //   return value;\n    // };\n    // Use a custom gradient for numerical stability.\n    const customOp = customGrad((logits, save) => {\n        const keepDims = true;\n        const xMax = max(logits, axis, true);\n        const shifted = sub(logits, xMax);\n        const value = sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis, keepDims)));\n        save([value]);\n        const gradFunc = (dy, saved) => {\n            const [value] = saved;\n            const keepDims = true;\n            const softmax = exp(value);\n            return sub(dy, mul(sum(dy, axis, keepDims), softmax));\n        };\n        return { value, gradFunc };\n    });\n    return customOp($logits);\n    // TODO Use Engine.runKernel when CPU/WebGL/WASM backends implement this.\n    // const inputs: LogSoftmaxInputs = {logits: $logits};\n    // const attrs: LogSoftmaxAttrs = {axis};\n    // return ENGINE.runKernel(\n    //            LogSoftmax, inputs as {} as NamedTensorMap,\n    //            attrs as {} as NamedAttrMap);\n}\nexport const logSoftmax = op({ logSoftmax_ });\n//# sourceMappingURL=log_softmax.js.map"],"names":["qr2d","x","fullMatrices","tidy","shape","length","m","n","q","r","one2D","w","iters","j","rTemp","wTemp","qTemp","rjEnd1","normX","rjj","s","u1","wPre","tau","rjEndAll","tauTimesW","wT","rTimesTau","tawTimesWT","qAllJEnd","qTimesTau","qr","op","qr_","rank","outerDimsProd","slice","reduce","value","prev","x2ds","q2ds","r2ds","forEach","x2d","q2d","r2d","push","transform","transform_","image","transforms","interpolation","fillMode","fillValue","outputShape","$image","$transforms","inputs","attrs","runKernel","inTopKAsync","async","predictions","targets","k","$predictions","$targets","lastDim","predictionsVals","data","targetsVals","batch","size","precision","b","offset","vals","subarray","valAndInd","i","index","sort","a","dispose","nonMaxSuppressionPaddedAsync","boxes","scores","maxOutputSize","iouThreshold","scoreThreshold","Number","NEGATIVE_INFINITY","padToMaxOutputSize","$boxes","$scores","params","$maxOutputSize","$iouThreshold","$scoreThreshold","boxesVals","scoresVals","Promise","all","selectedIndices","validOutputs","t","isNaN","isNaN_","nonMaxSuppressionWithScoreAsync","softNmsSigma","boxesAndScores","selectedScores","multiRNNCell","multiRNNCell_","lstmCells","c","h","$data","$c","$h","input","newStates","output","newC","newH","mul","mul_","$a","$b","makeTypesMatch","logicalOr","logicalOr_","log1p","log1p_","mean","mean_","axis","keepDims","minimum","minimum_","dtype","logSumExp","logSumExp_","$x","axes","xMax","d","res","newShape","greaterEqual","greaterEqual_","logicalAnd","logicalAnd_","maxPool3d","maxPool3d_","filterSize","strides","pad","dimRoundingMode","dataFormat","x5D","reshapedTo5D","meshgrid","y","indexing","TypeError","undefined","$y","floor","floor_","logicalXor","logicalXor_","logLoss","logLoss_","labels","weights","epsilon","reduction","SUM_BY_NONZERO_WEIGHTS","$labels","$weights","one","epsilonScalar","l1","l2","losses","meanSquaredError","meanSquaredError_","maxPoolGrad","maxPoolGrad_","dy","$dy","$input","$output","Reduction","maxPool3dGrad","maxPool3dGrad_","dy5D","input5D","output5D","matMul","fusedMatMul_","transposeA","transposeB","bias","activation","preluActivationWeights","leakyreluAlpha","state","gradientDepth","result","innerShapeA","innerShapeB","outerShapeA","outerShapeB","outerDimsA","outerDimsB","batchDimA","batchDimB","outShape","concat","a3D","b3D","$bias","$preluActivationWeights","grad","saved","dyActivation","aDer","bDer","customOp","save","gradFunc","customOpWithBias","maximum","maximum_","cropAndResize","cropAndResize_","boxInd","cropSize","method","extrapolationValue","$boxInd","numBoxes","isInf","isInf_","imag","imag_","bandPart","bandPart_","numLower","numUpper","M","N","Error","ij","inBand","zero","map","mat","cosineDistance","cosineDistance_","huberLoss","huberLoss_","delta","deltaScalar","error","quadratic","linear","max","max_","reductionIndices","resizeNearestNeighbor","resizeNearestNeighbor_","images","alignCorners","halfPixelCenters","$images","batchImages","reshapedTo4D","log","log_","resizeBilinear","resizeBilinear_","maxPoolWithArgmax","maxPoolWithArgmax_","includeBatchInIndex","indexes","linspace","start","stop","num","gather","gather_","indices","batchDims","gatherND","gatherND_","$indices","nonMaxSuppression","nonMaxSuppression_","lessEqual","lessEqual_","min","min_","sigmoidCrossEntropy","sigmoidCrossEntropy_","multiClassLabels","logits","labelSmoothing","$multiClassLabels","$logits","labelSmoothingScalar","half","maxOutput","outputXTarget","sigmoidOutput","sigmoidCrossEntropyWithLogits_","mod","mod_","less","less_","greater","greater_","rotateWithOffset","rotateWithOffset_","radians","center","matMul_","movingAverage","movingAverage_","v","decay","step","zeroDebias","$v","$decay","assertTypesMatch","oneMinusDecay","update","$step","mirrorPad","mirrorPad_","paddings","mode","shapeOffset","leakyRelu","leakyRelu_","alpha","isFinite","isFinite_","getFusedDyActivation","getFusedBiasGradient","reduceAxes","applyActivation","shouldFuse","computeWeightedLoss","computeWeightedLoss_","$losses","weightedLoss","NONE","SUM","MEAN","broadcastFactor","broadcastedWeights","numNonZeros","softmaxCrossEntropy","softmaxCrossEntropy_","onehotLabels","$onehotLabels","numClasses","dim","lse","logResult","costVector","dyShape","softmaxCrossEntropyWithLogits_","nonMaxSuppressionAsync","hingeLoss","hingeLoss_","multinomial","multinomial_","numSamples","seed","normalized","numOutcomes","origRank","Math","random","localResponseNormalizationBackprop","localResponseNormalizationBackprop_","depthRadius","beta","flipLeftRight","flipLeftRight_","gramSchmidt","gramSchmidt_","xs","inputIsTensor2D","Array","isArray","ys","xs1d","proj","threshold","threshold_","inverted","threshValue","totalPixelsInImage","g","grayscale","$threshold","$r","$g","histogram","total","classFirst","classSecond","meanFirst","meanSec","weightForeground","weightBack","bestThresh","bestInBetVar","cInBetVar","meanFirstDivA","meanSecFill","meanSecAdd","meanSecMul","cInBetVarSubA","cInBetVarSubB","cInBetVarMul","condition","otsu","invCondition","prepareAndValidate","tensor","tensorRank","indicesRank","indicesShape","sliceRank","nResult","inputShape","resultShape","pop","sliceSize","stride","nonMaxSuppressionPadded","nonMaxSuppressionPadded_","maxPool","maxPool_","x4D","floorDiv","floorDiv_","absoluteDifference","absoluteDifference_","logicalNot","logicalNot_","moments","moments_","xMean","keepDimsShape","devSquared","variance","depthwiseConv2d","fusedDepthwiseConv2d_","filter","dilations","$filter","convInfo","xDer","filterDer","conv2d","fusedConv2d_","der","biasDer","localResponseNormalization","localResponseNormalization_","nonMaxSuppressionWithScore","nonMaxSuppressionWithScore_","logSigmoid","logSigmoid_","fill","neg","neg_","logSoftmax","logSoftmax_","shifted","softmax"],"sourceRoot":""}