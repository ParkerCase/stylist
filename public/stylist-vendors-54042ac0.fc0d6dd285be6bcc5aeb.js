"use strict";(self.webpackChunkStylistWidget=self.webpackChunkStylistWidget||[]).push([[7888],{79730:function(t,e,n){n.d(e,{Ar:function(){return c},Wd:function(){return y},X6:function(){return m},bP:function(){return g},eO:function(){return d}});var s=n(9495),i=n(91686),h=n(39459),o=n(15841),a=n(59351),u=n(44813),r=n(63057),l=n(54390),p=n(71765);class d{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class c{constructor(t,e,n,s,o,a,u){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=o,this.outputTensorIndex=u,this.id=(0,i.j)(),null!=a&&(this.originalName=(0,h.BC)(a),this.name=(0,h.Uc)(this.originalName)),this.rank=e.length}}let f=0;class g{constructor(t,e){this.callArgs=e,this.id=f++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const n of t.inboundLayers)null!=n&&n.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let b=0;class y extends s.serialization.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=b++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=u.uc(t)+"_"+(0,i.v)(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new o.bu(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new o.Qp(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return u.wL(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return u.wL(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new o.l7(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);if(0===this.inboundNodes.length)throw new o.l7(`Layer ${this.name} is not connected, no input to return.`);return u.wL(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new o.l7(`Layer ${this.name} has no inbound nodes.`);if(this.inboundNodes.length>1)throw new o.l7(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);return u.wL(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){const e=u.st(t);if(null==this.inputSpec||0===this.inputSpec.length)return;const n=u.st(this.inputSpec);if(e.length!==n.length)throw new o.Qp(`Layer ${this.name} expects ${n.length} inputs, but it received ${e.length} input tensors. Input received: ${t}`);for(let s=0;s<e.length;s++){const t=e[s],i=n[s];if(null==i)continue;const h=t.rank;if(null!=i.ndim&&h!==i.ndim)throw new o.Qp(`Input ${s} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${h}`);if(null!=i.maxNDim&&h>i.maxNDim)throw new o.Qp(`Input ${s} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${h}`);if(null!=i.minNDim&&h<i.minNDim)throw new o.Qp(`Input ${s} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${h}.`);if(null!=i.dtype&&t.dtype!==i.dtype)throw new o.Qp(`Input ${s} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${t.dtype}.`);if(i.axes){const e=t.shape;for(const t in i.axes){const n=Number(t),h=i.axes[t],a=n>=0?e[n]:e[e.length+n];if(null!=h&&-1===[h,null].indexOf(a))throw new o.Qp(`Input ${s} is incompatible with layer ${this.name}: expected axis ${n} of input shape to have value ${h} but got shape ${e}.`)}}if(null!=i.shape)for(let e=0;e<i.shape.length;++e){const n=i.shape[e],h=t.shape[e];if(null!=n&&null!=h&&n!==h)throw new o.Qp(`Input ${s} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${t.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=u.st(t),s=function(t){let e=!0;for(const n of u.st(t))if(!(n instanceof c)){e=!1;break}return e}(t),i=function(t){let e=!0;for(const n of u.st(t))if(n instanceof c){e=!1;break}return e}(t);if(s===i)throw new o.Qp("Arguments to apply() must be all SymbolicTensors or all Tensors");return(0,h.IU)(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of u.st(t))e.push(n.shape);this.build(u.wL(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);this.supportsMasking&&this.setMaskMetadata(t,s);const i=u.st(s),h=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),h.push(t);if(s=u.wL(h),null!=this.activityRegularizer)throw new o.EH("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=u.st(t);const e=[];for(const n of t)e.push(n.shape);return u.wL(e)}(t),s=this.computeOutputShape(n);let i;const h="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map(((n,s)=>new c(h,n,this,u.st(t),e,this.name,s))):new c(h,s,this,u.st(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new o.EH("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length);else{let e=!1;this.batchInputShape.forEach(((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)}))}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new o.l7(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new o.l7(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new o.bu(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return l.Y(this.weights)}build(t){this.built=!0}getWeights(t=!1){return(0,p.ex)(t?this.trainableWeights:this.weights)}setWeights(t){(0,s.tidy)((()=>{const e=this.weights;if(e.length!==t.length)throw new o.Qp(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${e.length} weights. Provided weights: ${t}...`);if(0===e.length)return;const n=[],i=(0,p.ex)(e);for(let h=0;h<i.length;++h){const a=i[h],u=e[h],r=t[h];if(!s.util.arraysEqual(a.shape,r.shape))throw new o.Qp(`Layer weight shape ${a.shape} not compatible with provided weight shape ${r.shape}`);n.push([u,r])}(0,p.UM)(n)}))}addWeight(t,e,n,s,i,h,u,r){if(-1!==this._addedWeightNames.indexOf(t))throw new o.Qp(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=null!=r?r():(0,a.Fe)("zeros"));const l=s.apply(e,n),d=new p.eR(l,n,t,h,u);return l.dispose(),null!=i&&this.addLoss((()=>i.apply(d.read()))),null==h&&(h=!0),h?this._trainableWeights.push(d):this._nonTrainableWeights.push(d),d}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=u.st(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach((t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return e}setMaskMetadata(t,e,n){if(!this.supportsMasking)return;const s=this.computeMask(t,n),i=u.st(e),h=u.st(s);if(i.length!==h.length)throw new Error(`${this.name} outputs ${i.length} tensors but ${i.length} masks for those tensors`);for(let o=0;o<i.length;o++)i[o].kerasMask=h[o]}addInboundNode(t,e,n,s,i,h,o=null){const a=u.st(t);e=u.st(e),n=u.st(n),s=u.st(s),i=r.FS(i),h=r.FS(h);const l=[],p=[],d=[];for(const u of a)l.push(u.sourceLayer),p.push(u.nodeIndex),d.push(u.tensorIndex);new g({outboundLayer:this,inboundLayers:l,nodeIndices:p,tensorIndices:d,inputTensors:a,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:h},o);for(let u=0;u<e.length;u++)e[u].sourceLayer=this,e[u].nodeIndex=this.inboundNodes.length-1,e[u].tensorIndex=u}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0===--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function m(t,e,n){if((null==e||null!=n&&n>0)&&(e=t.sourceLayer,n=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[n];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let n=0;n<t.inboundLayers.length;n++){const s=m(t.inputTensors[n],t.inboundLayers[n],t.nodeIndices[n]);for(const t of s)-1===e.indexOf(t)&&e.push(t)}return e}}}}}]);
//# sourceMappingURL=stylist-vendors-54042ac0.fc0d6dd285be6bcc5aeb.js.map