{"version":3,"file":"stylist-vendors-54c80241.d363dad177d8b05a440f.js","mappings":"sPAgCO,MAAMA,EAUT,WAAAC,CAAYC,EAASC,EAAcC,EAAcC,GAAiB,GAC9DC,KAAKJ,QAAUA,EACfI,KAAKH,aAAeA,EACpBG,KAAKF,aAAeA,EACL,MAAXF,GACAA,EAAQK,SAAQC,IACZ,GAAIJ,IAAiBI,EAAOC,MACxB,MAAM,IAAIC,MAAM,mCAAmCN,wBAAmCI,EAAOC,UAEjG,QAAoCN,EAAcK,EAAOG,MAAO,gCAChE,IAAAC,MAAKJ,EAAO,IAGpBF,KAAKO,UAAW,IAAAC,QAAO,GACvBR,KAAKD,eAAiBA,GACtB,IAAAO,MAAKN,KAAKO,SACd,CACA,MAAIE,GACA,OAAOT,KAAKO,SAASE,EACzB,CAIA,IAAAC,GACI,OAAO,IAAIhB,EAAW,IAAIM,KAAKJ,SAAUI,KAAKH,aAAcG,KAAKF,aACrE,CAIA,aAAAa,CAAcC,GACVZ,KAAKJ,QAAQK,SAAQC,IACF,MAAXU,GAAoBA,EAAQC,IAAIX,EAAOO,KACvCP,EAAOY,SACX,IAEJd,KAAKJ,QAAQmB,OAAS,EACtBf,KAAKO,SAASO,SAClB,CAIA,IAAAE,GACI,OAAOhB,KAAKJ,QAAQmB,MACxB,CAQA,KAAAE,CAAMpB,EAAcC,EAAcoB,GAAc,GAC5C,GAAIpB,IAAiBE,KAAKF,aACtB,MAAM,IAAIM,MAAM,mCAAmCN,wBAAmCE,KAAKF,gBAE/F,IAAqB,IAAjBoB,GAAsBlB,KAAKJ,QAAQmB,SAAWG,EAC9C,MAAM,IAAId,MAAM,kCAAkCc,kCAA4ClB,KAAKJ,QAAQmB,qBAE/G,QAAoClB,EAAcG,KAAKH,aAAc,+BACrE,MAAMsB,GAAqB,QAAkBnB,KAAKH,aAAcG,KAAKJ,QAASC,GAC9E,OAAO,IAAAuB,OAAK,KACR,MAAMC,EAAkBrB,KAAKJ,QAAQ0B,KAAIpB,IAAU,IAAAqB,SAAQrB,EAAQiB,KACnE,OAAO,IAAAF,OAAMI,EAAiB,EAAE,GAExC,CAMA,OAAAG,CAAQ3B,EAAcC,GAClB,GAAIA,IAAiBE,KAAKF,aACtB,MAAM,IAAIM,MAAM,mCAAmCN,wBAAmCE,KAAKF,gBAE/F,GAAoB,IAAhBE,KAAKgB,OACL,MAAM,IAAIZ,MAAM,qCAEpB,MAAMe,GAAqB,QAAkBnB,KAAKH,aAAcG,KAAKJ,QAASC,GACxEK,EAASF,KAAKJ,QAAQ6B,MAE5B,OADA,QAAoCvB,EAAOG,MAAOR,EAAc,gCACzD,IAAA0B,SAAQrB,EAAQiB,EAC3B,CAKA,QAAAO,CAASxB,GACL,GAAIA,EAAOC,QAAUH,KAAKF,aACtB,MAAM,IAAIM,MAAM,mCAAmCF,EAAOC,4BAA4BH,KAAKF,gBAG/F,IADA,QAAoCI,EAAOG,MAAOL,KAAKH,aAAc,+BACjEG,KAAKD,iBAAmBC,KAAKgB,OAC7B,MAAM,IAAIZ,MAAM,6CAEpB,IAAAE,MAAKJ,GACLF,KAAKJ,QAAQ+B,KAAKzB,EACtB,CAKA,MAAA0B,CAAOZ,GACH,GAAIA,EAAO,EACP,MAAM,IAAIZ,MAAM,0DAA0DY,KAE9E,IAA6B,IAAzBhB,KAAKD,gBAAyBiB,EAAOhB,KAAKD,eAC1C,MAAM,IAAIK,MAAM,+BAA+BY,8BAAiChB,KAAKD,mBAEzFC,KAAKJ,QAAQmB,OAASC,CAC1B,CAOA,OAAAa,CAAQC,EAAcjC,EAAcC,GAChC,GAAIA,IAAiBE,KAAKF,aACtB,MAAM,IAAIM,MAAM,mCAAmCN,wBAAmCE,KAAKF,gBAE/F,GAAIgC,EAAe,GAAKA,EAAe9B,KAAKJ,QAAQmB,OAChD,MAAM,IAAIX,MAAM,4BAA4B0B,oBAA+B9B,KAAKJ,QAAQmB,oBAE5F,GAAkC,MAA9Bf,KAAKJ,QAAQkC,GACb,MAAM,IAAI1B,MAAM,oBAAoB0B,eAExC,QAAoC9B,KAAKJ,QAAQkC,GAAczB,MAAOR,EAAc,+BACpF,MAAMsB,GAAqB,QAAkBnB,KAAKH,aAAcG,KAAKJ,QAASC,GAC9E,OAAO,IAAA0B,SAAQvB,KAAKJ,QAAQkC,GAAeX,EAC/C,CAMA,OAAAY,CAAQD,EAAc5B,GAClB,GAAIA,EAAOC,QAAUH,KAAKF,aACtB,MAAM,IAAIM,MAAM,mCAAmCF,EAAOC,4BAA4BH,KAAKF,gBAE/F,GAAIgC,EAAe,IACU,IAAzB9B,KAAKD,gBAAyB+B,GAAgB9B,KAAKD,eACnD,MAAM,IAAIK,MAAM,yBAAyB0B,wBAAmC9B,KAAKD,6BAErF,QAAoCC,KAAKH,aAAcK,EAAOG,MAAO,gCACrE,IAAAC,MAAKJ,GACLF,KAAKJ,QAAQkC,GAAgB5B,CACjC,CAQA,MAAA8B,CAAOC,EAASnC,EAAcD,GAC1B,GAAIC,IAAiBE,KAAKF,aACtB,MAAM,IAAIM,MAAM,mCAAmCN,wBAAmCE,KAAKF,iBAE/F,QAAoCE,KAAKH,aAAcA,EAAc,+BAGrEoC,EAAUA,EAAQC,MAAM,EAAGlC,KAAKgB,QAChC,MAAMG,GAAqB,QAAkBnB,KAAKH,aAAcG,KAAKJ,QAASC,GAC9E,OAAuB,IAAnBoC,EAAQlB,QACD,IAAAb,QAAO,GAAI,CAAC,GAAGiC,OAAOhB,KAE1B,IAAAC,OAAK,KACR,MAAMxB,EAAUqC,EAAQX,KAAIc,IAAK,IAAAb,SAAQvB,KAAKJ,QAAQwC,GAAIjB,KAC1D,OAAO,IAAAF,OAAMrB,EAAS,EAAE,GAEhC,CAMA,MAAAuC,CAAOrC,EAAcD,GACjB,GAAMC,GAAgBA,IAAiBE,KAAKF,aACxC,MAAM,IAAIM,MAAM,uBAAuBJ,KAAKF,2CAA2CA,MAE3F,QAAoCE,KAAKH,aAAcA,EAAc,+BACrE,MAAMsB,GAAqB,QAAkBnB,KAAKH,aAAcG,KAAKJ,QAASC,GAC9E,OAAoB,IAAhBG,KAAKgB,QACE,IAAAd,QAAO,GAAI,CAAC,GAAGiC,OAAOhB,KAE1B,IAAAC,OAAK,KACR,MAAMxB,EAAUI,KAAKJ,QAAQ0B,KAAIe,IAAK,IAAAd,SAAQc,EAAGlB,KACjD,OAAO,IAAAgB,QAAOvC,EAAS,EAAE,GAEjC,EAOG,SAAS0C,EAAWpC,EAAQL,EAAcC,GAC7C,MAAMK,EAAQD,EAAOC,MACrB,GAAID,EAAOG,MAAMU,OAAS,EACtB,MAAM,IAAIX,MAAM,oDAAoDF,EAAOG,SAE/E,GAAIH,EAAOC,QAAUL,EACjB,MAAM,IAAIM,MAAM,mCAAmCF,EAAOC,4BAA4BL,KAE1F,MAAMyC,EAAqBrC,EAAOG,MAAM6B,MAAM,IAC9C,QAAoCK,EAAoB1C,EAAc,+BACtE,MAAM2C,GAAa,IAAAC,SAAQvC,GAC3B,OAAO,IAAIR,EAAW8C,EAAY3C,EAAcM,EACpD,CAOO,SAASuC,EAAQ7C,EAAcC,EAAcoB,GAChD,OAAO,IAAIxB,EAAW,GAAIG,EAAcC,EAAcoB,EAC1D,CAQO,SAASyB,EAAQzC,EAAQ+B,EAASpC,EAAcqB,GACnD,GAAIe,EAAQlB,SAAWb,EAAOG,MAAM,GAChC,MAAM,IAAID,MAAM,sDAAsD6B,EAAQlB,cAAcb,EAAOG,MAAM,MAE7G,MAAMuC,EAAWC,KAAKC,OAAOb,GAC7B,GAAmB,MAAff,IAAwC,IAAjBA,GAAsB0B,GAAY1B,EACzD,MAAM,IAAId,MAAM,mCAAmCwC,UAAiB1B,MAExE,MAAM6B,EAAO,IAAIrD,EAAW,GAAIG,EAAcK,EAAOC,MAAOe,GACtDtB,GAAU,IAAA6C,SAAQvC,EAAQ,GAIhC,OAHA+B,EAAQhC,SAAQ,CAAC+C,EAAOC,KACpBF,EAAKhB,QAAQiB,EAAOpD,EAAQqD,GAAO,IAEhCF,CACX,CAQO,SAASG,EAAMhD,EAAQa,EAAQlB,GAClC,IAAIsD,EAAc,EAClB,MAAMC,EAAoBrC,EAAOO,KAAI+B,IACjCF,GAAeE,EACRF,KAEX,GAAIA,IAAgBjD,EAAOG,MAAM,GAC7B,MAAM,IAAID,MAAM,qGAEd+C,6BAAuCjD,EAAOG,SAEpD,MAAMiD,EAAuBpD,EAAOG,MAAM6B,MAAM,GAC1Cf,GAAqB,QAAkBmC,EAAsBzD,GAC7D0D,EAAgC,IAAhBJ,EAAoB,EAAIjD,EAAOc,KAAOmC,EACtDvD,GAAU,IAAAwB,OAAK,KACjB,MAAMxB,EAAU,GAChBM,GAAS,IAAAqB,SAAQrB,EAAQ,CAAC,EAAGiD,EAAaI,IAC1C,IAAK,IAAInB,EAAI,EAAGA,EAAIrB,EAAOA,SAAUqB,EAAG,CACpC,MACMH,EAAU,CAAC,EADa,IAANG,EAAW,EAAIgB,EAAkBhB,EAAI,GACzB,GAC9BoB,EAAQ,CAAC,EAAGzC,EAAOqB,GAAImB,GAC7B3D,EAAQwC,IAAK,IAAAb,UAAQ,IAAAW,OAAMhC,EAAQ+B,EAASuB,GAAQrC,EACxD,CAEA,OADAjB,EAAOY,UACAlB,CAAO,IAEZmD,EAAO,IAAIrD,EAAW,GAAIG,EAAcK,EAAOC,MAAOY,EAAOA,QACnE,IAAK,IAAIqB,EAAI,EAAGA,EAAIxC,EAAQmB,OAAQqB,IAChCW,EAAKhB,QAAQK,EAAGxC,EAAQwC,IAE5B,OAAOW,CACX,C,iFC1SO,MAAMU,EAOT,WAAA9D,CAAY+D,EAAUC,GAClB3D,KAAK0D,SAAWA,EAChB1D,KAAK2D,WAAaA,EAClB3D,KAAK4D,QAAS,IAAApD,QAAO,GAErBR,KAAK6D,UAAY,IAAIC,KACrB,IAAAxD,MAAKN,KAAK4D,OACd,CACA,MAAInD,GACA,OAAOT,KAAK4D,OAAOnD,EACvB,CAIA,aAAAE,GACIX,KAAK6D,UAAU5D,SAAQ+C,GAASA,EAAMlC,YACtCd,KAAK6D,UAAUE,QACf/D,KAAK4D,OAAO9C,SAChB,CAIA,IAAAE,GACI,OAAOhB,KAAK6D,UAAU7C,IAC1B,CAIA,UAAAgD,GACI,OAAO,IAAahE,KAAKgB,OAAQ,QACrC,CAMA,YAAMiD,CAAOC,EAAMC,GACfnE,KAAKoE,uBAAuBF,EAAMC,GAGlC,MAAME,QAAcH,EAAKI,OAIzB,OAFAtE,KAAK6D,UAAU5D,SAAQ+C,GAASA,EAAMlC,YACtCd,KAAK6D,UAAUE,SACR,IAAA3C,OAAK,KACR,MAAMmD,GAAU,IAAA9B,SAAQ0B,GAClBK,EAAaH,EAAMtD,OACnB0D,EAAeF,EAAQxD,OAC7B,EAAA2D,KAAA,OAAYF,IAAeC,GAAc,IACrC,kDAAGD,8BAAuCC,gBAE9C,IAAK,IAAIrC,EAAI,EAAGA,EAAIoC,EAAYpC,IAAK,CACjC,MAAMuC,EAAMN,EAAMjC,GACZY,EAAQuB,EAAQnC,IACtB,IAAA9B,MAAK0C,GACLhD,KAAK6D,UAAUe,IAAID,EAAK3B,EAC5B,CACA,OAAOhD,KAAK4D,MAAM,GAE1B,CAgBA,UAAMiB,CAAKX,EAAMY,GACb9E,KAAKoE,uBAAuBF,EAAMY,GAClC,MAAMT,QAAcH,EAAKI,OACzB,OAAO,IAAAlD,OAAK,KACR,MAAM2D,EAAS,GACf,IAAK,IAAI3C,EAAI,EAAGA,EAAIiC,EAAMtD,OAAQqB,IAAK,CACnC,MAAMuC,EAAMN,EAAMjC,GACZY,EAAQhD,KAAKgF,gBAAgBL,EAAKG,GACxCC,EAAOpD,KAAKqB,EAChB,CACA,OAAO,IAAA/B,OAAM8D,EAAO,GAE5B,CAEA,eAAAC,CAAgBL,EAAKG,GACjB,MAAMC,EAAS/E,KAAK6D,UAAUoB,IAAIN,GAClC,OAAiB,MAAVI,EAAiBA,EAASD,CACrC,CACA,sBAAAV,CAAuBO,EAAK3B,GACxB,GAAI2B,EAAIxE,QAAUH,KAAK0D,SACnB,MAAM,IAAItD,MAAM,oBAAoBJ,KAAK0D,qBAClCiB,EAAIxE,SAEf,GAAI6C,EAAM7C,QAAUH,KAAK2D,WACrB,MAAM,IAAIvD,MAAM,sBAAsBJ,KAAK2D,uBACpCX,EAAM7C,QAErB,E,iFC7GG,MAAM+E,EACT,WAAAvF,CAAYwF,EAAMhF,EAAOiF,EAASvF,EAAcwF,EAAwBC,EAAaC,GACjFvF,KAAKmF,KAAOA,EACZnF,KAAKG,MAAQA,EACbH,KAAKoF,QAAUA,EACfpF,KAAKH,aAAeA,EACpBG,KAAKqF,uBAAyBA,EAC9BrF,KAAKsF,YAAcA,EACnBtF,KAAKuF,eAAiBA,EACtBvF,KAAKJ,QAAU,GACfI,KAAKwF,SAAU,EACfxF,KAAKO,UAAW,IAAAC,QAAO,IACvB,IAAAF,MAAKN,KAAKO,SACd,CACA,MAAIE,GACA,OAAOT,KAAKO,SAASE,EACzB,CACA,UAAIgF,GACA,OAAOzF,KAAKwF,OAChB,CAIA,aAAA7E,CAAcC,GACVZ,KAAKJ,QAAQK,SAAQC,IACF,MAAXU,GAAoBA,EAAQC,IAAIX,EAAOA,OAAOO,KAC9CP,EAAOA,OAAOY,SAClB,IAEJd,KAAKJ,QAAU,GACfI,KAAKwF,SAAU,EACfxF,KAAKO,SAASO,SAClB,CACA,IAAAE,GACI,OAAOhB,KAAKJ,QAAQmB,MACxB,CAKA,IAAA2E,CAAKzC,GACD,GAAIjD,KAAKwF,QACL,MAAM,IAAIpF,MAAM,eAAeJ,KAAKmF,iCAExC,GAAIlC,EAAQ,GAAKA,GAASjD,KAAKgB,OAC3B,MAAM,IAAIZ,MAAM,4BAA4B6C,yBAA6BjD,KAAKgB,UAElF,MAAM2E,EAAkB3F,KAAKJ,QAAQqD,GACrC,GAAI0C,EAAgBC,QAChB,MAAM,IAAIxF,MAAM,eAAeJ,KAAKmF,8BAA8BlC,yGAOtE,OAJIjD,KAAKuF,iBACLI,EAAgBC,SAAU,GAE9BD,EAAgBD,MAAO,EAChBC,EAAgBzF,MAC3B,CAIA,QAAA2F,CAAS5D,GACL,OAAOA,EAAQX,KAAI2B,GAASjD,KAAK0F,KAAKzC,IAC1C,CAMA,KAAA6C,CAAM7C,EAAO/C,GACT,GAAIF,KAAKwF,QACL,MAAM,IAAIpF,MAAM,eAAeJ,KAAKmF,iCAExC,GAAIlC,EAAQ,IAAMjD,KAAKsF,aAAerC,GAASjD,KAAKoF,QAChD,MAAM,IAAIhF,MAAM,2BAA2B6C,+CAAmDjD,KAAKoF,WAEvG,MAAM/C,EAAIrC,KAAKJ,QAAQqD,IAAU,CAAC,EAClC,GAAI/C,EAAOC,QAAUH,KAAKG,MACtB,MAAM,IAAIC,MAAM,eAAeJ,KAAKmF,8CAA8ClC,4CACvD/C,EAAOC,mCAAmCH,KAAKG,UAQ9E,GALoB,IAAhBH,KAAKgB,QACiB,MAArBhB,KAAKH,cAAqD,IAA7BG,KAAKH,aAAakB,SAChDf,KAAKH,aAAeK,EAAOG,QAE/B,QAAoCL,KAAKH,aAAcK,EAAOG,MAAO,eAAeL,KAAKmF,8CAA8ClC,MACnIZ,EAAEqD,KACF,MAAM,IAAItF,MAAM,eAAeJ,KAAKmF,8CAA8ClC,wCAEtF,GAAIZ,EAAE0D,QACF,MAAM,IAAI3F,MAAM,eAAeJ,KAAKmF,8CAA8ClC,2CAEtFZ,EAAEnC,OAASA,GACX,IAAAI,MAAKJ,GACLmC,EAAE0D,SAAU,EACZ/F,KAAKJ,QAAQqD,GAASZ,CAC1B,CAIA,SAAA2D,CAAU/D,EAASrC,GACf,GAAIqC,EAAQlB,SAAWnB,EAAQmB,OAC3B,MAAM,IAAIX,MAAM,eAAeJ,KAAKmF,kEACLlD,EAAQlB,2CAA2CnB,EAAQmB,WAE9FkB,EAAQhC,SAAQ,CAACmC,EAAGa,IAAUjD,KAAK8F,MAAM1D,EAAGxC,EAAQqD,KACxD,CASA,MAAAjB,CAAOC,EAAS9B,GACZ,GAAMA,GAASA,IAAUH,KAAKG,MAC1B,MAAM,IAAIC,MAAM,wBAAwBJ,KAAKG,oCAAoCA,KAErF,GAAK8B,EAODA,EAAUA,EAAQC,MAAM,EAAGlC,KAAKgB,YAPtB,CACViB,EAAU,GACV,IAAK,IAAIG,EAAI,EAAGA,EAAIpC,KAAKgB,OAAQoB,IAC7BH,EAAQN,KAAKS,EAErB,CAIA,GAAuB,IAAnBH,EAAQlB,OACR,OAAO,IAAAb,QAAO,GAAI,CAAC,GAAGiC,OAAOnC,KAAKH,eAItC,MAAMD,EAAUI,KAAK6F,SAAS5D,GAE9B,OADA,QAAoCjC,KAAKH,aAAcD,EAAQ,GAAGS,MAAO,iCAClE,IAAAY,OAAMrB,EAAS,EAC1B,CAIA,MAAAuC,CAAOhC,GACH,GAAMA,GAASA,IAAUH,KAAKG,MAC1B,MAAM,IAAIC,MAAM,wBAAwBJ,KAAKG,oCAAoCA,KAErF,GAAoB,IAAhBH,KAAKgB,OACL,OAAO,IAAAd,QAAO,GAAI,CAAC,GAAGiC,OAAOnC,KAAKH,eAEtC,MAAMoC,EAAU,GAChB,IAAK,IAAIG,EAAI,EAAGA,EAAIpC,KAAKgB,OAAQoB,IAC7BH,EAAQN,KAAKS,GAGjB,MAAMxC,EAAUI,KAAK6F,SAAS5D,GAE9B,OADA,QAAoCjC,KAAKH,aAAcD,EAAQ,GAAGS,MAAO,mDAAmDL,KAAKH,wCAAwCD,EAAQ,GAAGS,WAC7K,IAAA8B,QAAOvC,EAAS,EAC3B,CAOA,OAAA+C,CAAQV,EAAS/B,GACb,GAAIA,EAAOC,QAAUH,KAAKG,MACtB,MAAM,IAAIC,MAAM,wBAAwBJ,KAAKG,8BAA8BD,EAAOC,SAEtF,GAAI8B,EAAQlB,SAAWb,EAAOG,MAAM,GAChC,MAAM,IAAID,MAAM,sDAAsD6B,EAAQlB,cAAcb,EAAOG,MAAM,MAE7G,MAAMuC,EAAWC,KAAKC,OAAOb,GAC7B,IAAKjC,KAAKsF,aAAe1C,GAAY5C,KAAKoF,QACtC,MAAM,IAAIhF,MAAM,mCAAmCwC,UAAiB5C,KAAKoF,YAE7EpF,KAAKgG,UAAU/D,GAAS,IAAAQ,SAAQvC,EAAQ,GAC5C,CAOA,KAAAgD,CAAMnC,EAAQb,GACV,GAAIA,EAAOC,QAAUH,KAAKG,MACtB,MAAM,IAAIC,MAAM,wBAAwBJ,KAAKG,8BAA8BD,EAAOC,SAEtF,IAAIgD,EAAc,EAClB,MAAMC,EAAoBrC,EAAOO,KAAI+B,IACjCF,GAAeE,EACRF,KAEX,GAAIA,IAAgBjD,EAAOG,MAAM,GAC7B,MAAM,IAAID,MAAM,qGAElB+C,6BAAuCjD,EAAOG,SAEhD,IAAKL,KAAKsF,aAAevE,EAAOA,SAAWf,KAAKoF,QAC5C,MAAM,IAAIhF,MAAM,2DAA2DJ,KAAKoF,eAAerE,EAAOA,wEAG1G,MAAMwC,EAAgC,IAAhBJ,EAAoB,EAAIjD,EAAOc,KAAOmC,EACtDvD,EAAU,IAChB,IAAAwB,OAAK,KACDlB,GAAS,IAAAqB,SAAQrB,EAAQ,CAAC,EAAGiD,EAAaI,IAC1C,IAAK,IAAInB,EAAI,EAAGA,EAAIrB,EAAOA,SAAUqB,EAAG,CACpC,MACMH,EAAU,CAAC,EADa,IAANG,EAAW,EAAIgB,EAAkBhB,EAAI,GACzB,GAC9BoB,EAAQ,CAAC,EAAGzC,EAAOqB,GAAImB,GAC7B3D,EAAQwC,IAAK,IAAAb,UAAQ,IAAAW,OAAMhC,EAAQ+B,EAASuB,GAAQxD,KAAKH,aAC7D,CACA,OAAOD,CAAO,IAElB,MAAMqC,EAAU,GAChB,IAAK,IAAIG,EAAI,EAAGA,EAAIrB,EAAOA,OAAQqB,IAC/BH,EAAQG,GAAKA,EAEjBpC,KAAKgG,UAAU/D,EAASrC,EAC5B,E,uHClNG,SAASqG,EAAoCC,EAAQC,EAAQC,EAAqB,IAErF,GAAsB,kBAAXF,GAAyC,kBAAXC,EAAzC,CAGA,EAAAzB,KAAA,OAAYwB,EAAOnF,SAAWoF,EAAOpF,QAAQ,IAAMqF,EAAqB,WAAWF,SAAcC,iBACjG,IAAK,IAAI/D,EAAI,EAAGA,EAAI8D,EAAOnF,OAAQqB,IAAK,CACpC,MAAMiE,EAAOH,EAAO9D,GACdkE,EAAOH,EAAO/D,GACpB,EAAAsC,KAAA,OAAY2B,EAAO,GAAKC,EAAO,GAAKD,IAASC,GAAM,IAAMF,EAAqB,WAAWF,SAAcC,gBAC3G,CANA,CAOJ,CACO,SAASI,EAAiB1G,GAC7B,MAA4B,kBAAjBA,IAA6BA,EAAa2G,MAAKC,GAAOA,EAAM,GAI3E,CAQO,SAASC,EAAkBC,EAAkB/G,EAASC,GACzD,IAAI+G,EAAeC,EAAkBF,EAAkB9G,GACvD,MAAMiH,GAAuBP,EAAiBK,GAC9C,GAAIE,GAA0C,IAAnBlH,EAAQmB,OAC/B,MAAM,IAAIX,MACN,qFAAyCwG,KAOjD,GALIE,GACAlH,EAAQK,SAAQC,IACZ0G,EAAeC,EAAkB3G,EAAOG,MAAOuG,EAAa,KAG/DL,EAAiBK,GAClB,MAAM,IAAIxG,MAAM,mCAAmCwG,KAEvD,OAAOA,CACX,CACO,SAASC,EAAkBE,EAAeC,GAC7C,GAA6B,kBAAlBD,EACP,OAAOC,EAEX,GAA6B,kBAAlBA,EACP,OAAOD,EAEX,GAAIA,EAAchG,SAAWiG,EAAcjG,OACvC,MAAM,IAAIX,MAAM,oCAAoC2G,SAAqBC,KAE7E,MAAMjC,EAAS,GACf,IAAK,IAAI3C,EAAI,EAAGA,EAAI2E,EAAchG,SAAUqB,EAAG,CAC3C,MAAMiE,EAAOU,EAAc3E,GACrBkE,EAAOU,EAAc5E,GAC3B,GAAIiE,GAAQ,GAAKC,GAAQ,GAAKD,IAASC,EACnC,MAAM,IAAIlG,MAAM,oCAAoC2G,SAAqBC,KAE7EjC,EAAO3C,GAAKiE,GAAQ,EAAIA,EAAOC,CACnC,CACA,OAAOvB,CACX,C,iIClFO,MAAMkC,EACT,WAAAtH,CAAYuH,EAAY,CAAC,EAAGC,EAAiB,CAAC,EAAGC,EAAgB,CAAC,EAAGC,EAAc,CAAC,GAChFrH,KAAKkH,UAAYA,EACjBlH,KAAKmH,eAAiBA,EACtBnH,KAAKoH,cAAgBA,EACrBpH,KAAKqH,YAAcA,EACnBrH,KAAKsH,YAAc,CAAE7G,GAAI,EAAG8G,UAAW,GAAIC,YAAa,GACxDxH,KAAKyH,SAAW,CAACzH,KAAKsH,aACtBtH,KAAK0H,OAAS,EACd1H,KAAK2H,2BACT,CACA,QAAAC,CAASnH,EAAI8G,GACT,MAAO,CAAE9G,KAAI8G,YAAWC,YAAa,EACzC,CAMA,kBAAIK,CAAeJ,GACXzH,KAAKyH,WAAaA,IAClBzH,KAAKyH,SAAWA,EAChBzH,KAAK2H,4BAEb,CACA,kBAAIE,GACA,OAAO7H,KAAKyH,QAChB,CAIA,oBAAIK,GACA,OAAO9H,KAAK+H,mBAAmB,EACnC,CAKA,qBAAIC,GACA,OAAOhI,KAAK+H,kBAChB,CACA,yBAAAJ,GACI,MAAMM,EAAQ,GACd,IAAK,IAAI7F,EAAI,EAAGA,EAAIpC,KAAKyH,SAAS1G,OAAS,EAAGqB,IAAK,CAC/C,MAAMqF,EAAWzH,KAAKyH,SAASvF,MAAM,EAAGlC,KAAKyH,SAAS1G,OAASqB,GAC/D6F,EAAMtG,KAAK3B,KAAKkI,qBAAqBT,GACzC,CACAQ,EAAMtG,KAAK,IACX3B,KAAK+H,mBAAqBE,CAC9B,CACA,oBAAAC,CAAqBT,GACjB,OAAOA,EACHA,EACKnG,KAAI6G,GAA2B,IAAfA,EAAQ1H,IAAoC,IAAxB0H,EAAQX,YAC7C,GACA,GAAGW,EAAQZ,aAAaY,EAAQX,gBAC/BY,KAAK,KACV,EACR,CAKA,UAAAC,CAAWC,GACHtI,KAAKyH,WACLzH,KAAK0H,SACL1H,KAAKyH,SAAWzH,KAAKyH,SAASvF,QAC9BlC,KAAKyH,SAAS9F,KAAK3B,KAAK4H,SAAS5H,KAAK0H,OAAQY,IAC9CtI,KAAK+H,mBAAmBQ,QAAQvI,KAAKkI,qBAAqBlI,KAAKyH,WAEvE,CAKA,SAAAe,GACI,KAAIxI,KAAKyH,UAAYzH,KAAKyH,SAAS1G,OAAS,GAMxC,MAAM,IAAIX,MAAM,2CALhBJ,KAAKyH,SAAWzH,KAAKyH,SAASvF,QAC9BlC,KAAKyH,SAASgB,QAAQ,GACtBzI,KAAKgI,kBAAkBU,OAK/B,CAKA,aAAAC,GACI,KAAI3I,KAAKyH,UAAYzH,KAAKyH,SAAS1G,OAAS,GAUxC,MAAM,IAAIX,MAAM,yDAV2B,CAC3CJ,KAAKyH,SAAWzH,KAAKyH,SAASvF,QAC9BlC,KAAK0H,SACL,MAAMS,EAAUS,OAAOC,OAAO,CAAC,EAAG7I,KAAKyH,SAASzH,KAAKyH,SAAS1G,OAAS,IACvEoH,EAAQX,aAAe,EACvBW,EAAQ1H,GAAKT,KAAK0H,OAClB1H,KAAKyH,SAASgB,QAAQ,EAAG,EAAGN,GAC5BnI,KAAK+H,mBAAmBU,OAAO,EAAG,EAAGzI,KAAKkI,qBAAqBlI,KAAKyH,UACxE,CAIJ,CACA,SAAAqB,CAAU3D,GACN,OAAOnF,KAAKkH,UAAU/B,EAC1B,CACA,cAAA4D,CAAeC,GACXhJ,KAAKmH,eAAe6B,EAAYvI,IAAMuI,CAC1C,CACA,cAAAC,CAAexI,GACX,OAAOT,KAAKmH,eAAe1G,EAC/B,CACA,aAAAyI,CAAc1G,GACVxC,KAAKoH,cAAc5E,EAAW/B,IAAM+B,CACxC,CACA,aAAA2G,CAAc1I,GACV,OAAOT,KAAKoH,cAAc3G,EAC9B,CACA,OAAAK,CAAQF,GACJ,IAAK,MAAM+D,KAAO3E,KAAKmH,eACnBnH,KAAKmH,eAAexC,GAAKhE,cAAcC,GAE3C,IAAK,MAAM+D,KAAO3E,KAAKoH,cACnBpH,KAAKoH,cAAczC,GAAKhE,cAAcC,EAE9C,EC7GG,SAASwI,EAAqBC,EAAQC,EAASpC,EAAWqC,GAC7D,MAAMC,EAAY,IAAIC,IAChBC,EAAgB,GACtB,IAAIC,EAAc,KACdC,EAAa,KAGjB,MAAMC,EAAO,IAAIJ,IACXK,EAAiBlB,OAAO1E,KAAKmF,GAAQ/H,KAAI6D,IAAQ,QAAcA,GAAM,KAC3E,IAAI4E,EAAgB,GACH,MAAbR,IACAQ,EAAgBR,EAAUjI,KAAI0I,IAAQ,QAAcA,EAAK7E,MAAM,MAEnE,MAAM8E,EAAW,IAAIX,GACrB,KAAOW,EAASlJ,OAAS,GAAG,CACxB,MAAMiJ,EAAOC,EAASxI,OAClByI,EAAcF,IAASG,EAAeH,IAASI,EAAYJ,KACxC,MAAfL,IACAA,EAAcK,EACdJ,EAAaD,EAAYU,SAAS/I,KAAIgJ,GAASA,EAAMnF,OAChDoF,QAAOpF,GAAQqE,EAAU3I,IAAIsE,MAG1CqE,EAAUgB,IAAIR,EAAK7E,MAES,MAAxB+B,EAAU8C,EAAK7E,SAIwB,IAAvC2E,EAAeW,QAAQT,EAAK7E,QAIU,IAAtC4E,EAAcU,QAAQT,EAAK7E,QAGJ,IAAvB6E,EAAKX,OAAOtI,OAIhBiJ,EAAKX,OAAOpJ,SAAQyK,IAEZb,EAAKhJ,IAAI6J,EAAMvF,QAGnB0E,EAAKW,IAAIE,EAAMvF,MACf8E,EAAStI,KAAK+I,GAAM,IATpBhB,EAAc/H,KAAKqI,EAAK7E,OAWhC,CACA,MAAO,CAAEkE,SAAQC,UAASE,YAAWE,gBAAeC,cAAaC,aACrE,CA8CA,MAAMe,EAAmB,CACrB,SAAU,QAAS,QAAS,OAAQ,gBAAiB,cACrD,iBAAkB,KAAM,SAEtBC,EAAoB,CACtB,sBAAuB,sBAAuB,sBAAuB,SAEnEC,EAAiB,CACnB,YAAa,cAAe,oBAAqB,sBACjD,kBAAmB,oBAAqB,kBAAmB,qBAExD,SAASX,EAAcF,GAC1B,OAAOW,EAAiBF,QAAQT,EAAKc,KAAO,CAChD,CACO,SAASX,EAAeH,GAC3B,OAAOY,EAAkBH,QAAQT,EAAKc,KAAO,CACjD,CACO,SAASV,EAAYJ,GACxB,OAAOa,EAAeJ,QAAQT,EAAKc,KAAO,CAC9C,CCvHO,MAAMC,EAST,WAAApL,CAAYqL,EAAOC,GACfjL,KAAKgL,MAAQA,EACbhL,KAAKiL,OAASA,EACdjL,KAAKkL,YAAc,IAAIpH,IACvB9D,KAAKmL,WAAa,CAAC,EACnBnL,KAAKoL,UAAY,IACjBpL,KAAKqL,WAAa,CAAC,EACnBrL,KAAKsL,qBAAuB,CAAC,EAC7BtL,KAAKuL,SAAWP,EAAM1B,QACtBtJ,KAAKwL,QAAUR,EAAM3B,OACrBrJ,KAAKyL,WAAaT,EAAMzB,UACxBvJ,KAAK0L,WAAaV,EAAMW,UACxB3L,KAAKqL,WAAaL,EAAMY,UAED,MAAnBZ,EAAMY,WACNhD,OAAO1E,KAAK8G,EAAMY,WAAW3L,SAAQkF,IACjCnF,KAAKsL,qBAAqBnG,GACtB,IAAI4F,EAAcC,EAAMY,UAAUzG,GAAOnF,KAAK,GAG9D,CACA,aAAI6L,GACA,OAAO7L,KAAKiL,OAASjL,KAAKiL,OAAOY,UAAY7L,KAAK8L,UACtD,CACA,uBAAIC,GACA,OAAO/L,KAAKiL,OAASjL,KAAKiL,OAAOc,oBAC7B/L,KAAKsL,oBACb,CACA,aAAIpE,GACA,OAAOlH,KAAKiL,OAASjL,KAAKiL,OAAO/D,UAAYlH,KAAKmL,UACtD,CACA,aAAIjE,CAAUA,GACV,MAAM2E,EAAYjD,OAAO1E,KAAKgD,GAAW5F,KAAIqD,GAAOuC,EAAUvC,GAAKrD,KAAIpB,GAAUA,EAAOO,OACxFT,KAAK8L,WAAa,GAAG3J,UAAU0J,GAC/B7L,KAAKmL,WAAajE,CACtB,CAKA,mBAAI8E,CAAgBA,GAChBhM,KAAKiM,iBAAmBD,CAC5B,CACA,UAAI3C,GACA,OAAOrJ,KAAKwL,QAAQlK,KAAI0I,IACb,CACH7E,KAAM6E,EAAK7E,KACX9E,MAAO2J,EAAKkC,WAAkB,MAC1BlC,EAAKkC,WAAkB,MAAElJ,WACzBmJ,EACJhM,MAAO6J,EAAKkC,WAAkB,MAC1BlC,EAAKkC,WAAkB,MAAElJ,WACzBmJ,KAGhB,CACA,WAAI7C,GACA,OAAOtJ,KAAKuL,SAASjK,KAAI0I,IACd,CACH7E,KAAM6E,EAAK7E,KACX9E,MAAO2J,EAAKkC,WAAkB,MAC1BlC,EAAKkC,WAAkB,MAAElJ,WACzBmJ,EACJhM,MAAO6J,EAAKkC,WAAkB,MAC1BlC,EAAKkC,WAAkB,MAAElJ,WACzBmJ,KAGhB,CACA,cAAIC,GACA,OAAOpM,KAAKwL,QAAQlK,KAAI0I,GAAQA,EAAKqC,cAAgBrC,EAAK7E,MAC9D,CACA,eAAImH,GACA,OAAOtM,KAAKuL,SAASjK,KAAK0I,IACtB,MAAM7E,EAAO6E,EAAKqC,cAAgBrC,EAAK7E,KACvC,OAAO6E,EAAKuC,cAAgB,GAAIpH,KAAQ6E,EAAKuC,gBAAmBpH,CAAI,GAE5E,CACA,aAAIyG,GACA,OAAOhD,OAAO1E,KAAKlE,KAAKqL,YAAYmB,QAAO,CAAClL,EAAKqD,KAC7CrD,EAAIqD,GAAO3E,KAAKqL,WAAW1G,GAAKgH,UACzBrK,IACR,CAAC,EACR,CACA,iBAAAmL,CAAkBpD,EAAQC,GACtB,MAAMoD,EAAerD,EAAO/H,KAAI0I,GAAQA,EAAK7E,OAAMwH,OAC7CC,EAAgBtD,EAAQhI,KAAI0I,GAAQA,EAAK7E,OAAMwH,OACrD,OAAOD,EAAatE,KAAKpI,KAAKoL,WAAa,KACvCwB,EAAcxE,KAAKpI,KAAKoL,UAChC,CAKA,OAAAyB,CAAQxD,EAAQC,GACZ,MAAMwD,EAAgB1D,EAAqBC,EAAQC,EAAStJ,KAAKkH,UAAWlH,KAAKyL,aAC3E,cAAE/B,EAAa,YAAEC,EAAW,WAAEC,GAAekD,EACnD,GAAmB,MAAfnD,EACA,MAAM,IAAIvJ,MAAM,qCAAqCuJ,EAAYxE,oCAC1CwE,EAAYmB,8GAEKlB,MAE5C,GAAIF,EAAc3I,OAAS,EAAG,CAC1B,MAAMgM,EAAWzD,EAAQhI,KAAI0L,GAAKA,EAAE7H,OAC9B8H,EAAUrE,OAAO1E,KAAKmF,GAC5B,MAAM,IAAIjJ,MAAM,+BAA+B2M,gCACvCE,sCAA4CvD,KACxD,CACA,OD3DD,SAAoCsB,EAAO9D,EAAW4F,GACzD,MAAM,UAAEtD,EAAS,OAAEH,GAAWyD,EACxB7C,EAAW,GACXmC,EAAaxD,OAAO1E,KAAKmF,GAC1B/H,KAAI6D,IAAQ,QAAcA,GAAM,KAChC7D,KAAI6D,GAAQ6F,EAAMkC,MAAM/H,KACvBoE,EAAYyB,EAAMzB,UACxB6C,EAAWnM,SAAQyK,IACXlB,EAAU3I,IAAI6J,EAAMvF,OACpB8E,EAAStI,KAAK+I,EAClB,IAEJM,EAAMmC,QAAQlN,SAAQmN,IACd5D,EAAU3I,IAAIuM,EAAOjI,OACrB8E,EAAStI,KAAKyL,EAClB,IAEa,MAAb7D,GACAA,EAAUtJ,SAAQ+J,IACVR,EAAU3I,IAAImJ,EAAK7E,OACnB8E,EAAStI,KAAKqI,EAClB,IAGR,MAAMH,EAAO,IAAIJ,IACX4D,EAAe,GACrB,KAAOpD,EAASlJ,OAAS,GAAG,CACxB,MAAMiJ,EAAOC,EAASxI,MACtBoI,EAAKW,IAAIR,EAAK7E,MACT+B,EAAU8C,EAAK7E,OAChBkI,EAAa1L,KAAKqI,GAEtBA,EAAKK,SAASpK,SAAQqK,KACbT,EAAKhJ,IAAIyJ,EAAMnF,OAASqE,EAAU3I,IAAIyJ,EAAMnF,OAC7CmF,EAAMjB,OAAOiE,OAAM5C,GAASb,EAAKhJ,IAAI6J,EAAMvF,SAC3C8E,EAAStI,KAAK2I,EAClB,GAER,CACA,OAAO+C,CACX,CCmBeE,CAA2BvN,KAAKgL,MAAOhL,KAAKkH,UAAW4F,EAClE,CAUA,OAAAU,CAAQnE,EAAQC,GACZD,EAASrJ,KAAKyN,UAAUpE,GACxB,MAAMpB,EAAQW,OAAO1E,KAAKmF,GAAQsD,OAClC3M,KAAK0N,YAAYrE,GACjBrJ,KAAK2N,uBAAuBtE,GAC5BC,EAAUtJ,KAAK4N,WAAWtE,GAC1BtJ,KAAK6N,aAAavE,GAClB,MAAM8C,EAAanE,EAAM3G,KAAI6D,GAAQnF,KAAKgL,MAAMkC,OAAM,QAAc/H,GAAM,MACpE2I,EAAkBxE,EAAQhI,KAAI6D,IAAQ,QAAcA,GAAM,KAChE,IAAImH,EAAcwB,EAAgBxM,KAAI6D,GAAQnF,KAAKgL,MAAMkC,MAAM/H,KAEpC,IAAvBmH,EAAYvL,SACZuL,EAActM,KAAKuL,UAEvB,MAAMwC,EAAiB/N,KAAKyM,kBAAkBL,EAAYE,GAE1D,IAAIe,EAAerN,KAAKkL,YAAYjG,IAAI8I,GACpB,MAAhBV,IACAA,EAAerN,KAAK6M,QAAQxD,EAAQiD,GACpCtM,KAAKkL,YAAYtG,IAAImJ,EAAgBV,IAEzC,MAAMlG,EAAiB,CAAC,EAClBC,EAAgB,CAAC,EACvB,OAAO,IAAAhG,OAAK,KACR,MAAM+G,EAAU,IAAIlB,EAAiBjH,KAAKkH,UAAWC,EAAgBC,EAAepH,KAAK+L,qBACnFiC,EAAapF,OAAOC,OAAO,CAAC,EAAG7I,KAAKkH,WAC1C0B,OAAO1E,KAAKmF,GAAQpJ,SAAQkF,IACxB,MAAO8I,EAAUhL,IAAS,QAAckC,GAClCvF,EAAU,GAChBA,EAAQqD,GAASoG,EAAOlE,GACxB6I,EAAWC,GAAYrO,CAAO,IAElC,MAAMsO,EAAgBlO,KAAKmO,mBAAmBH,GACxCI,EAAkC,CAAC,EACzC,IAAK,IAAIhM,EAAI,EAAGA,EAAIiL,EAAatM,OAAQqB,IAAK,CAC1C,MAAM4H,EAAOqD,EAAajL,GAC1B,IAAK4L,EAAWhE,EAAK7E,MAAO,CACxB,MAAMvF,GAAU,OAAUoK,EAAMgE,EAAY7F,EAASnI,KAAKiM,kBAC1D,GAAI,EAAAvH,KAAA,UAAe9E,GACf,MAAM,IAAIQ,MAAM,4BAA4B4J,EAAKc,oEAGrDkD,EAAWhE,EAAK7E,MAAQvF,EACxBI,KAAKqO,uBAAuBrE,EAAK7E,KAAM6E,EAAMgE,EAAY7F,EAAS+F,EAAeJ,EAAiBM,EACtG,CACJ,CAKA,OAHmB,MAAfpO,KAAKiL,QACL9C,EAAQrH,QAAQoN,GAEb5E,EAAQhI,KAAI6D,IAAQ,QAAUA,EAAM6I,EAAY7F,IAAS,GAExE,CACA,kBAAAgG,CAAmBtK,GACf,MAAMyK,EAAM,GAAGnM,OAAOoM,MAAM,GAAI3F,OAAO1E,KAAKL,GACvCvC,KAAIqD,GAAOd,EAAUc,KACrBrD,KAAI1B,GAAWA,EAAQ0B,KAAIpB,GAAUA,EAAOO,QACjD,OAAO,IAAIgJ,IAAI6E,EACnB,CACA,sBAAAD,CAAuBJ,EAAUjE,EAAMnG,EAAWsE,EAAS+F,EAAeM,EAAaJ,GAG7D,YAAlBpE,EAAKyE,WAA6D,IAAnCD,EAAY/D,QAAQwD,KAGvDpK,EAAUoK,GAAUhO,SAAQC,IACV,MAAVA,IACAkO,EAAgClO,EAAOO,KAClC2N,EAAgClO,EAAOO,KAAO,GAC3CuJ,EAAKK,SAAStJ,OAC1B,IAEJiJ,EAAKX,OAAOpJ,SAAQyK,IAGhB,GAAuB,YAAnBA,EAAM+D,SAAwB,CAC9B,MAAM7O,GAAU,QAA6B8K,EAAMvF,KAAMtB,EAAWsE,GACrD,MAAXvI,GACAA,EAAQK,SAAQC,IACZ,GAAIA,IAAWA,EAAOwO,OAASR,EAAcrN,IAAIX,EAAOO,IAAK,CACzD,MAAMkO,EAAQP,EAAgClO,EAAOO,IACvC,IAAVkO,GACAzO,EAAOY,iBACAsN,EAAgClO,EAAOO,KAEhC,MAATkO,GAGLP,EAAgClO,EAAOO,KAE/C,IAGZ,KAER,CAUA,kBAAMmO,CAAavF,EAAQC,GACvB,OAAOtJ,KAAK6O,cAAcxF,EAAQC,EACtC,CAeA,mBAAMuF,CAAcxF,EAAQC,EAASwF,GAAsB,EAAO3H,EAAiB,CAAC,EAAGC,EAAgB,CAAC,GAC/F0H,IACDzF,EAASrJ,KAAKyN,UAAUpE,GACxBrJ,KAAK0N,YAAYrE,GACjBrJ,KAAK2N,uBAAuBtE,GAC5BC,EAAUtJ,KAAK4N,WAAWtE,GAC1BtJ,KAAK6N,aAAavE,IAEtB,MAAMnB,EAAU,IAAIlB,EAAiBjH,KAAKkH,UAAWC,EAAgBC,EAAepH,KAAK+L,qBAInFlI,QAAkB7D,KAAK+O,uBAAuB1F,EAAQlB,EAASmB,EAASwF,GACxEE,EAAU1F,EAAQhI,KAAI6D,IAAQ,QAAUA,EAAMtB,EAAWsE,KAEzD8G,EAAYD,EAAQ1N,KAAIe,GAAKA,EAAE5B,KAC/ByO,EAAWtG,OAAO1E,KAAKmF,GAAQ/H,KAAI6D,GAAQkE,EAAOlE,GAAM1E,KACxDG,EAAU,IAAI6I,IAAI,IAAIwF,KAAcC,KAAalP,KAAK6L,YAc5D,OAbAjD,OAAO1E,KAAKL,GAAW5D,SAAQ0E,IACPd,EAAUc,GAClB1E,SAAQC,KACZA,GAAWA,EAAOwO,MAASxO,EAAOiP,YACjCvO,EAAQC,IAAIX,EAAOO,KACpBP,EAAOY,SACX,GACF,IAGa,MAAfd,KAAKiL,QACL9C,EAAQrH,QAAQF,GAEboO,CACX,CACA,0BAAMI,CAAqB/F,EAAQlC,EAAgBC,GAC/C,MAAMiI,EAAehG,EAAOmD,QAAO,CAAClL,EAAKpB,EAAQ+C,KAC7C3B,EAAItB,KAAKqJ,OAAOpG,GAAOkC,MAAQjF,EACxBoB,IACR,CAAC,GACJ,OAAOtB,KAAK6O,cAAcQ,EAAcrP,KAAKsM,aAAa,EAAMnF,EAAgBC,EACpF,CAYA,4BAAM2H,CAAuB1F,EAAQlB,EAASqG,EAAaM,GACvD,MAAM7G,EAAQW,OAAO1E,KAAKmF,GACpB+C,EAAanE,EAAM3G,KAAI6D,GAAQnF,KAAKgL,MAAMkC,OAAM,QAAc/H,GAAM,MACpE2I,EAAkBU,EAAYlN,KAAI6D,IAAQ,QAAcA,GAAM,KACpE,IAAImH,EAAcwB,EAAgBxM,KAAI6D,GAAQnF,KAAKgL,MAAMkC,MAAM/H,KAEpC,IAAvBmH,EAAYvL,SACZuL,EAActM,KAAKuL,UAEvB,MAAM,UAAE/B,EAAS,cAAEE,EAAa,YAAEC,EAAW,WAAEC,GAAeR,EAAqBC,EAAQiD,EAAatM,KAAKkH,UAAWlH,KAAKyL,YAEvHxK,EAAQ,IACPmL,KAAepM,KAAKgL,MAAMmC,WAAanN,KAAKyL,YAAc,IAC/DnK,KAAI0I,IACK,CAAEA,OAAMvC,SAAUU,EAAQN,mBAE/BmG,EAAapF,OAAOC,OAAO,CAAC,EAAG7I,KAAKkH,WAC1C0B,OAAO1E,KAAKmF,GAAQpJ,SAAQkF,IACxB,MAAO8I,EAAUhL,IAAS,QAAckC,GAClCvF,EAAU,GAChBA,EAAQqD,GAASoG,EAAOlE,GACxB6I,EAAWC,GAAYrO,CAAO,IAElC,MAAMwO,EAAkC,CAAC,EACnCF,EAAgBlO,KAAKmO,mBAAmBH,GACxCsB,EAAQ,CAAC,EACf,KAAOrO,EAAMF,OAAS,GAAG,CACrB,MAAMwO,EAAWvP,KAAKwP,aAAapD,EAAYnL,EAAOkH,EAAS6F,EAAYsB,EAAOpB,EAAeJ,EAAiBM,EAAiC5E,SAC7IiG,QAAQC,IAAIH,EACtB,CAKA,MAAMI,EAAiBrD,EAClB/B,QAAOP,IAASE,EAAcF,MAC9B,QAAUA,EAAK7E,KAAM6I,EAAY7F,KACjC7G,KAAI0I,GAAQA,EAAK7E,OACtB,GAAIwK,EAAe5O,OAAS,EAAG,CAC3B,IAAI6O,EAAiB,GAMrB,MALmB,MAAfjG,IACAiG,EAEQ,wFAA2BhG,MAEjC,IAAIxJ,MAAM,+BAA+BuP,gCAChC1H,iDACPyB,OAAmBkG,IAC/B,CACA,OAAO5B,CACX,CACA,YAAAwB,CAAapD,EAAYnL,EAAOkH,EAAStE,EAAWyL,EAAOpB,EAAeM,EAAaJ,EAAiC5E,GACpH,MAAM+F,EAAW,GACjB,KAAOtO,EAAMF,OAAS,GAAG,CACrB,MAAM8O,EAAO5O,EAAMQ,MACnB0G,EAAQN,eAAiBgI,EAAKpI,SAC9B,IAAIwG,EAAW,GAUf,GANqB,UAAjB4B,EAAK7F,KAAKc,KACV,QAAc,aAAc+E,EAAK7F,KAAMnG,EAAWsE,MACjD8F,IAAY,QAAoB4B,EAAK7F,KAAK7E,KAAMgD,IAIpB,MAA7BtE,EAAUgM,EAAK7F,KAAK7E,MAAe,CACnC,MAAMvF,GAAU,OAAUiQ,EAAK7F,KAAMnG,EAAWsE,EAASnI,KAAKiM,kBACzDgC,KACAA,IAAY,QAAoB4B,EAAK7F,KAAK7E,KAAMgD,IAErD,MAAMN,EAAiBM,EAAQN,eAC3B,EAAAnD,KAAA,UAAe9E,GACf2P,EAAS5N,KAAK/B,EAAQkQ,MAAKzN,IACvBwB,EAAUoK,GAAY5L,EACtB8F,EAAQN,eAAiBA,EACzB7H,KAAKqO,uBAAuBJ,EAAU4B,EAAK7F,KAAMnG,EAAWsE,EAAS+F,EAAeM,EAAaJ,GACjGpO,KAAK+P,kBAAkBF,EAAK7F,KAAM/I,EAAOkH,EAAStE,EAAWyL,EAAO9F,GAC7DnH,OAIXwB,EAAUoK,GAAYrO,EACtBI,KAAKqO,uBAAuBJ,EAAU4B,EAAK7F,KAAMnG,EAAWsE,EAAS+F,EAAeM,EAAaJ,GACjGpO,KAAK+P,kBAAkBF,EAAK7F,KAAM/I,EAAOkH,EAAStE,EAAWyL,EAAO9F,GAE5E,MAEIxJ,KAAK+P,kBAAkBF,EAAK7F,KAAM/I,EAAOkH,EAAStE,EAAWyL,EAAO9F,EAE5E,CACA,OAAO+F,CACX,CACA,iBAAAQ,CAAkB/F,EAAM/I,EAAOkH,EAAStE,EAAWyL,EAAO9F,GACtDQ,EAAKK,SAASpK,SAAS+P,IACnB,MAAO/B,IAAa,QAAoB+B,EAAU7K,KAAMgD,IACpDmH,EAAMrB,IAAczE,EAAU3I,IAAImP,EAAU7K,QAI3B,UAAjB6K,EAAUlF,GACNkF,EAAUC,WAAWzJ,MAAKrB,MACjB,QAAUA,EAAMtB,EAAWsE,OAEpCmH,EAAMrB,IAAY,EAClBhN,EAAMU,KAAK,CAAE8F,SAAUU,EAAQN,eAAgBmC,KAAMgG,KAIxDA,EAAUC,WAAW3C,OAAMnI,MACnB,QAAUA,EAAMtB,EAAWsE,OAEpCmH,EAAMrB,IAAY,EAClBhN,EAAMU,KAAK,CAAE8F,SAAUU,EAAQN,eAAgBmC,KAAMgG,KACzD,GAER,CAIA,OAAAlP,GACI8H,OAAO1E,KAAKlE,KAAKkH,WACZjH,SAAQ0E,GAAO3E,KAAKkH,UAAUvC,GAAK1E,SAAQC,GAAUA,EAAOY,aACrE,CACA,sBAAA6M,CAAuBtE,GACnBT,OAAO1E,KAAKmF,GAAQpJ,SAAQkF,IACxB,MAAMuF,EAAQrB,EAAOlE,IACd8I,IAAa,QAAc9I,GAC5B6E,EAAOhK,KAAKgL,MAAMkC,MAAMe,GAC9B,GAAIjE,EAAKkC,WAAkB,OAAKlC,EAAKkC,WAAkB,MAAElJ,MAAO,CAC5D,MAAM3C,EAAQ2J,EAAKkC,WAAkB,MAAElJ,MACjCkN,EAAQ7P,EAAMU,SAAW2J,EAAMrK,MAAMU,QACvC2J,EAAMrK,MAAMiN,OAAM,CAAC7G,EAAKxD,KAA4B,IAAlB5C,EAAM4C,IAAiB5C,EAAM4C,KAAWwD,IAC9E,EAAA/B,KAAA,OAAYwL,GAAO,IAAM,sBAAsBlG,EAAK7E,mDAChB9E,gBAC5BqK,EAAMrK,UAClB,CACI2J,EAAKkC,WAAkB,OAAKlC,EAAKkC,WAAkB,MAAElJ,OACrD,EAAA0B,KAAA,OAAYgG,EAAMvK,QAAU6J,EAAKkC,WAAkB,MAAElJ,OAAO,IAAM,sBAAsBgH,EAAK7E,kDAEtF6E,EAAKkC,WAAkB,MAAElJ,kBAAkB0H,EAAMvK,SAC5D,GAER,CACA,SAAAsN,CAAUpE,GACN,MAAMtE,EAAS,CAAC,EAChB,IAAK,MAAMoL,KAAa9G,EACpB,GAAuB,MAAnBrJ,KAAK0L,YAAgD,MAA1B1L,KAAK0L,WAAWrC,QACN,MAArCrJ,KAAK0L,WAAWrC,OAAO8G,GAAoB,CAE3CpL,EADe/E,KAAK0L,WAAWrC,OAAO8G,GACxBhL,MAAQkE,EAAO8G,EACjC,MAEIpL,EAAOoL,GAAa9G,EAAO8G,GAGnC,OAAOpL,CACX,CACA,WAAA2I,CAAYrE,GACR,MAAM+G,EAAaxH,OAAO1E,KAAKmF,GAAQkB,QAAOpF,IAC1C,MAAO8I,IAAY,QAAc9I,GACjC,OAAqC,MAA9BnF,KAAKgL,MAAMkC,MAAMe,EAAiB,IAE7C,GAAImC,EAAWrP,OAAS,EACpB,MAAM,IAAIX,MACN,uDAAUgQ,gCAEtB,CACA,UAAAxC,CAAWtE,GACP,OAAOA,EAAQhI,KAAI6D,IACf,GAAuB,MAAnBnF,KAAK0L,YAAiD,MAA3B1L,KAAK0L,WAAWpC,SACV,MAAjCtJ,KAAK0L,WAAWpC,QAAQnE,GAAe,CAEvC,OADenF,KAAK0L,WAAWpC,QAAQnE,GACzBA,IAClB,CACA,OAAOA,CAAI,GACZ,CAAC,EACR,CACA,YAAA0I,CAAavE,GACTA,EAAQrJ,SAAQkF,IACZ,MAAOkL,IAAkB,QAAclL,GACvC,IAAKnF,KAAKgL,MAAMkC,MAAMmD,GAClB,MAAM,IAAIjQ,MAAM,eAAe+E,+BACnC,GAER,ECzfG,MAAMmL,EACT,WAAA3Q,CAAY4Q,EAAwB,CAAC,EAAGC,EAAe,CAAC,GACpDxQ,KAAKuQ,sBAAwBA,EAC7BvQ,KAAKwQ,aAAeA,CACxB,CAUA,YAAAC,CAAatL,EAAMuL,GACf1Q,KAAKuQ,sBAAsBpL,GAAQuL,EAAU9M,OAC7C5D,KAAKwQ,aAAaE,EAAUjQ,IAAMiQ,CACtC,CAMA,wBAAAC,CAAyBxL,GACrB,OAAOnF,KAAKuQ,sBAAsBpL,EACtC,CAKA,gBAAAyL,CAAiBnQ,GACb,OAAOT,KAAKwQ,aAAa/P,EAC7B,CAIA,OAAAK,GACI,IAAK,MAAM6D,KAAO3E,KAAKwQ,aACnBxQ,KAAKwQ,aAAa7L,GAAKhE,uBAChBX,KAAKwQ,aAAa7L,GAE7B,IAAK,MAAMQ,KAAQnF,KAAKuQ,sBACpBvQ,KAAKuQ,sBAAsBpL,GAAMrE,iBAC1Bd,KAAKuQ,sBAAsBpL,EAE1C,EC5BG,MAAM0L,EAAqB,oBACrBC,EAAqB,aAW3B,MAAMC,EAUT,WAAApR,CAAYqR,EAAUC,EAAc,CAAC,GACjCjR,KAAKgR,SAAWA,EAChBhR,KAAKiR,YAAcA,EACnBjR,KAAKkR,QAAU,MACI,MAAfD,IACAjR,KAAKiR,YAAc,CAAC,GAExBjR,KAAKgM,gBAAkB,IAAIsE,CAC/B,CAEA,gBAAIa,GACA,OAAOnR,KAAKkR,OAChB,CACA,cAAI9E,GACA,OAAOpM,KAAKoR,SAAShF,UACzB,CACA,eAAIE,GACA,OAAOtM,KAAKoR,SAAS9E,WACzB,CACA,UAAIjD,GACA,OAAOrJ,KAAKoR,SAAS/H,MACzB,CACA,WAAIC,GACA,OAAOtJ,KAAKoR,SAAS9H,OACzB,CACA,WAAI6D,GACA,OAAOnN,KAAKoR,SAASlK,SACzB,CACA,YAAImK,GACA,OAAOrR,KAAKsR,UAAUC,mBAC1B,CACA,kBAAIC,GACA,OAAOxR,KAAK2L,SAChB,CACA,aAAA8F,GACI,MAAMC,EAAO1R,KAAKgR,SAClB,GAAiB,MAAbU,EAAKC,KAEL3R,KAAK4R,QAAUF,OAEd,GAAoC,MAAhC1R,KAAKiR,YAAYY,YACtB7R,KAAK4R,QAAU,EAAAE,GAAA,mBAAsBJ,EAAM1R,KAAKiR,iBAE/C,CACD,MAAMc,EAAW,EAAAD,GAAA,gBAAmBJ,EAAM1R,KAAKiR,aAC/C,GAAwB,IAApBc,EAAShR,OAGTgR,EAASpQ,KAAK,EAAAmQ,GAAA,mBAAsBJ,EAAM1R,KAAKiR,mBAE9C,GAAIc,EAAShR,OAAS,EACvB,MAAM,IAAIX,MAAM,wBAAwB2R,EAAShR,kCACrC,CAAC2Q,OAEjB1R,KAAK4R,QAAUG,EAAS,EAC5B,CACJ,CAKA,UAAMJ,GAEF,GADA3R,KAAKyR,gBACoB,MAArBzR,KAAK4R,QAAQD,KACb,MAAM,IAAIvR,MAAM,iHAGpB,MAAMkR,QAAkBtR,KAAK4R,QAAQD,OACrC,OAAO3R,KAAKgS,SAASV,EACzB,CAOA,QAAAU,CAASV,GACLtR,KAAKsR,UAAYA,EACjB,MAAMtG,EAAQhL,KAAKsR,UAAUW,cAC7B,IAAItG,EAGAA,EAFsC,MAAtC3L,KAAKsR,UAAUC,qBACiC,MAAhDvR,KAAKsR,UAAUC,oBAAoB5F,UAE/B3L,KAAKsR,UAAUC,oBAAoB5F,UAG3B3L,KAAKsR,UAAU3F,UAE/B3L,KAAK2L,UAAYA,EACjB3L,KAAKkR,QAAU,GAAGlG,EAAMkH,SAASC,YAAYnH,EAAMkH,SAASE,cAC5D,MAAMlL,EAAY,EAAA4K,GAAA,cAAiB9R,KAAKsR,UAAUe,WAAYrS,KAAKsR,UAAUgB,aAM7E,GALAtS,KAAKoR,SAAW,IAAIrG,EAAc,KAAgBwH,SAASC,eAAexH,EAAOhL,KAAK2L,YACtF3L,KAAKoR,SAASlK,UAAYlH,KAAKyS,6BAA6BvL,GAG5DlH,KAAKoR,SAASpF,gBAAkBhM,KAAKgM,gBACH,MAA9BsF,EAAUoB,kBACyB,MAAnCpB,EAAUoB,iBAAiB1I,KAAc,CACzC,MAAM2I,EAAc,KAAgBJ,SAASC,eAAelB,EAAUoB,kBACtE1S,KAAK2S,YAAc,IAAI5H,EAAc4H,GACrC3S,KAAK2S,YAAYzL,UAAYlH,KAAKoR,SAASlK,UAI3ClH,KAAK2S,YAAY3G,gBAAkBhM,KAAKgM,gBACxChM,KAAK2S,YAAY/D,aAAa,CAAC,EAAG,GACtC,CACA,OAAO,CACX,CA6CA,UAAMgE,CAAKC,EAAcC,GACrB,GAA4B,kBAAjBD,EAA2B,CAClC,MAAMd,EAAW,EAAAD,GAAA,gBAAmBe,GACpC,GAAwB,IAApBd,EAAShR,OACT,MAAM,IAAIX,MAAM,0CAA0CyS,MAEzD,GAAId,EAAShR,OAAS,EACvB,MAAM,IAAIX,MAAM,wBAAwB2R,EAAShR,kCACrC8R,MAEhBA,EAAed,EAAS,EAC5B,CACA,GAAyB,MAArBc,EAAaD,KACb,MAAM,IAAIxS,MAAM,+GAGpB,OAAOyS,EAAaD,KAAK5S,KAAKsR,UAClC,CAuCA,OAAAyB,CAAQ1J,EAAQyJ,GACZ,OAAO9S,KAAKwN,QAAQnE,EAAQrJ,KAAKsM,YACrC,CACA,eAAA0G,CAAgB3J,GACZ,KAAMA,aAAkB,EAAA4J,UAAYC,MAAMC,QAAQ9J,GAE9C,OAAOA,EAGX,IADAA,EAAS6J,MAAMC,QAAQ9J,GAAUA,EAAS,CAACA,IAChCtI,SAAWf,KAAKoM,WAAWrL,OAClC,MAAM,IAAIX,MACN,mDAAuBJ,KAAKoM,WAAWrL,wCACpBsI,EAAOtI,yBAElC,OAAOf,KAAKoM,WAAWI,QAAO,CAAClL,EAAK6O,EAAW/N,KAC3Cd,EAAI6O,GAAa9G,EAAOjH,GACjBd,IACR,CAAC,EACR,CACA,gBAAA8R,CAAiB9J,GAEb,OADAA,EAAUA,GAAWtJ,KAAKsM,YAClB4G,MAAMC,QAAQ7J,GAAuBA,EAAZ,CAACA,EACtC,CAiBA,OAAAkE,CAAQnE,EAAQC,GACZD,EAASrJ,KAAKgT,gBAAgB3J,GAC9BC,EAAUtJ,KAAKoT,iBAAiB9J,GAChC,MAAMvE,EAAS/E,KAAKoR,SAAS5D,QAAQnE,EAAQC,GAC7C,OAAOvE,EAAOhE,OAAS,EAAIgE,EAASA,EAAO,EAC/C,CAiBA,kBAAM6J,CAAavF,EAAQC,GACvBD,EAASrJ,KAAKgT,gBAAgB3J,GAC9BC,EAAUtJ,KAAKoT,iBAAiB9J,GAChC,MAAMvE,QAAe/E,KAAKoR,SAASxC,aAAavF,EAAQC,GACxD,OAAOvE,EAAOhE,OAAS,EAAIgE,EAASA,EAAO,EAC/C,CACA,4BAAA0N,CAA6BnR,GACzB,OAAOsH,OAAO1E,KAAK5C,GAAKkL,QAAO,CAAC6G,EAAQ1O,KACpC0O,EAAO1O,GAAO,CAACrD,EAAIqD,IACZ0O,IACR,CAAC,EACR,CAMA,OAAAvS,GACId,KAAKoR,SAAStQ,UACVd,KAAK2S,aACL3S,KAAK2S,YAAY7R,UAErBd,KAAKgM,gBAAgBlL,SACzB,EAgCGwS,eAAeC,EAAevC,EAAUwC,EAAU,CAAC,GACtD,GAAgB,MAAZxC,EACA,MAAM,IAAI5Q,MAAM,0GAGL,MAAXoT,IACAA,EAAU,CAAC,GAEXA,EAAQC,WACa,MAAjBzC,EAASW,OACJX,EAAS0C,SAAS,OACnB1C,GAAsB,KAE1BA,EAAW,GAAGA,IAAWF,IAAqBD,KAGtD,MAAM8C,EAAQ,IAAI5C,EAAWC,EAAUwC,GAEvC,aADMG,EAAMhC,OACLgC,CACX,C,+CChXO,IAAIC,EAgCAC,E,gCA/BX,SAAWD,GACPA,EAASA,EAAqB,WAAI,GAAK,aACvCA,EAASA,EAAmB,SAAI,GAAK,WACrCA,EAASA,EAAoB,UAAI,GAAK,YACtCA,EAASA,EAAmB,SAAI,GAAK,WACrCA,EAASA,EAAmB,SAAI,GAAK,WACrCA,EAASA,EAAmB,SAAI,GAAK,WACrCA,EAASA,EAAkB,QAAI,GAAK,UACpCA,EAASA,EAAoB,UAAI,GAAK,YACtCA,EAASA,EAAuB,aAAI,GAAK,eACzCA,EAASA,EAAmB,SAAI,GAAK,WACrCA,EAASA,EAAkB,QAAI,IAAM,UACrCA,EAASA,EAAmB,SAAI,IAAM,WACtCA,EAASA,EAAoB,UAAI,IAAM,YACvCA,EAASA,EAAoB,UAAI,IAAM,YACvCA,EAASA,EAAsB,YAAI,IAAM,cACzCA,EAASA,EAAuB,aAAI,KAAO,eAC3CA,EAASA,EAAwB,cAAI,KAAO,gBAC5CA,EAASA,EAAuB,aAAI,KAAO,eAC3CA,EAASA,EAAuB,aAAI,KAAO,eAC3CA,EAASA,EAAuB,aAAI,KAAO,eAC3CA,EAASA,EAAsB,YAAI,KAAO,cAC1CA,EAASA,EAAwB,cAAI,KAAO,gBAC5CA,EAASA,EAA2B,iBAAI,KAAO,mBAC/CA,EAASA,EAAuB,aAAI,KAAO,eAC3CA,EAASA,EAAsB,YAAI,KAAO,cAC1CA,EAASA,EAAuB,aAAI,KAAO,eAC3CA,EAASA,EAAwB,cAAI,KAAO,gBAC5CA,EAASA,EAAwB,cAAI,KAAO,gBAC5CA,EAASA,EAA0B,gBAAI,KAAO,iBACjD,CA9BD,CA8BGA,IAAaA,EAAW,CAAC,IAE5B,SAAWC,GAEP,IAAIC,GACJ,SAAWA,GACPA,EAAwBA,EAAgC,OAAI,GAAK,SACjEA,EAAwBA,EAA4B,GAAI,GAAK,KAC7DA,EAAwBA,EAA4B,GAAI,GAAK,IAChE,CAJD,CAIGA,EAA0BD,EAASC,0BAA4BD,EAASC,wBAA0B,CAAC,GACzG,CARD,CAQGD,IAAaA,EAAW,CAAC,G","sources":["webpack://StylistWidget/./node_modules/@tensorflow/tfjs-converter/dist/executor/tensor_list.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-converter/dist/executor/hash_table.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-converter/dist/executor/tensor_array.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-converter/dist/executor/tensor_utils.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-converter/dist/executor/execution_context.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-converter/dist/executor/model_analysis.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-converter/dist/executor/graph_executor.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-converter/dist/executor/resource_manager.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-converter/dist/executor/graph_model.js","webpack://StylistWidget/./node_modules/@tensorflow/tfjs-converter/dist/data/compiled_api.js"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { concat, keep, reshape, scalar, slice, stack, tensor, tidy, unstack } from '@tensorflow/tfjs-core';\nimport { assertShapesMatchAllowUndefinedSize, inferElementShape, mergeElementShape } from './tensor_utils';\n/**\n * TensorList stores a container of `tf.Tensor` objects, which are accessible\n * via tensors field.\n *\n * In order to get a copy of the underlying list, use the copy method:\n * ```\n *    TensorList b = a.copy();\n *    b.tensors().pushBack(t);  // This does not modify a.tensors().\n * ```\n *\n * Note that this is not a deep copy: the memory locations of the underlying\n * tensors will still point to the same locations of the corresponding tensors\n * in the original.\n */\nexport class TensorList {\n    /**\n     *\n     * @param tensors list of tensors\n     * @param elementShape shape of each tensor, this can be a single number (any\n     * shape is allowed) or partial shape (dim = -1).\n     * @param elementDtype data type of each tensor\n     * @param maxNumElements The maximum allowed size of `tensors`. Defaults to -1\n     *   meaning that the size of `tensors` is unbounded.\n     */\n    constructor(tensors, elementShape, elementDtype, maxNumElements = -1) {\n        this.tensors = tensors;\n        this.elementShape = elementShape;\n        this.elementDtype = elementDtype;\n        if (tensors != null) {\n            tensors.forEach(tensor => {\n                if (elementDtype !== tensor.dtype) {\n                    throw new Error(`Invalid data types; op elements ${elementDtype}, but list elements ${tensor.dtype}`);\n                }\n                assertShapesMatchAllowUndefinedSize(elementShape, tensor.shape, 'TensorList shape mismatch: ');\n                keep(tensor);\n            });\n        }\n        this.idTensor = scalar(0);\n        this.maxNumElements = maxNumElements;\n        keep(this.idTensor);\n    }\n    get id() {\n        return this.idTensor.id;\n    }\n    /**\n     * Get a new TensorList containing a copy of the underlying tensor container.\n     */\n    copy() {\n        return new TensorList([...this.tensors], this.elementShape, this.elementDtype);\n    }\n    /**\n     * Dispose the tensors and idTensor and clear the tensor list.\n     */\n    clearAndClose(keepIds) {\n        this.tensors.forEach(tensor => {\n            if (keepIds == null || !keepIds.has(tensor.id)) {\n                tensor.dispose();\n            }\n        });\n        this.tensors.length = 0;\n        this.idTensor.dispose();\n    }\n    /**\n     * The size of the tensors in the tensor list.\n     */\n    size() {\n        return this.tensors.length;\n    }\n    /**\n     * Return a tensor that stacks a list of rank-R tf.Tensors into one rank-(R+1)\n     * tf.Tensor.\n     * @param elementShape shape of each tensor\n     * @param elementDtype data type of each tensor\n     * @param numElements the number of elements to stack\n     */\n    stack(elementShape, elementDtype, numElements = -1) {\n        if (elementDtype !== this.elementDtype) {\n            throw new Error(`Invalid data types; op elements ${elementDtype}, but list elements ${this.elementDtype}`);\n        }\n        if (numElements !== -1 && this.tensors.length !== numElements) {\n            throw new Error(`Operation expected a list with ${numElements} elements but got a list with ${this.tensors.length} elements.`);\n        }\n        assertShapesMatchAllowUndefinedSize(elementShape, this.elementShape, 'TensorList shape mismatch: ');\n        const outputElementShape = inferElementShape(this.elementShape, this.tensors, elementShape);\n        return tidy(() => {\n            const reshapedTensors = this.tensors.map(tensor => reshape(tensor, outputElementShape));\n            return stack(reshapedTensors, 0);\n        });\n    }\n    /**\n     * Pop a tensor from the end of the list.\n     * @param elementShape shape of the tensor\n     * @param elementDtype data type of the tensor\n     */\n    popBack(elementShape, elementDtype) {\n        if (elementDtype !== this.elementDtype) {\n            throw new Error(`Invalid data types; op elements ${elementDtype}, but list elements ${this.elementDtype}`);\n        }\n        if (this.size() === 0) {\n            throw new Error('Trying to pop from an empty list.');\n        }\n        const outputElementShape = inferElementShape(this.elementShape, this.tensors, elementShape);\n        const tensor = this.tensors.pop();\n        assertShapesMatchAllowUndefinedSize(tensor.shape, elementShape, 'TensorList shape mismatch: ');\n        return reshape(tensor, outputElementShape);\n    }\n    /**\n     * Push a tensor to the end of the list.\n     * @param tensor Tensor to be pushed.\n     */\n    pushBack(tensor) {\n        if (tensor.dtype !== this.elementDtype) {\n            throw new Error(`Invalid data types; op elements ${tensor.dtype}, but list elements ${this.elementDtype}`);\n        }\n        assertShapesMatchAllowUndefinedSize(tensor.shape, this.elementShape, 'TensorList shape mismatch: ');\n        if (this.maxNumElements === this.size()) {\n            throw new Error(`Trying to push element into a full list.`);\n        }\n        keep(tensor);\n        this.tensors.push(tensor);\n    }\n    /**\n     * Update the size of the list.\n     * @param size the new size of the list.\n     */\n    resize(size) {\n        if (size < 0) {\n            throw new Error(`TensorListResize expects size to be non-negative. Got: ${size}`);\n        }\n        if (this.maxNumElements !== -1 && size > this.maxNumElements) {\n            throw new Error(`TensorListResize input size ${size} is greater maxNumElement ${this.maxNumElements}.`);\n        }\n        this.tensors.length = size;\n    }\n    /**\n     * Retrieve the element at the provided index\n     * @param elementShape shape of the tensor\n     * @param elementDtype dtype of the tensor\n     * @param elementIndex index of the tensor\n     */\n    getItem(elementIndex, elementShape, elementDtype) {\n        if (elementDtype !== this.elementDtype) {\n            throw new Error(`Invalid data types; op elements ${elementDtype}, but list elements ${this.elementDtype}`);\n        }\n        if (elementIndex < 0 || elementIndex > this.tensors.length) {\n            throw new Error(`Trying to access element ${elementIndex} in a list with ${this.tensors.length} elements.`);\n        }\n        if (this.tensors[elementIndex] == null) {\n            throw new Error(`element at index ${elementIndex} is null.`);\n        }\n        assertShapesMatchAllowUndefinedSize(this.tensors[elementIndex].shape, elementShape, 'TensorList shape mismatch: ');\n        const outputElementShape = inferElementShape(this.elementShape, this.tensors, elementShape);\n        return reshape(this.tensors[elementIndex], outputElementShape);\n    }\n    /**\n     * Set the tensor at the index\n     * @param elementIndex index of the tensor\n     * @param tensor the tensor to be inserted into the list\n     */\n    setItem(elementIndex, tensor) {\n        if (tensor.dtype !== this.elementDtype) {\n            throw new Error(`Invalid data types; op elements ${tensor.dtype}, but list elements ${this.elementDtype}`);\n        }\n        if (elementIndex < 0 ||\n            this.maxNumElements !== -1 && elementIndex >= this.maxNumElements) {\n            throw new Error(`Trying to set element ${elementIndex} in a list with max ${this.maxNumElements} elements.`);\n        }\n        assertShapesMatchAllowUndefinedSize(this.elementShape, tensor.shape, 'TensorList shape mismatch: ');\n        keep(tensor);\n        this.tensors[elementIndex] = tensor;\n    }\n    /**\n     * Return selected values in the TensorList as a stacked Tensor. All of\n     * selected values must have been written and their shapes must all match.\n     * @param indices indices of tensors to gather\n     * @param elementDtype output tensor dtype\n     * @param elementShape output tensor element shape\n     */\n    gather(indices, elementDtype, elementShape) {\n        if (elementDtype !== this.elementDtype) {\n            throw new Error(`Invalid data types; op elements ${elementDtype}, but list elements ${this.elementDtype}`);\n        }\n        assertShapesMatchAllowUndefinedSize(this.elementShape, elementShape, 'TensorList shape mismatch: ');\n        // When indices is greater than the size of the list, indices beyond the\n        // size of the list are ignored.\n        indices = indices.slice(0, this.size());\n        const outputElementShape = inferElementShape(this.elementShape, this.tensors, elementShape);\n        if (indices.length === 0) {\n            return tensor([], [0].concat(outputElementShape));\n        }\n        return tidy(() => {\n            const tensors = indices.map(i => reshape(this.tensors[i], outputElementShape));\n            return stack(tensors, 0);\n        });\n    }\n    /**\n     * Return the values in the TensorList as a concatenated Tensor.\n     * @param elementDtype output tensor dtype\n     * @param elementShape output tensor element shape\n     */\n    concat(elementDtype, elementShape) {\n        if (!!elementDtype && elementDtype !== this.elementDtype) {\n            throw new Error(`TensorList dtype is ${this.elementDtype} but concat requested dtype ${elementDtype}`);\n        }\n        assertShapesMatchAllowUndefinedSize(this.elementShape, elementShape, 'TensorList shape mismatch: ');\n        const outputElementShape = inferElementShape(this.elementShape, this.tensors, elementShape);\n        if (this.size() === 0) {\n            return tensor([], [0].concat(outputElementShape));\n        }\n        return tidy(() => {\n            const tensors = this.tensors.map(t => reshape(t, outputElementShape));\n            return concat(tensors, 0);\n        });\n    }\n}\n/**\n * Creates a TensorList which, when stacked, has the value of tensor.\n * @param tensor from tensor\n * @param elementShape output tensor element shape\n */\nexport function fromTensor(tensor, elementShape, elementDtype) {\n    const dtype = tensor.dtype;\n    if (tensor.shape.length < 1) {\n        throw new Error(`Tensor must be at least a vector, but saw shape: ${tensor.shape}`);\n    }\n    if (tensor.dtype !== elementDtype) {\n        throw new Error(`Invalid data types; op elements ${tensor.dtype}, but list elements ${elementDtype}`);\n    }\n    const tensorElementShape = tensor.shape.slice(1);\n    assertShapesMatchAllowUndefinedSize(tensorElementShape, elementShape, 'TensorList shape mismatch: ');\n    const tensorList = unstack(tensor);\n    return new TensorList(tensorList, elementShape, dtype);\n}\n/**\n * Return a TensorList of the given size with empty elements.\n * @param elementShape the shape of the future elements of the list\n * @param elementDtype the desired type of elements in the list\n * @param numElements the number of elements to reserve\n */\nexport function reserve(elementShape, elementDtype, numElements) {\n    return new TensorList([], elementShape, elementDtype, numElements);\n}\n/**\n * Put tensors at specific indices of a stacked tensor into a TensorList.\n * @param indices list of indices on how to scatter the tensor.\n * @param tensor input tensor.\n * @param elementShape the shape of the future elements of the list\n * @param numElements the number of elements to scatter\n */\nexport function scatter(tensor, indices, elementShape, numElements) {\n    if (indices.length !== tensor.shape[0]) {\n        throw new Error(`Expected len(indices) == tensor.shape[0], but saw: ${indices.length} vs. ${tensor.shape[0]}`);\n    }\n    const maxIndex = Math.max(...indices);\n    if (numElements != null && numElements !== -1 && maxIndex >= numElements) {\n        throw new Error(`Max index must be < array size (${maxIndex}  vs. ${numElements})`);\n    }\n    const list = new TensorList([], elementShape, tensor.dtype, numElements);\n    const tensors = unstack(tensor, 0);\n    indices.forEach((value, index) => {\n        list.setItem(value, tensors[index]);\n    });\n    return list;\n}\n/**\n * Split the values of a Tensor into a TensorList.\n * @param length the lengths to use when splitting value along\n *    its first dimension.\n * @param tensor the tensor to split.\n * @param elementShape the shape of the future elements of the list\n */\nexport function split(tensor, length, elementShape) {\n    let totalLength = 0;\n    const cumulativeLengths = length.map(len => {\n        totalLength += len;\n        return totalLength;\n    });\n    if (totalLength !== tensor.shape[0]) {\n        throw new Error(`Expected sum of lengths to be equal to\n          tensor.shape[0], but sum of lengths is\n        ${totalLength}, and tensor's shape is: ${tensor.shape}`);\n    }\n    const shapeWithoutFirstDim = tensor.shape.slice(1);\n    const outputElementShape = mergeElementShape(shapeWithoutFirstDim, elementShape);\n    const elementPerRow = totalLength === 0 ? 0 : tensor.size / totalLength;\n    const tensors = tidy(() => {\n        const tensors = [];\n        tensor = reshape(tensor, [1, totalLength, elementPerRow]);\n        for (let i = 0; i < length.length; ++i) {\n            const previousLength = (i === 0) ? 0 : cumulativeLengths[i - 1];\n            const indices = [0, previousLength, 0];\n            const sizes = [1, length[i], elementPerRow];\n            tensors[i] = reshape(slice(tensor, indices, sizes), outputElementShape);\n        }\n        tensor.dispose();\n        return tensors;\n    });\n    const list = new TensorList([], elementShape, tensor.dtype, length.length);\n    for (let i = 0; i < tensors.length; i++) {\n        list.setItem(i, tensors[i]);\n    }\n    return list;\n}\n//# sourceMappingURL=tensor_list.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { keep, scalar, stack, tidy, unstack, util } from '@tensorflow/tfjs-core';\n// tslint:disable-next-line: no-imports-from-dist\nimport * as tfOps from '@tensorflow/tfjs-core/dist/ops/ops_for_converter';\n/**\n * Hashtable contains a set of tensors, which can be accessed by key.\n */\nexport class HashTable {\n    /**\n     * Constructor of HashTable. Creates a hash table.\n     *\n     * @param keyDType `dtype` of the table keys.\n     * @param valueDType `dtype` of the table values.\n     */\n    constructor(keyDType, valueDType) {\n        this.keyDType = keyDType;\n        this.valueDType = valueDType;\n        this.handle = scalar(0);\n        // tslint:disable-next-line: no-any\n        this.tensorMap = new Map();\n        keep(this.handle);\n    }\n    get id() {\n        return this.handle.id;\n    }\n    /**\n     * Dispose the tensors and handle and clear the hashtable.\n     */\n    clearAndClose() {\n        this.tensorMap.forEach(value => value.dispose());\n        this.tensorMap.clear();\n        this.handle.dispose();\n    }\n    /**\n     * The number of items in the hash table.\n     */\n    size() {\n        return this.tensorMap.size;\n    }\n    /**\n     * The number of items in the hash table as a rank-0 tensor.\n     */\n    tensorSize() {\n        return tfOps.scalar(this.size(), 'int32');\n    }\n    /**\n     * Replaces the contents of the table with the specified keys and values.\n     * @param keys Keys to store in the hashtable.\n     * @param values Values to store in the hashtable.\n     */\n    async import(keys, values) {\n        this.checkKeyAndValueTensor(keys, values);\n        // We only store the primitive values of the keys, this allows lookup\n        // to be O(1).\n        const $keys = await keys.data();\n        // Clear the hashTable before inserting new values.\n        this.tensorMap.forEach(value => value.dispose());\n        this.tensorMap.clear();\n        return tidy(() => {\n            const $values = unstack(values);\n            const keysLength = $keys.length;\n            const valuesLength = $values.length;\n            util.assert(keysLength === valuesLength, () => `The number of elements doesn't match, keys has ` +\n                `${keysLength} elements, the values has ${valuesLength} ` +\n                `elements.`);\n            for (let i = 0; i < keysLength; i++) {\n                const key = $keys[i];\n                const value = $values[i];\n                keep(value);\n                this.tensorMap.set(key, value);\n            }\n            return this.handle;\n        });\n    }\n    /**\n     * Looks up keys in a hash table, outputs the corresponding values.\n     *\n     * Performs batch lookups, for every element in the key tensor, `find`\n     * stacks the corresponding value into the return tensor.\n     *\n     * If an element is not present in the table, the given `defaultValue` is\n     * used.\n     *\n     * @param keys Keys to look up. Must have the same type as the keys of the\n     *     table.\n     * @param defaultValue The scalar `defaultValue` is the value output for keys\n     *     not present in the table. It must also be of the same type as the\n     *     table values.\n     */\n    async find(keys, defaultValue) {\n        this.checkKeyAndValueTensor(keys, defaultValue);\n        const $keys = await keys.data();\n        return tidy(() => {\n            const result = [];\n            for (let i = 0; i < $keys.length; i++) {\n                const key = $keys[i];\n                const value = this.findWithDefault(key, defaultValue);\n                result.push(value);\n            }\n            return stack(result);\n        });\n    }\n    // tslint:disable-next-line: no-any\n    findWithDefault(key, defaultValue) {\n        const result = this.tensorMap.get(key);\n        return result != null ? result : defaultValue;\n    }\n    checkKeyAndValueTensor(key, value) {\n        if (key.dtype !== this.keyDType) {\n            throw new Error(`Expect key dtype ${this.keyDType}, but got ` +\n                `${key.dtype}`);\n        }\n        if (value.dtype !== this.valueDType) {\n            throw new Error(`Expect value dtype ${this.valueDType}, but got ` +\n                `${value.dtype}`);\n        }\n    }\n}\n//# sourceMappingURL=hash_table.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { concat, keep, reshape, scalar, slice, stack, tensor, tidy, unstack } from '@tensorflow/tfjs-core';\nimport { assertShapesMatchAllowUndefinedSize } from './tensor_utils';\n/**\n * The TensorArray object keeps an array of Tensors.  It\n * allows reading from the array and writing to the array.\n */\nexport class TensorArray {\n    constructor(name, dtype, maxSize, elementShape, identicalElementShapes, dynamicSize, clearAfterRead) {\n        this.name = name;\n        this.dtype = dtype;\n        this.maxSize = maxSize;\n        this.elementShape = elementShape;\n        this.identicalElementShapes = identicalElementShapes;\n        this.dynamicSize = dynamicSize;\n        this.clearAfterRead = clearAfterRead;\n        this.tensors = [];\n        this.closed_ = false;\n        this.idTensor = scalar(0);\n        keep(this.idTensor);\n    }\n    get id() {\n        return this.idTensor.id;\n    }\n    get closed() {\n        return this.closed_;\n    }\n    /**\n     * Dispose the tensors and idTensor and mark the TensoryArray as closed.\n     */\n    clearAndClose(keepIds) {\n        this.tensors.forEach(tensor => {\n            if (keepIds == null || !keepIds.has(tensor.tensor.id)) {\n                tensor.tensor.dispose();\n            }\n        });\n        this.tensors = [];\n        this.closed_ = true;\n        this.idTensor.dispose();\n    }\n    size() {\n        return this.tensors.length;\n    }\n    /**\n     * Read the value at location index in the TensorArray.\n     * @param index Number the index to read from.\n     */\n    read(index) {\n        if (this.closed_) {\n            throw new Error(`TensorArray ${this.name} has already been closed.`);\n        }\n        if (index < 0 || index >= this.size()) {\n            throw new Error(`Tried to read from index ${index}, but array size is: ${this.size()}`);\n        }\n        const tensorWithState = this.tensors[index];\n        if (tensorWithState.cleared) {\n            throw new Error(`TensorArray ${this.name}: Could not read index ${index} twice because it was cleared after a previous read ` +\n                `(perhaps try setting clear_after_read = false?).`);\n        }\n        if (this.clearAfterRead) {\n            tensorWithState.cleared = true;\n        }\n        tensorWithState.read = true;\n        return tensorWithState.tensor;\n    }\n    /**\n     * Helper method to read multiple tensors from the specified indices.\n     */\n    readMany(indices) {\n        return indices.map(index => this.read(index));\n    }\n    /**\n     * Write value into the index of the TensorArray.\n     * @param index number the index to write to.\n     * @param tensor\n     */\n    write(index, tensor) {\n        if (this.closed_) {\n            throw new Error(`TensorArray ${this.name} has already been closed.`);\n        }\n        if (index < 0 || !this.dynamicSize && index >= this.maxSize) {\n            throw new Error(`Tried to write to index ${index}, but array is not resizeable and size is: ${this.maxSize}`);\n        }\n        const t = this.tensors[index] || {};\n        if (tensor.dtype !== this.dtype) {\n            throw new Error(`TensorArray ${this.name}: Could not write to TensorArray index ${index},\n          because the value dtype is ${tensor.dtype}, but TensorArray dtype is ${this.dtype}.`);\n        }\n        // Set the shape for the first time write to unknow shape tensor array\n        if (this.size() === 0 &&\n            (this.elementShape == null || this.elementShape.length === 0)) {\n            this.elementShape = tensor.shape;\n        }\n        assertShapesMatchAllowUndefinedSize(this.elementShape, tensor.shape, `TensorArray ${this.name}: Could not write to TensorArray index ${index}.`);\n        if (t.read) {\n            throw new Error(`TensorArray ${this.name}: Could not write to TensorArray index ${index}, because it has already been read.`);\n        }\n        if (t.written) {\n            throw new Error(`TensorArray ${this.name}: Could not write to TensorArray index ${index}, because it has already been written.`);\n        }\n        t.tensor = tensor;\n        keep(tensor);\n        t.written = true;\n        this.tensors[index] = t;\n    }\n    /**\n     * Helper method to write multiple tensors to the specified indices.\n     */\n    writeMany(indices, tensors) {\n        if (indices.length !== tensors.length) {\n            throw new Error(`TensorArray ${this.name}: could not write multiple tensors,` +\n                `because the index size: ${indices.length} is not the same as tensors size: ${tensors.length}.`);\n        }\n        indices.forEach((i, index) => this.write(i, tensors[index]));\n    }\n    /**\n     * Return selected values in the TensorArray as a packed Tensor. All of\n     * selected values must have been written and their shapes must all match.\n     * @param [indices] number[] Optional. Taking values in [0, max_value). If the\n     *    TensorArray is not dynamic, max_value=size(). If not specified returns\n     *    all tensors in the original order.\n     * @param [dtype]\n     */\n    gather(indices, dtype) {\n        if (!!dtype && dtype !== this.dtype) {\n            throw new Error(`TensorArray dtype is ${this.dtype} but gather requested dtype ${dtype}`);\n        }\n        if (!indices) {\n            indices = [];\n            for (let i = 0; i < this.size(); i++) {\n                indices.push(i);\n            }\n        }\n        else {\n            indices = indices.slice(0, this.size());\n        }\n        if (indices.length === 0) {\n            return tensor([], [0].concat(this.elementShape));\n        }\n        // Read all the PersistentTensors into a vector to keep track of\n        // their memory.\n        const tensors = this.readMany(indices);\n        assertShapesMatchAllowUndefinedSize(this.elementShape, tensors[0].shape, 'TensorArray shape mismatch: ');\n        return stack(tensors, 0);\n    }\n    /**\n     * Return the values in the TensorArray as a concatenated Tensor.\n     */\n    concat(dtype) {\n        if (!!dtype && dtype !== this.dtype) {\n            throw new Error(`TensorArray dtype is ${this.dtype} but concat requested dtype ${dtype}`);\n        }\n        if (this.size() === 0) {\n            return tensor([], [0].concat(this.elementShape));\n        }\n        const indices = [];\n        for (let i = 0; i < this.size(); i++) {\n            indices.push(i);\n        }\n        // Collect all the tensors from the tensors array.\n        const tensors = this.readMany(indices);\n        assertShapesMatchAllowUndefinedSize(this.elementShape, tensors[0].shape, `TensorArray shape mismatch: tensor array shape (${this.elementShape}) vs first tensor shape (${tensors[0].shape})`);\n        return concat(tensors, 0);\n    }\n    /**\n     * Scatter the values of a Tensor in specific indices of a TensorArray.\n     * @param indices nummber[] values in [0, max_value). If the\n     *    TensorArray is not dynamic, max_value=size().\n     * @param tensor Tensor input tensor.\n     */\n    scatter(indices, tensor) {\n        if (tensor.dtype !== this.dtype) {\n            throw new Error(`TensorArray dtype is ${this.dtype} but tensor has dtype ${tensor.dtype}`);\n        }\n        if (indices.length !== tensor.shape[0]) {\n            throw new Error(`Expected len(indices) == tensor.shape[0], but saw: ${indices.length} vs. ${tensor.shape[0]}`);\n        }\n        const maxIndex = Math.max(...indices);\n        if (!this.dynamicSize && maxIndex >= this.maxSize) {\n            throw new Error(`Max index must be < array size (${maxIndex}  vs. ${this.maxSize})`);\n        }\n        this.writeMany(indices, unstack(tensor, 0));\n    }\n    /**\n     * Split the values of a Tensor into the TensorArray.\n     * @param length number[] with the lengths to use when splitting value along\n     *    its first dimension.\n     * @param tensor Tensor, the tensor to split.\n     */\n    split(length, tensor) {\n        if (tensor.dtype !== this.dtype) {\n            throw new Error(`TensorArray dtype is ${this.dtype} but tensor has dtype ${tensor.dtype}`);\n        }\n        let totalLength = 0;\n        const cumulativeLengths = length.map(len => {\n            totalLength += len;\n            return totalLength;\n        });\n        if (totalLength !== tensor.shape[0]) {\n            throw new Error(`Expected sum of lengths to be equal to\n          tensor.shape[0], but sum of lengths is\n        ${totalLength}, and tensor's shape is: ${tensor.shape}`);\n        }\n        if (!this.dynamicSize && length.length !== this.maxSize) {\n            throw new Error(`TensorArray's size is not equal to the size of lengths (${this.maxSize} vs. ${length.length}), ` +\n                'and the TensorArray is not marked as dynamically resizeable');\n        }\n        const elementPerRow = totalLength === 0 ? 0 : tensor.size / totalLength;\n        const tensors = [];\n        tidy(() => {\n            tensor = reshape(tensor, [1, totalLength, elementPerRow]);\n            for (let i = 0; i < length.length; ++i) {\n                const previousLength = (i === 0) ? 0 : cumulativeLengths[i - 1];\n                const indices = [0, previousLength, 0];\n                const sizes = [1, length[i], elementPerRow];\n                tensors[i] = reshape(slice(tensor, indices, sizes), this.elementShape);\n            }\n            return tensors;\n        });\n        const indices = [];\n        for (let i = 0; i < length.length; i++) {\n            indices[i] = i;\n        }\n        this.writeMany(indices, tensors);\n    }\n}\n//# sourceMappingURL=tensor_array.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n/**\n * This differs from util.assertShapesMatch in that it allows values of\n * negative one, an undefined size of a dimensinon, in a shape to match\n * anything.\n */\nimport { util } from '@tensorflow/tfjs-core';\n/**\n * Used by TensorList and TensorArray to verify if elementShape matches, support\n * negative value as the dim shape.\n * @param shapeA\n * @param shapeB\n * @param errorMessagePrefix\n */\nexport function assertShapesMatchAllowUndefinedSize(shapeA, shapeB, errorMessagePrefix = '') {\n    // constant shape means unknown rank\n    if (typeof shapeA === 'number' || typeof shapeB === 'number') {\n        return;\n    }\n    util.assert(shapeA.length === shapeB.length, () => errorMessagePrefix + ` Shapes ${shapeA} and ${shapeB} must match`);\n    for (let i = 0; i < shapeA.length; i++) {\n        const dim0 = shapeA[i];\n        const dim1 = shapeB[i];\n        util.assert(dim0 < 0 || dim1 < 0 || dim0 === dim1, () => errorMessagePrefix + ` Shapes ${shapeA} and ${shapeB} must match`);\n    }\n}\nexport function fullDefinedShape(elementShape) {\n    if (typeof elementShape === 'number' || elementShape.some(dim => dim < 0)) {\n        return false;\n    }\n    return true;\n}\n/**\n * Generate the output element shape from the list elementShape, list tensors\n * and input param.\n * @param listElementShape\n * @param tensors\n * @param elementShape\n */\nexport function inferElementShape(listElementShape, tensors, elementShape) {\n    let partialShape = mergeElementShape(listElementShape, elementShape);\n    const notfullDefinedShape = !fullDefinedShape(partialShape);\n    if (notfullDefinedShape && tensors.length === 0) {\n        throw new Error(`Tried to calculate elements of an empty list` +\n            ` with non-fully-defined elementShape: ${partialShape}`);\n    }\n    if (notfullDefinedShape) {\n        tensors.forEach(tensor => {\n            partialShape = mergeElementShape(tensor.shape, partialShape);\n        });\n    }\n    if (!fullDefinedShape(partialShape)) {\n        throw new Error(`Non-fully-defined elementShape: ${partialShape}`);\n    }\n    return partialShape;\n}\nexport function mergeElementShape(elementShapeA, elementShapeB) {\n    if (typeof elementShapeA === 'number') {\n        return elementShapeB;\n    }\n    if (typeof elementShapeB === 'number') {\n        return elementShapeA;\n    }\n    if (elementShapeA.length !== elementShapeB.length) {\n        throw new Error(`Incompatible ranks during merge: ${elementShapeA} vs. ${elementShapeB}`);\n    }\n    const result = [];\n    for (let i = 0; i < elementShapeA.length; ++i) {\n        const dim0 = elementShapeA[i];\n        const dim1 = elementShapeB[i];\n        if (dim0 >= 0 && dim1 >= 0 && dim0 !== dim1) {\n            throw new Error(`Incompatible shape during merge: ${elementShapeA} vs. ${elementShapeB}`);\n        }\n        result[i] = dim0 >= 0 ? dim0 : dim1;\n    }\n    return result;\n}\n//# sourceMappingURL=tensor_utils.js.map","/**\n * ExecutionContext captures the runtime environment of the node. It keeps\n * track of the current frame and iteration for the control flow ops.\n *\n * For example, typical Dynamic RNN model may contain loops, for which\n * TensorFlow will generate graphs with Enter/Exit nodes to control the\n * current execution frame, and NextIteration Nodes for iteration id increment.\n * For model with branch logic, TensorFLow will generate Switch/Merge ops.\n */\nexport class ExecutionContext {\n    constructor(weightMap = {}, tensorArrayMap = {}, tensorListMap = {}, functionMap = {}) {\n        this.weightMap = weightMap;\n        this.tensorArrayMap = tensorArrayMap;\n        this.tensorListMap = tensorListMap;\n        this.functionMap = functionMap;\n        this.rootContext = { id: 0, frameName: '', iterationId: 0 };\n        this.contexts = [this.rootContext];\n        this.lastId = 0;\n        this.generateCurrentContextIds();\n    }\n    newFrame(id, frameName) {\n        return { id, frameName, iterationId: 0 };\n    }\n    /**\n     * Set the current context\n     * @param contexts: ExecutionContextInfo[] the current path of execution\n     * frames\n     */\n    set currentContext(contexts) {\n        if (this.contexts !== contexts) {\n            this.contexts = contexts;\n            this.generateCurrentContextIds();\n        }\n    }\n    get currentContext() {\n        return this.contexts;\n    }\n    /**\n     * Returns the current context in string format.\n     */\n    get currentContextId() {\n        return this._currentContextIds[0];\n    }\n    /**\n     * Returns the current context and all parent contexts in string format.\n     * This allow access to the nodes in the current and parent frames.\n     */\n    get currentContextIds() {\n        return this._currentContextIds;\n    }\n    generateCurrentContextIds() {\n        const names = [];\n        for (let i = 0; i < this.contexts.length - 1; i++) {\n            const contexts = this.contexts.slice(0, this.contexts.length - i);\n            names.push(this.contextIdforContexts(contexts));\n        }\n        names.push('');\n        this._currentContextIds = names;\n    }\n    contextIdforContexts(contexts) {\n        return contexts ?\n            contexts\n                .map(context => (context.id === 0 && context.iterationId === 0) ?\n                '' :\n                `${context.frameName}-${context.iterationId}`)\n                .join('/') :\n            '';\n    }\n    /**\n     * Enter a new frame, a new context is pushed on the current context list.\n     * @param frameId new frame id\n     */\n    enterFrame(frameId) {\n        if (this.contexts) {\n            this.lastId++;\n            this.contexts = this.contexts.slice();\n            this.contexts.push(this.newFrame(this.lastId, frameId));\n            this._currentContextIds.unshift(this.contextIdforContexts(this.contexts));\n        }\n    }\n    /**\n     * Exit the current frame, the last context is removed from the current\n     * context list.\n     */\n    exitFrame() {\n        if (this.contexts && this.contexts.length > 1) {\n            this.contexts = this.contexts.slice();\n            this.contexts.splice(-1);\n            this.currentContextIds.shift();\n        }\n        else {\n            throw new Error('Cannot exit frame, the context is empty');\n        }\n    }\n    /**\n     * Enter the next iteration of a loop, the iteration id of last context is\n     * increased.\n     */\n    nextIteration() {\n        if (this.contexts && this.contexts.length > 0) {\n            this.contexts = this.contexts.slice();\n            this.lastId++;\n            const context = Object.assign({}, this.contexts[this.contexts.length - 1]);\n            context.iterationId += 1;\n            context.id = this.lastId;\n            this.contexts.splice(-1, 1, context);\n            this._currentContextIds.splice(0, 1, this.contextIdforContexts(this.contexts));\n        }\n        else {\n            throw new Error('Cannot increase frame iteration, the context is empty');\n        }\n    }\n    getWeight(name) {\n        return this.weightMap[name];\n    }\n    addTensorArray(tensorArray) {\n        this.tensorArrayMap[tensorArray.id] = tensorArray;\n    }\n    getTensorArray(id) {\n        return this.tensorArrayMap[id];\n    }\n    addTensorList(tensorList) {\n        this.tensorListMap[tensorList.id] = tensorList;\n    }\n    getTensorList(id) {\n        return this.tensorListMap[id];\n    }\n    dispose(keepIds) {\n        for (const key in this.tensorArrayMap) {\n            this.tensorArrayMap[key].clearAndClose(keepIds);\n        }\n        for (const key in this.tensorListMap) {\n            this.tensorListMap[key].clearAndClose(keepIds);\n        }\n    }\n}\n//# sourceMappingURL=execution_context.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { parseNodeName } from '../operations/executors/utils';\n/**\n * Given graph inputs and desired outputs, find the minimal set of nodes\n * to execute in order to compute the outputs. In addition return other useful\n * info such:\n * - Missing inputs needed to compute the output.\n * - Whether the subgraph contains dynamic ops (control flow, dynamic shape).\n * - Alternative inputs in order to avoid async (dynamic op) execution.\n */\nexport function getExecutionSubgraph(inputs, outputs, weightMap, initNodes) {\n    const usedNodes = new Set();\n    const missingInputs = [];\n    let dynamicNode = null;\n    let syncInputs = null;\n    // Start with the outputs, going backwards and find all the nodes that are\n    // needed to compute those outputs.\n    const seen = new Set();\n    const inputNodeNames = Object.keys(inputs).map(name => parseNodeName(name)[0]);\n    let initNodeNames = [];\n    if (initNodes != null) {\n        initNodeNames = initNodes.map(node => parseNodeName(node.name)[0]);\n    }\n    const frontier = [...outputs];\n    while (frontier.length > 0) {\n        const node = frontier.pop();\n        if (isControlFlow(node) || isDynamicShape(node) || isHashTable(node)) {\n            if (dynamicNode == null) {\n                dynamicNode = node;\n                syncInputs = dynamicNode.children.map(child => child.name)\n                    .filter(name => usedNodes.has(name));\n            }\n        }\n        usedNodes.add(node.name);\n        // Weights are dead end since we already have their values.\n        if (weightMap[node.name] != null) {\n            continue;\n        }\n        // This node is a dead end since it's one of the user-provided inputs.\n        if (inputNodeNames.indexOf(node.name) !== -1) {\n            continue;\n        }\n        // This node is a dead end since it doesn't have any inputs.\n        if (initNodeNames.indexOf(node.name) !== -1) {\n            continue;\n        }\n        if (node.inputs.length === 0) {\n            missingInputs.push(node.name);\n            continue;\n        }\n        node.inputs.forEach(input => {\n            // Don't add to the frontier if it is already there.\n            if (seen.has(input.name)) {\n                return;\n            }\n            seen.add(input.name);\n            frontier.push(input);\n        });\n    }\n    return { inputs, outputs, usedNodes, missingInputs, dynamicNode, syncInputs };\n}\n/**\n * Given the execution info, return a list of nodes in topological order that\n * need to be executed to compute the output.\n */\nexport function getNodesInTopologicalOrder(graph, weightMap, executionInfo) {\n    const { usedNodes, inputs } = executionInfo;\n    const frontier = [];\n    const inputNodes = Object.keys(inputs)\n        .map(name => parseNodeName(name)[0])\n        .map(name => graph.nodes[name]);\n    const initNodes = graph.initNodes;\n    inputNodes.forEach(input => {\n        if (usedNodes.has(input.name)) {\n            frontier.push(input);\n        }\n    });\n    graph.weights.forEach(weight => {\n        if (usedNodes.has(weight.name)) {\n            frontier.push(weight);\n        }\n    });\n    if (initNodes != null) {\n        initNodes.forEach(node => {\n            if (usedNodes.has(node.name)) {\n                frontier.push(node);\n            }\n        });\n    }\n    const seen = new Set();\n    const orderedNodes = [];\n    while (frontier.length > 0) {\n        const node = frontier.pop();\n        seen.add(node.name);\n        if (!weightMap[node.name]) {\n            orderedNodes.push(node);\n        }\n        node.children.forEach(child => {\n            if (!seen.has(child.name) && usedNodes.has(child.name) &&\n                child.inputs.every(input => seen.has(input.name))) {\n                frontier.push(child);\n            }\n        });\n    }\n    return orderedNodes;\n}\nconst CONTROL_FLOW_OPS = [\n    'Switch', 'Merge', 'Enter', 'Exit', 'NextIteration', 'StatelessIf',\n    'StatelessWhile', 'if', 'While'\n];\nconst DYNAMIC_SHAPE_OPS = [\n    'NonMaxSuppressionV2', 'NonMaxSuppressionV3', 'NonMaxSuppressionV5', 'Where'\n];\nconst HASH_TABLE_OPS = [\n    'HashTable', 'HashTableV2', 'LookupTableImport', 'LookupTableImportV2',\n    'LookupTableFind', 'LookupTableFindV2', 'LookupTableSize', 'LookupTableSizeV2'\n];\nexport function isControlFlow(node) {\n    return CONTROL_FLOW_OPS.indexOf(node.op) >= 0;\n}\nexport function isDynamicShape(node) {\n    return DYNAMIC_SHAPE_OPS.indexOf(node.op) >= 0;\n}\nexport function isHashTable(node) {\n    return HASH_TABLE_OPS.indexOf(node.op) >= 0;\n}\n//# sourceMappingURL=model_analysis.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { tidy, util } from '@tensorflow/tfjs-core';\nimport { getNodeNameAndIndex, getParamValue, getTensor, getTensorsForCurrentContenxt, parseNodeName } from '../operations/executors/utils';\nimport { executeOp } from '../operations/operation_executor';\nimport { ExecutionContext } from './execution_context';\nimport { getExecutionSubgraph, getNodesInTopologicalOrder, isControlFlow } from './model_analysis';\nexport class GraphExecutor {\n    /**\n     *\n     * @param graph Graph the model or function graph to be executed.\n     * @param parent When building function exector you need to set the parent\n     * executor. Since the weights and function executor maps are set at parant\n     * level, that function executor can access the function maps and weight maps\n     * through the parent.\n     */\n    constructor(graph, parent) {\n        this.graph = graph;\n        this.parent = parent;\n        this.compiledMap = new Map();\n        this._weightMap = {};\n        this.SEPERATOR = ',';\n        this._functions = {};\n        this._functionExecutorMap = {};\n        this._outputs = graph.outputs;\n        this._inputs = graph.inputs;\n        this._initNodes = graph.initNodes;\n        this._signature = graph.signature;\n        this._functions = graph.functions;\n        // create sub-graph executors\n        if (graph.functions != null) {\n            Object.keys(graph.functions).forEach(name => {\n                this._functionExecutorMap[name] =\n                    new GraphExecutor(graph.functions[name], this);\n            });\n        }\n    }\n    get weightIds() {\n        return this.parent ? this.parent.weightIds : this._weightIds;\n    }\n    get functionExecutorMap() {\n        return this.parent ? this.parent.functionExecutorMap :\n            this._functionExecutorMap;\n    }\n    get weightMap() {\n        return this.parent ? this.parent.weightMap : this._weightMap;\n    }\n    set weightMap(weightMap) {\n        const weightIds = Object.keys(weightMap).map(key => weightMap[key].map(tensor => tensor.id));\n        this._weightIds = [].concat(...weightIds);\n        this._weightMap = weightMap;\n    }\n    /**\n     * Set `ResourceManager` shared by executors of a model.\n     * @param resourceManager: `ResourceManager` of the `GraphModel`.\n     */\n    set resourceManager(resourceManager) {\n        this._resourceManager = resourceManager;\n    }\n    get inputs() {\n        return this._inputs.map(node => {\n            return {\n                name: node.name,\n                shape: node.attrParams['shape'] ?\n                    node.attrParams['shape'].value :\n                    undefined,\n                dtype: node.attrParams['dtype'] ?\n                    node.attrParams['dtype'].value :\n                    undefined\n            };\n        });\n    }\n    get outputs() {\n        return this._outputs.map(node => {\n            return {\n                name: node.name,\n                shape: node.attrParams['shape'] ?\n                    node.attrParams['shape'].value :\n                    undefined,\n                dtype: node.attrParams['dtype'] ?\n                    node.attrParams['dtype'].value :\n                    undefined\n            };\n        });\n    }\n    get inputNodes() {\n        return this._inputs.map(node => node.signatureKey || node.name);\n    }\n    get outputNodes() {\n        return this._outputs.map((node) => {\n            const name = node.signatureKey || node.name;\n            return node.defaultOutput ? (`${name}:${node.defaultOutput}`) : name;\n        });\n    }\n    get functions() {\n        return Object.keys(this._functions).reduce((map, key) => {\n            map[key] = this._functions[key].signature;\n            return map;\n        }, {});\n    }\n    getCompilationKey(inputs, outputs) {\n        const sortedInputs = inputs.map(node => node.name).sort();\n        const sortedOutputs = outputs.map(node => node.name).sort();\n        return sortedInputs.join(this.SEPERATOR) + '--' +\n            sortedOutputs.join(this.SEPERATOR);\n    }\n    /**\n     * Compiles the inference graph and returns the minimal set of nodes that are\n     * required for execution, in the correct execution order.\n     */\n    compile(inputs, outputs) {\n        const executionInfo = getExecutionSubgraph(inputs, outputs, this.weightMap, this._initNodes);\n        const { missingInputs, dynamicNode, syncInputs } = executionInfo;\n        if (dynamicNode != null) {\n            throw new Error(`This execution contains the node '${dynamicNode.name}', which has ` +\n                `the dynamic op '${dynamicNode.op}'. Please use ` +\n                `model.executeAsync() instead. Alternatively, to avoid the ` +\n                `dynamic ops, specify the inputs [${syncInputs}]`);\n        }\n        if (missingInputs.length > 0) {\n            const outNames = outputs.map(n => n.name);\n            const inNames = Object.keys(inputs);\n            throw new Error(`Cannot compute the outputs [${outNames}] from the provided inputs ` +\n                `[${inNames}]. Missing the following inputs: [${missingInputs}]`);\n        }\n        return getNodesInTopologicalOrder(this.graph, this.weightMap, executionInfo);\n    }\n    /**\n     * Executes the inference for given input tensors.\n     * @param inputs Tensor map for the model inputs, keyed by the input node\n     * names.\n     * @param outputs Optional. output node name from the Tensorflow model, if\n     * no outputs are specified, the default outputs of the model would be used.\n     * You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     */\n    execute(inputs, outputs) {\n        inputs = this.mapInputs(inputs);\n        const names = Object.keys(inputs).sort();\n        this.checkInputs(inputs);\n        this.checkInputShapeAndType(inputs);\n        outputs = this.mapOutputs(outputs);\n        this.checkOutputs(outputs);\n        const inputNodes = names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n        const outputNodeNames = outputs.map(name => parseNodeName(name)[0]);\n        let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n        // If no outputs are specified, then use the default outputs of the model.\n        if (outputNodes.length === 0) {\n            outputNodes = this._outputs;\n        }\n        const compilationKey = this.getCompilationKey(inputNodes, outputNodes);\n        // Do nothing if the compiled graph cache contains the input.\n        let orderedNodes = this.compiledMap.get(compilationKey);\n        if (orderedNodes == null) {\n            orderedNodes = this.compile(inputs, outputNodes);\n            this.compiledMap.set(compilationKey, orderedNodes);\n        }\n        const tensorArrayMap = {};\n        const tensorListMap = {};\n        return tidy(() => {\n            const context = new ExecutionContext(this.weightMap, tensorArrayMap, tensorListMap, this.functionExecutorMap);\n            const tensorsMap = Object.assign({}, this.weightMap);\n            Object.keys(inputs).forEach(name => {\n                const [nodeName, index] = parseNodeName(name);\n                const tensors = [];\n                tensors[index] = inputs[name];\n                tensorsMap[nodeName] = tensors;\n            });\n            const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n            const intermediateTensorConsumerCount = {};\n            for (let i = 0; i < orderedNodes.length; i++) {\n                const node = orderedNodes[i];\n                if (!tensorsMap[node.name]) {\n                    const tensors = executeOp(node, tensorsMap, context, this._resourceManager);\n                    if (util.isPromise(tensors)) {\n                        throw new Error(`The execution of the op '${node.op}' returned a promise. ` +\n                            `Please use model.executeAsync() instead.`);\n                    }\n                    tensorsMap[node.name] = tensors;\n                    this.checkTensorForDisposal(node.name, node, tensorsMap, context, tensorsToKeep, outputNodeNames, intermediateTensorConsumerCount);\n                }\n            }\n            // dispose the context for the root executor\n            if (this.parent == null) {\n                context.dispose(tensorsToKeep);\n            }\n            return outputs.map(name => getTensor(name, tensorsMap, context));\n        });\n    }\n    getFrozenTensorIds(tensorMap) {\n        const ids = [].concat.apply([], Object.keys(tensorMap)\n            .map(key => tensorMap[key])\n            .map(tensors => tensors.map(tensor => tensor.id)));\n        return new Set(ids);\n    }\n    checkTensorForDisposal(nodeName, node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount) {\n        // Skip output nodes and any control flow nodes, since its dependency is\n        // tricky to track correctly.\n        if (node.category === 'control' || outputNames.indexOf(nodeName) !== -1) {\n            return;\n        }\n        tensorMap[nodeName].forEach(tensor => {\n            if (tensor != null) {\n                intermediateTensorConsumerCount[tensor.id] =\n                    (intermediateTensorConsumerCount[tensor.id] || 0) +\n                        node.children.length;\n            }\n        });\n        node.inputs.forEach(input => {\n            // Skip any control flow nodes, since its dependency is tricky to track\n            // correctly.\n            if (input.category !== 'control') {\n                const tensors = getTensorsForCurrentContenxt(input.name, tensorMap, context);\n                if (tensors != null) {\n                    tensors.forEach(tensor => {\n                        if (tensor && !tensor.kept && !tensorsToKeep.has(tensor.id)) {\n                            const count = intermediateTensorConsumerCount[tensor.id];\n                            if (count === 1) {\n                                tensor.dispose();\n                                delete intermediateTensorConsumerCount[tensor.id];\n                            }\n                            else if (count != null) {\n                                // only intermediate nodes has count set, inputs and weights are\n                                // not.\n                                intermediateTensorConsumerCount[tensor.id]--;\n                            }\n                        }\n                    });\n                }\n            }\n        });\n    }\n    /**\n     * Executes the inference for given input tensors in Async fashion.\n     * @param inputs Tensor map for the model inputs, keyed by the input node\n     * names.\n     * @param outputs output node name from the Tensorflow model, if no outputs\n     * are specified, the default outputs of the model would be used. You can\n     * inspect intermediate nodes of the model by adding them to the outputs\n     * array.\n     */\n    async executeAsync(inputs, outputs) {\n        return this._executeAsync(inputs, outputs);\n    }\n    /**\n     * Executes the inference for given input tensors in Async fashion.\n     * @param inputs Tensor map for the model inputs, keyed by the input node\n     * names.\n     * @param outputs Optional. output node name from the Tensorflow model,\n     * if no outputs are specified, the default outputs of the model would be\n     * used. You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     * @param isFunctionExecution Optional. Flag for executing a function.\n     * @param tensorArrayMap Optional, global TensorArray map by id. Used for\n     * function execution.\n     * @param tensorArrayMap Optinal global TensorList map by id. Used for\n     * function execution.\n     */\n    async _executeAsync(inputs, outputs, isFunctionExecution = false, tensorArrayMap = {}, tensorListMap = {}) {\n        if (!isFunctionExecution) {\n            inputs = this.mapInputs(inputs);\n            this.checkInputs(inputs);\n            this.checkInputShapeAndType(inputs);\n            outputs = this.mapOutputs(outputs);\n            this.checkOutputs(outputs);\n        }\n        const context = new ExecutionContext(this.weightMap, tensorArrayMap, tensorListMap, this.functionExecutorMap);\n        // Graph with control flow op requires runtime evaluation of the execution\n        // order, while without control flow the execution order is pre-determined\n        // in the compile method.\n        const tensorMap = await this.executeWithControlFlow(inputs, context, outputs, isFunctionExecution);\n        const results = outputs.map(name => getTensor(name, tensorMap, context));\n        // dispose all the intermediate tensors\n        const outputIds = results.map(t => t.id);\n        const inputIds = Object.keys(inputs).map(name => inputs[name].id);\n        const keepIds = new Set([...outputIds, ...inputIds, ...this.weightIds]);\n        Object.keys(tensorMap).forEach(key => {\n            const tensorArray = tensorMap[key];\n            tensorArray.forEach(tensor => {\n                if (tensor && !tensor.kept && !tensor.isDisposed &&\n                    !keepIds.has(tensor.id)) {\n                    tensor.dispose();\n                }\n            });\n        });\n        // dispose the context for the root executor\n        if (this.parent == null) {\n            context.dispose(keepIds);\n        }\n        return results;\n    }\n    async executeFunctionAsync(inputs, tensorArrayMap, tensorListMap) {\n        const mappedInputs = inputs.reduce((map, tensor, index) => {\n            map[this.inputs[index].name] = tensor;\n            return map;\n        }, {});\n        return this._executeAsync(mappedInputs, this.outputNodes, true, tensorArrayMap, tensorListMap);\n    }\n    /**\n     * When there are control flow nodes in the graph, the graph execution use\n     * ExecutionContext to keep track of the frames and loop iterators.\n     * @param inputs placeholder tensors for the graph.\n     * @param context the execution context object for current execution.\n     * @param outputNames Optional. output node name from the Tensorflow model,\n     * if no outputs are specified, the default outputs of the model would be\n     * used. You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     * @param isFunctionExecution Flag for executing a function.\n     */\n    async executeWithControlFlow(inputs, context, outputNames, isFunctionExecution) {\n        const names = Object.keys(inputs);\n        const inputNodes = names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n        const outputNodeNames = outputNames.map(name => parseNodeName(name)[0]);\n        let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n        // If no outputs are specified, then use the default outputs of the model.\n        if (outputNodes.length === 0) {\n            outputNodes = this._outputs;\n        }\n        const { usedNodes, missingInputs, dynamicNode, syncInputs } = getExecutionSubgraph(inputs, outputNodes, this.weightMap, this._initNodes);\n        // First nodes to execute include inputNodes, weights, and initNodes.\n        const stack = [\n            ...inputNodes, ...this.graph.weights, ...(this._initNodes || [])\n        ].map(node => {\n            return { node, contexts: context.currentContext };\n        });\n        const tensorsMap = Object.assign({}, this.weightMap);\n        Object.keys(inputs).forEach(name => {\n            const [nodeName, index] = parseNodeName(name);\n            const tensors = [];\n            tensors[index] = inputs[name];\n            tensorsMap[nodeName] = tensors;\n        });\n        const intermediateTensorConsumerCount = {};\n        const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n        const added = {};\n        while (stack.length > 0) {\n            const promises = this.processStack(inputNodes, stack, context, tensorsMap, added, tensorsToKeep, outputNodeNames, intermediateTensorConsumerCount, usedNodes);\n            await Promise.all(promises);\n        }\n        if (dynamicNode == null && !isFunctionExecution) {\n            console.warn(`This model execution did not contain any nodes with control flow ` +\n                `or dynamic output shapes. You can use model.execute() instead.`);\n        }\n        const missingOutputs = outputNodes\n            .filter(node => !isControlFlow(node) &&\n            !getTensor(node.name, tensorsMap, context))\n            .map(node => node.name);\n        if (missingOutputs.length > 0) {\n            let alternativeMsg = '';\n            if (dynamicNode != null) {\n                alternativeMsg =\n                    `Alternatively, to avoid the dynamic ops, use model.execute() ` +\n                        `and specify the inputs [${syncInputs}]`;\n            }\n            throw new Error(`Cannot compute the outputs [${missingOutputs}] from the provided ` +\n                `inputs [${names}]. Consider providing the following inputs: ` +\n                `[${missingInputs}]. ${alternativeMsg}`);\n        }\n        return tensorsMap;\n    }\n    processStack(inputNodes, stack, context, tensorMap, added, tensorsToKeep, outputNames, intermediateTensorConsumerCount, usedNodes) {\n        const promises = [];\n        while (stack.length > 0) {\n            const item = stack.pop();\n            context.currentContext = item.contexts;\n            let nodeName = '';\n            // The tensor of the Enter op with isConstant set should be set\n            // in the parent scope, so it will be available as constant for the\n            // whole loop.\n            if (item.node.op === 'Enter' &&\n                getParamValue('isConstant', item.node, tensorMap, context)) {\n                [nodeName] = getNodeNameAndIndex(item.node.name, context);\n            }\n            // only process nodes that are not in the tensorMap yet, this include\n            // inputNodes and internal initNodes.\n            if (tensorMap[item.node.name] == null) {\n                const tensors = executeOp(item.node, tensorMap, context, this._resourceManager);\n                if (!nodeName) {\n                    [nodeName] = getNodeNameAndIndex(item.node.name, context);\n                }\n                const currentContext = context.currentContext;\n                if (util.isPromise(tensors)) {\n                    promises.push(tensors.then(t => {\n                        tensorMap[nodeName] = t;\n                        context.currentContext = currentContext;\n                        this.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount);\n                        this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n                        return t;\n                    }));\n                }\n                else {\n                    tensorMap[nodeName] = tensors;\n                    this.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount);\n                    this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n                }\n            }\n            else {\n                this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n            }\n        }\n        return promises;\n    }\n    processChildNodes(node, stack, context, tensorMap, added, usedNodes) {\n        node.children.forEach((childNode) => {\n            const [nodeName,] = getNodeNameAndIndex(childNode.name, context);\n            if (added[nodeName] || !usedNodes.has(childNode.name)) {\n                return;\n            }\n            // Merge op can be pushed if any of its inputs has value.\n            if (childNode.op === 'Merge') {\n                if (childNode.inputNames.some(name => {\n                    return !!getTensor(name, tensorMap, context);\n                })) {\n                    added[nodeName] = true;\n                    stack.push({ contexts: context.currentContext, node: childNode });\n                }\n            }\n            else // Otherwise all inputs must to have value.\n             if (childNode.inputNames.every(name => {\n                return !!getTensor(name, tensorMap, context);\n            })) {\n                added[nodeName] = true;\n                stack.push({ contexts: context.currentContext, node: childNode });\n            }\n        });\n    }\n    /**\n     * Releases the memory used by the weight tensors.\n     */\n    dispose() {\n        Object.keys(this.weightMap)\n            .forEach(key => this.weightMap[key].forEach(tensor => tensor.dispose()));\n    }\n    checkInputShapeAndType(inputs) {\n        Object.keys(inputs).forEach(name => {\n            const input = inputs[name];\n            const [nodeName,] = parseNodeName(name);\n            const node = this.graph.nodes[nodeName];\n            if (node.attrParams['shape'] && node.attrParams['shape'].value) {\n                const shape = node.attrParams['shape'].value;\n                const match = shape.length === input.shape.length &&\n                    input.shape.every((dim, index) => shape[index] === -1 || shape[index] === dim);\n                util.assert(match, () => `The shape of dict['${node.name}'] provided in ` +\n                    `model.execute(dict) must be [${shape}], but was ` +\n                    `[${input.shape}]`);\n            }\n            if (node.attrParams['dtype'] && node.attrParams['dtype'].value) {\n                util.assert(input.dtype === node.attrParams['dtype'].value, () => `The dtype of dict['${node.name}'] provided in ` +\n                    `model.execute(dict) must be ` +\n                    `${node.attrParams['dtype'].value}, but was ${input.dtype}`);\n            }\n        });\n    }\n    mapInputs(inputs) {\n        const result = {};\n        for (const inputName in inputs) {\n            if (this._signature != null && this._signature.inputs != null &&\n                this._signature.inputs[inputName] != null) {\n                const tensor = this._signature.inputs[inputName];\n                result[tensor.name] = inputs[inputName];\n            }\n            else {\n                result[inputName] = inputs[inputName];\n            }\n        }\n        return result;\n    }\n    checkInputs(inputs) {\n        const notInGraph = Object.keys(inputs).filter(name => {\n            const [nodeName] = parseNodeName(name);\n            return this.graph.nodes[nodeName] == null;\n        });\n        if (notInGraph.length > 0) {\n            throw new Error(`The dict provided in model.execute(dict) has ` +\n                `keys: [${notInGraph}] that are not part of graph`);\n        }\n    }\n    mapOutputs(outputs) {\n        return outputs.map(name => {\n            if (this._signature != null && this._signature.outputs != null &&\n                this._signature.outputs[name] != null) {\n                const tensor = this._signature.outputs[name];\n                return tensor.name;\n            }\n            return name;\n        }, {});\n    }\n    checkOutputs(outputs) {\n        outputs.forEach(name => {\n            const [normalizedName] = parseNodeName(name);\n            if (!this.graph.nodes[normalizedName]) {\n                throw new Error(`The output '${name}' is not found in the graph`);\n            }\n        });\n    }\n}\n//# sourceMappingURL=graph_executor.js.map","/**\n * Contains global resources of a model.\n */\nexport class ResourceManager {\n    constructor(hashTableNameToHandle = {}, hashTableMap = {}) {\n        this.hashTableNameToHandle = hashTableNameToHandle;\n        this.hashTableMap = hashTableMap;\n    }\n    /**\n     * Register a `HashTable` in the resource manager.\n     *\n     * The `HashTable` can be retrieved by `resourceManager.getHashTableById`,\n     * where id is the table handle tensor's id.\n     *\n     * @param name Op node name that creates the `HashTable`.\n     * @param hashTable The `HashTable` to be added to resource manager.\n     */\n    addHashTable(name, hashTable) {\n        this.hashTableNameToHandle[name] = hashTable.handle;\n        this.hashTableMap[hashTable.id] = hashTable;\n    }\n    /**\n     * Get the table handle by node name.\n     * @param name Op node name that creates the `HashTable`. This name is also\n     *     used in the inputs list of lookup and import `HashTable` ops.\n     */\n    getHashTableHandleByName(name) {\n        return this.hashTableNameToHandle[name];\n    }\n    /**\n     * Get the actual `HashTable` by its handle tensor's id.\n     * @param id The id of the handle tensor.\n     */\n    getHashTableById(id) {\n        return this.hashTableMap[id];\n    }\n    /**\n     * Dispose `ResourceManager`, including its hashTables and tensors in them.\n     */\n    dispose() {\n        for (const key in this.hashTableMap) {\n            this.hashTableMap[key].clearAndClose();\n            delete this.hashTableMap[key];\n        }\n        for (const name in this.hashTableNameToHandle) {\n            this.hashTableNameToHandle[name].dispose();\n            delete this.hashTableNameToHandle[name];\n        }\n    }\n}\n//# sourceMappingURL=resource_manager.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { io, Tensor } from '@tensorflow/tfjs-core';\nimport { OperationMapper } from '../operations/operation_mapper';\nimport { GraphExecutor } from './graph_executor';\nimport { ResourceManager } from './resource_manager';\nexport const TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport const DEFAULT_MODEL_NAME = 'model.json';\n/**\n * A `tf.GraphModel` is a directed, acyclic graph built from a\n * SavedModel GraphDef and allows inference execution.\n *\n * A `tf.GraphModel` can only be created by loading from a model converted from\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\n * the command line converter tool and loaded via `tf.loadGraphModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport class GraphModel {\n    /**\n     * @param modelUrl url for the model, or an `io.IOHandler`.\n     * @param weightManifestUrl url for the weight file generated by\n     * scripts/convert.py script.\n     * @param requestOption options for Request, which allows to send credentials\n     * and custom headers.\n     * @param onProgress Optional, progress callback function, fired periodically\n     * before the load is completed.\n     */\n    constructor(modelUrl, loadOptions = {}) {\n        this.modelUrl = modelUrl;\n        this.loadOptions = loadOptions;\n        this.version = 'n/a';\n        if (loadOptions == null) {\n            this.loadOptions = {};\n        }\n        this.resourceManager = new ResourceManager();\n    }\n    // Returns the version information for the tensorflow model GraphDef.\n    get modelVersion() {\n        return this.version;\n    }\n    get inputNodes() {\n        return this.executor.inputNodes;\n    }\n    get outputNodes() {\n        return this.executor.outputNodes;\n    }\n    get inputs() {\n        return this.executor.inputs;\n    }\n    get outputs() {\n        return this.executor.outputs;\n    }\n    get weights() {\n        return this.executor.weightMap;\n    }\n    get metadata() {\n        return this.artifacts.userDefinedMetadata;\n    }\n    get modelSignature() {\n        return this.signature;\n    }\n    findIOHandler() {\n        const path = this.modelUrl;\n        if (path.load != null) {\n            // Path is an IO Handler.\n            this.handler = path;\n        }\n        else if (this.loadOptions.requestInit != null) {\n            this.handler = io.browserHTTPRequest(path, this.loadOptions);\n        }\n        else {\n            const handlers = io.getLoadHandlers(path, this.loadOptions);\n            if (handlers.length === 0) {\n                // For backward compatibility: if no load handler can be found,\n                // assume it is a relative http path.\n                handlers.push(io.browserHTTPRequest(path, this.loadOptions));\n            }\n            else if (handlers.length > 1) {\n                throw new Error(`Found more than one (${handlers.length}) load handlers for ` +\n                    `URL '${[path]}'`);\n            }\n            this.handler = handlers[0];\n        }\n    }\n    /**\n     * Loads the model and weight files, construct the in memory weight map and\n     * compile the inference graph.\n     */\n    async load() {\n        this.findIOHandler();\n        if (this.handler.load == null) {\n            throw new Error('Cannot proceed with model loading because the IOHandler provided ' +\n                'does not have the `load` method implemented.');\n        }\n        const artifacts = await this.handler.load();\n        return this.loadSync(artifacts);\n    }\n    /**\n     * Synchronously construct the in memory weight map and\n     * compile the inference graph. Also initialize hashtable if any.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n     */\n    loadSync(artifacts) {\n        this.artifacts = artifacts;\n        const graph = this.artifacts.modelTopology;\n        let signature;\n        if (this.artifacts.userDefinedMetadata != null &&\n            this.artifacts.userDefinedMetadata.signature != null) {\n            signature = // tslint:disable-next-line:no-any\n                this.artifacts.userDefinedMetadata.signature;\n        }\n        else {\n            signature = this.artifacts.signature;\n        }\n        this.signature = signature;\n        this.version = `${graph.versions.producer}.${graph.versions.minConsumer}`;\n        const weightMap = io.decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\n        this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(graph, this.signature));\n        this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap);\n        // Attach a model-level resourceManager to each executor to share resources,\n        // such as `HashTable`.\n        this.executor.resourceManager = this.resourceManager;\n        if (artifacts.modelInitializer != null &&\n            artifacts.modelInitializer.node != null) {\n            const initializer = OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n            this.initializer = new GraphExecutor(initializer);\n            this.initializer.weightMap = this.executor.weightMap;\n            // Attach a model-level resourceManager to the initializer, the\n            // hashTables created from when executing the initializer will be stored\n            // in the resourceManager.\n            this.initializer.resourceManager = this.resourceManager;\n            this.initializer.executeAsync({}, []);\n        }\n        return true;\n    }\n    /**\n     * Save the configuration and/or weights of the GraphModel.\n     *\n     * An `IOHandler` is an object that has a `save` method of the proper\n     * signature defined. The `save` method manages the storing or\n     * transmission of serialized data (\"artifacts\") that represent the\n     * model's topology and weights onto or via a specific medium, such as\n     * file downloads, local storage, IndexedDB in the web browser and HTTP\n     * requests to a server. TensorFlow.js provides `IOHandler`\n     * implementations for a number of frequently used saving mediums, such as\n     * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n     * for more details.\n     *\n     * This method also allows you to refer to certain types of `IOHandler`s\n     * as URL-like string shortcuts, such as 'localstorage://' and\n     * 'indexeddb://'.\n     *\n     * Example 1: Save `model`'s topology and weights to browser [local\n     * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n     * then load it back.\n     *\n     * ```js\n     * const modelUrl =\n     *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n     * const model = await tf.loadGraphModel(modelUrl);\n     * const zeros = tf.zeros([1, 224, 224, 3]);\n     * model.predict(zeros).print();\n     *\n     * const saveResults = await model.save('localstorage://my-model-1');\n     *\n     * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\n     * console.log('Prediction from loaded model:');\n     * model.predict(zeros).print();\n     * ```\n     *\n     * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n     * scheme-based string shortcut for `IOHandler`.\n     * @param config Options for saving the model.\n     * @returns A `Promise` of `SaveResult`, which summarizes the result of\n     * the saving, such as byte sizes of the saved artifacts for the model's\n     *   topology and weight values.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n     */\n    async save(handlerOrURL, config) {\n        if (typeof handlerOrURL === 'string') {\n            const handlers = io.getSaveHandlers(handlerOrURL);\n            if (handlers.length === 0) {\n                throw new Error(`Cannot find any save handlers for URL '${handlerOrURL}'`);\n            }\n            else if (handlers.length > 1) {\n                throw new Error(`Found more than one (${handlers.length}) save handlers for ` +\n                    `URL '${handlerOrURL}'`);\n            }\n            handlerOrURL = handlers[0];\n        }\n        if (handlerOrURL.save == null) {\n            throw new Error('GraphModel.save() cannot proceed because the IOHandler ' +\n                'provided does not have the `save` attribute defined.');\n        }\n        return handlerOrURL.save(this.artifacts);\n    }\n    /**\n     * Execute the inference for the input tensors.\n     *\n     * @param input The input tensors, when there is single input for the model,\n     * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\n     * inputs params should be in either `tf.Tensor`[] if the input order is\n     * fixed, or otherwise NamedTensorMap format.\n     *\n     * For model with multiple inputs, we recommend you use NamedTensorMap as the\n     * input type, if you use `tf.Tensor`[], the order of the array needs to\n     * follow the\n     * order of inputNodes array. @see {@link GraphModel.inputNodes}\n     *\n     * You can also feed any intermediate nodes using the NamedTensorMap as the\n     * input type. For example, given the graph\n     *    InputNode => Intermediate => OutputNode,\n     * you can execute the subgraph Intermediate => OutputNode by calling\n     *    model.execute('IntermediateNode' : tf.tensor(...));\n     *\n     * This is useful for models that uses tf.dynamic_rnn, where the intermediate\n     * state needs to be fed manually.\n     *\n     * For batch inference execution, the tensors for each input need to be\n     * concatenated together. For example with mobilenet, the required input shape\n     * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\n     * If we are provide a batched data of 100 images, the input tensor should be\n     * in the shape of [100, 244, 244, 3].\n     *\n     * @param config Prediction configuration for specifying the batch size and\n     * output node names. Currently the batch size option is ignored for graph\n     * model.\n     *\n     * @returns Inference result tensors. The output would be single `tf.Tensor`\n     * if model has single output node, otherwise Tensor[] or NamedTensorMap[]\n     * will be returned for model with multiple outputs.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    predict(inputs, config) {\n        return this.execute(inputs, this.outputNodes);\n    }\n    normalizeInputs(inputs) {\n        if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n            // The input is already a NamedTensorMap.\n            return inputs;\n        }\n        inputs = Array.isArray(inputs) ? inputs : [inputs];\n        if (inputs.length !== this.inputNodes.length) {\n            throw new Error('Input tensor count mismatch,' +\n                `the graph model has ${this.inputNodes.length} placeholders, ` +\n                `while there are ${inputs.length} input tensors.`);\n        }\n        return this.inputNodes.reduce((map, inputName, i) => {\n            map[inputName] = inputs[i];\n            return map;\n        }, {});\n    }\n    normalizeOutputs(outputs) {\n        outputs = outputs || this.outputNodes;\n        return !Array.isArray(outputs) ? [outputs] : outputs;\n    }\n    /**\n     * Executes inference for the model for given input tensors.\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\n     * model, keyed by the input node names.\n     * @param outputs output node name from the Tensorflow model, if no\n     * outputs are specified, the default outputs of the model would be used.\n     * You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     *\n     * @returns A single tensor if provided with a single output or no outputs\n     * are provided and there is only one default output, otherwise return a\n     * tensor array. The order of the tensor array is the same as the outputs\n     * if provided, otherwise the order of outputNodes attribute of the model.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    execute(inputs, outputs) {\n        inputs = this.normalizeInputs(inputs);\n        outputs = this.normalizeOutputs(outputs);\n        const result = this.executor.execute(inputs, outputs);\n        return result.length > 1 ? result : result[0];\n    }\n    /**\n     * Executes inference for the model for given input tensors in async\n     * fashion, use this method when your model contains control flow ops.\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\n     * model, keyed by the input node names.\n     * @param outputs output node name from the Tensorflow model, if no outputs\n     * are specified, the default outputs of the model would be used. You can\n     * inspect intermediate nodes of the model by adding them to the outputs\n     * array.\n     *\n     * @returns A Promise of single tensor if provided with a single output or\n     * no outputs are provided and there is only one default output, otherwise\n     * return a tensor map.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    async executeAsync(inputs, outputs) {\n        inputs = this.normalizeInputs(inputs);\n        outputs = this.normalizeOutputs(outputs);\n        const result = await this.executor.executeAsync(inputs, outputs);\n        return result.length > 1 ? result : result[0];\n    }\n    convertTensorMapToTensorsMap(map) {\n        return Object.keys(map).reduce((newMap, key) => {\n            newMap[key] = [map[key]];\n            return newMap;\n        }, {});\n    }\n    /**\n     * Releases the memory used by the weight tensors and resourceManager.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    dispose() {\n        this.executor.dispose();\n        if (this.initializer) {\n            this.initializer.dispose();\n        }\n        this.resourceManager.dispose();\n    }\n}\n/**\n * Load a graph model given a URL to the model definition.\n *\n * Example of loading MobileNetV2 from a URL and making a prediction with a\n * zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n * const model = await tf.loadGraphModel(modelUrl);\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n *\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction with\n * a zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\n * @param options Options for the HTTP request, which allows to send credentials\n *    and custom headers.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\nexport async function loadGraphModel(modelUrl, options = {}) {\n    if (modelUrl == null) {\n        throw new Error('modelUrl in loadGraphModel() cannot be null. Please provide a url ' +\n            'or an IOHandler that loads the model');\n    }\n    if (options == null) {\n        options = {};\n    }\n    if (options.fromTFHub) {\n        if (modelUrl.load == null) {\n            if (!modelUrl.endsWith('/')) {\n                modelUrl = modelUrl + '/';\n            }\n            modelUrl = `${modelUrl}${DEFAULT_MODEL_NAME}${TFHUB_SEARCH_PARAM}`;\n        }\n    }\n    const model = new GraphModel(modelUrl, options);\n    await model.load();\n    return model;\n}\n//# sourceMappingURL=graph_model.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n/** DataType enum. */\nexport var DataType;\n(function (DataType) {\n    DataType[DataType[\"DT_INVALID\"] = 0] = \"DT_INVALID\";\n    DataType[DataType[\"DT_FLOAT\"] = 1] = \"DT_FLOAT\";\n    DataType[DataType[\"DT_DOUBLE\"] = 2] = \"DT_DOUBLE\";\n    DataType[DataType[\"DT_INT32\"] = 3] = \"DT_INT32\";\n    DataType[DataType[\"DT_UINT8\"] = 4] = \"DT_UINT8\";\n    DataType[DataType[\"DT_INT16\"] = 5] = \"DT_INT16\";\n    DataType[DataType[\"DT_INT8\"] = 6] = \"DT_INT8\";\n    DataType[DataType[\"DT_STRING\"] = 7] = \"DT_STRING\";\n    DataType[DataType[\"DT_COMPLEX64\"] = 8] = \"DT_COMPLEX64\";\n    DataType[DataType[\"DT_INT64\"] = 9] = \"DT_INT64\";\n    DataType[DataType[\"DT_BOOL\"] = 10] = \"DT_BOOL\";\n    DataType[DataType[\"DT_QINT8\"] = 11] = \"DT_QINT8\";\n    DataType[DataType[\"DT_QUINT8\"] = 12] = \"DT_QUINT8\";\n    DataType[DataType[\"DT_QINT32\"] = 13] = \"DT_QINT32\";\n    DataType[DataType[\"DT_BFLOAT16\"] = 14] = \"DT_BFLOAT16\";\n    DataType[DataType[\"DT_FLOAT_REF\"] = 101] = \"DT_FLOAT_REF\";\n    DataType[DataType[\"DT_DOUBLE_REF\"] = 102] = \"DT_DOUBLE_REF\";\n    DataType[DataType[\"DT_INT32_REF\"] = 103] = \"DT_INT32_REF\";\n    DataType[DataType[\"DT_UINT8_REF\"] = 104] = \"DT_UINT8_REF\";\n    DataType[DataType[\"DT_INT16_REF\"] = 105] = \"DT_INT16_REF\";\n    DataType[DataType[\"DT_INT8_REF\"] = 106] = \"DT_INT8_REF\";\n    DataType[DataType[\"DT_STRING_REF\"] = 107] = \"DT_STRING_REF\";\n    DataType[DataType[\"DT_COMPLEX64_REF\"] = 108] = \"DT_COMPLEX64_REF\";\n    DataType[DataType[\"DT_INT64_REF\"] = 109] = \"DT_INT64_REF\";\n    DataType[DataType[\"DT_BOOL_REF\"] = 110] = \"DT_BOOL_REF\";\n    DataType[DataType[\"DT_QINT8_REF\"] = 111] = \"DT_QINT8_REF\";\n    DataType[DataType[\"DT_QUINT8_REF\"] = 112] = \"DT_QUINT8_REF\";\n    DataType[DataType[\"DT_QINT32_REF\"] = 113] = \"DT_QINT32_REF\";\n    DataType[DataType[\"DT_BFLOAT16_REF\"] = 114] = \"DT_BFLOAT16_REF\";\n})(DataType || (DataType = {}));\nexport var SaverDef;\n(function (SaverDef) {\n    /** CheckpointFormatVersion enum. */\n    let CheckpointFormatVersion;\n    (function (CheckpointFormatVersion) {\n        CheckpointFormatVersion[CheckpointFormatVersion[\"LEGACY\"] = 0] = \"LEGACY\";\n        CheckpointFormatVersion[CheckpointFormatVersion[\"V1\"] = 1] = \"V1\";\n        CheckpointFormatVersion[CheckpointFormatVersion[\"V2\"] = 2] = \"V2\";\n    })(CheckpointFormatVersion = SaverDef.CheckpointFormatVersion || (SaverDef.CheckpointFormatVersion = {}));\n})(SaverDef || (SaverDef = {}));\n//# sourceMappingURL=compiled_api.js.map"],"names":["TensorList","constructor","tensors","elementShape","elementDtype","maxNumElements","this","forEach","tensor","dtype","Error","shape","keep","idTensor","scalar","id","copy","clearAndClose","keepIds","has","dispose","length","size","stack","numElements","outputElementShape","tidy","reshapedTensors","map","reshape","popBack","pop","pushBack","push","resize","getItem","elementIndex","setItem","gather","indices","slice","concat","i","t","fromTensor","tensorElementShape","tensorList","unstack","reserve","scatter","maxIndex","Math","max","list","value","index","split","totalLength","cumulativeLengths","len","shapeWithoutFirstDim","elementPerRow","sizes","HashTable","keyDType","valueDType","handle","tensorMap","Map","clear","tensorSize","import","keys","values","checkKeyAndValueTensor","$keys","data","$values","keysLength","valuesLength","util","key","set","find","defaultValue","result","findWithDefault","get","TensorArray","name","maxSize","identicalElementShapes","dynamicSize","clearAfterRead","closed_","closed","read","tensorWithState","cleared","readMany","write","written","writeMany","assertShapesMatchAllowUndefinedSize","shapeA","shapeB","errorMessagePrefix","dim0","dim1","fullDefinedShape","some","dim","inferElementShape","listElementShape","partialShape","mergeElementShape","notfullDefinedShape","elementShapeA","elementShapeB","ExecutionContext","weightMap","tensorArrayMap","tensorListMap","functionMap","rootContext","frameName","iterationId","contexts","lastId","generateCurrentContextIds","newFrame","currentContext","currentContextId","_currentContextIds","currentContextIds","names","contextIdforContexts","context","join","enterFrame","frameId","unshift","exitFrame","splice","shift","nextIteration","Object","assign","getWeight","addTensorArray","tensorArray","getTensorArray","addTensorList","getTensorList","getExecutionSubgraph","inputs","outputs","initNodes","usedNodes","Set","missingInputs","dynamicNode","syncInputs","seen","inputNodeNames","initNodeNames","node","frontier","isControlFlow","isDynamicShape","isHashTable","children","child","filter","add","indexOf","input","CONTROL_FLOW_OPS","DYNAMIC_SHAPE_OPS","HASH_TABLE_OPS","op","GraphExecutor","graph","parent","compiledMap","_weightMap","SEPERATOR","_functions","_functionExecutorMap","_outputs","_inputs","_initNodes","_signature","signature","functions","weightIds","_weightIds","functionExecutorMap","resourceManager","_resourceManager","attrParams","undefined","inputNodes","signatureKey","outputNodes","defaultOutput","reduce","getCompilationKey","sortedInputs","sort","sortedOutputs","compile","executionInfo","outNames","n","inNames","nodes","weights","weight","orderedNodes","every","getNodesInTopologicalOrder","execute","mapInputs","checkInputs","checkInputShapeAndType","mapOutputs","checkOutputs","outputNodeNames","compilationKey","tensorsMap","nodeName","tensorsToKeep","getFrozenTensorIds","intermediateTensorConsumerCount","checkTensorForDisposal","ids","apply","outputNames","category","kept","count","executeAsync","_executeAsync","isFunctionExecution","executeWithControlFlow","results","outputIds","inputIds","isDisposed","executeFunctionAsync","mappedInputs","added","promises","processStack","Promise","all","missingOutputs","alternativeMsg","item","then","processChildNodes","childNode","inputNames","match","inputName","notInGraph","normalizedName","ResourceManager","hashTableNameToHandle","hashTableMap","addHashTable","hashTable","getHashTableHandleByName","getHashTableById","TFHUB_SEARCH_PARAM","DEFAULT_MODEL_NAME","GraphModel","modelUrl","loadOptions","version","modelVersion","executor","metadata","artifacts","userDefinedMetadata","modelSignature","findIOHandler","path","load","handler","requestInit","io","handlers","loadSync","modelTopology","versions","producer","minConsumer","weightData","weightSpecs","Instance","transformGraph","convertTensorMapToTensorsMap","modelInitializer","initializer","save","handlerOrURL","config","predict","normalizeInputs","Tensor","Array","isArray","normalizeOutputs","newMap","async","loadGraphModel","options","fromTFHub","endsWith","model","DataType","SaverDef","CheckpointFormatVersion"],"sourceRoot":""}